{"version":"1","records":[{"hierarchy":{"lvl1":"Moving to Github!"},"type":"lvl1","url":"/content/blogs/2019/2019-05-18-moving-to-github","position":0},{"hierarchy":{"lvl1":"Moving to Github!"},"content":"GTSAM is now live on Github. Github is doing so many things right, in addition to being the go-to platform for open source: it has free continuous integration for open source projects, it supports building great web-sites, and it is itself supporting many great efforts such as VS-code and Atom. While we initially launched GTSAM on Bitbucket because of its unlimited private repos, we felt we could hold out no longer. Github, here we come :-)","type":"content","url":"/content/blogs/2019/2019-05-18-moving-to-github","position":1},{"hierarchy":{"lvl1":"Launching gtsam.org"},"type":"lvl1","url":"/content/blogs/2019/2019-05-20-gtsam-org","position":0},{"hierarchy":{"lvl1":"Launching gtsam.org"},"content":"Today we launched GTSAM’s new web presence, \n\ngtsam.org. The site is hosted by Github Pages, and is generated via \n\nJekyll, a simple static website generator.\n\nAt the moment, the page still looks rather spartan, but we’ll add a little more design features as time goes on. Plans also call for blog posts (such as this one) and tutorials, on factor graphs and on GTSAM’s implementation of it.","type":"content","url":"/content/blogs/2019/2019-05-20-gtsam-org","position":1},{"hierarchy":{"lvl1":"So, what makes legged robots different?"},"type":"lvl1","url":"/content/blogs/2019/2019-09-18-legged-robot-factors-part-i","position":0},{"hierarchy":{"lvl1":"So, what makes legged robots different?"},"content":"Author: Ross Hartleyemail: \n\nm​.ross​.hartley@gmail​.com\n\nThis is the first blog post in a series about using factor graphs for legged robot state estimation. It is meant to provide a high-level overview of what I call kinematic and contact factors and how they can be used in GTSAM. More details can be found in our conference papers:\n\nHybrid Contact Preintegration for Visual-Inertial-Contact State Estimation Using Factor Graphs\n\nLegged Robot State-Estimation Through Combined Forward Kinematic and Preintegrated Contact Factors\n\nIt is assumed that the reader is already familiar with the terminology and theory behind factor graph based smoothing.","type":"content","url":"/content/blogs/2019/2019-09-18-legged-robot-factors-part-i","position":1},{"hierarchy":{"lvl1":"So, what makes legged robots different?","lvl2":"So, what makes legged robots different?"},"type":"lvl2","url":"/content/blogs/2019/2019-09-18-legged-robot-factors-part-i#so-what-makes-legged-robots-different","position":2},{"hierarchy":{"lvl1":"So, what makes legged robots different?","lvl2":"So, what makes legged robots different?"},"content":"Factor graph methods have been widely successful for mobile robot state estimation and SLAM. A common application is the fusion of inertial data (from an IMU) with visual data (from camera and/or LiDAR sensors) to estimate the robot’s pose over time.\n\nAlthough this approach is often demonstrated on wheeled and flying robots, identical techniques can be applied to their walking brethren. What makes legged robots different, however, is the presence of additional encoder and contact sensors. As I’ll show, these extra sensor measurements can be leveraged to improve state estimation results.\n\nSensors typically found on legged robots:\n\nInertial Measurement Units (IMUs)\n\nVision Sensors (cameras, LiDARs)\n\nJoint Encoders\n\nContact Sensors\n\nAll of these sensors exist on the University of Michigan’s version of the Cassie robot (developed by \n\nAgility Robotics), which I’ll use as a concrete example.","type":"content","url":"/content/blogs/2019/2019-09-18-legged-robot-factors-part-i#so-what-makes-legged-robots-different","position":3},{"hierarchy":{"lvl1":"So, what makes legged robots different?","lvl2":"Factor Graph Formulation"},"type":"lvl2","url":"/content/blogs/2019/2019-09-18-legged-robot-factors-part-i#factor-graph-formulation","position":4},{"hierarchy":{"lvl1":"So, what makes legged robots different?","lvl2":"Factor Graph Formulation"},"content":"Let’s see how we can create a factor graph using these 4 sensor types to estimate the trajectory of a legged robot. Each node in the graph represents the robot’s state at a particular timestep. This state includes the 3D orientation, position, and velocity of the robot’s base frame along with the IMU biases. We also include the pose of the contact frame (where the foot hits the ground) to this list of states. For simplicity, the base frame is assumed to be collocated with the inertial/vision sensor frames.\n\nEstimated States:\n\nBase pose, X\n\nBase velocity, V\n\nContact pose, C\n\nIMU biases, b\n\nEach independent sensor measurement will place a measurement factor on the graph’s nodes. Solving the factor graph consists of searching for the maximum a posteriori state estimate that minimizes the error between the predicted and actual measurements.\n\nThe robot’s inertial measurements can be incorporated into the graph using the preintegrated IMU factor built into GTSAM 4.0. This factor relates the base pose, velocity, and IMU biases across consecutive timesteps.\n\nVision data can be incorporated into the graph using a number of different factors depending on the sensor type and application. Here we will simply assume that vision provides a relative pose factor between two nodes in the graph. This can be from either visual odometry or loop closures.\n\nAt each timestep, the joint encoder data can be used to compute the relative pose transformation between the robot’s base and contact frames (through forward kinematics). This measurement be captured in a unary forward kinematic factor.\n\nOf course, adding the forward kinematic factor will not affect the optimal state estimate unless additional constraints are placed on the contact frame poses. This is achieved using a binary contact factor which uses contact measurements to infer the movement of the contact frame over time. The simplest case being contact implies zero movement of this frame. In other words, this contact factor tries to keep the contact pose fixed across timesteps where contact was measured. When contact is absent, this factor can simply be omitted in the graph.\n\nIf we know how the contact frame moves over time and we can measure the relative pose between the robot’s contact and base frames, then we have an implicit measurement of how the robot’s base moves over time.\n\nThe combined forward kinematic and contact factors can be viewed as kinematic odometry measurements of the robot’s base frame.\n\nAll together, a typical legged robot factor graph may be represented by the picture below. It will contain inertial, vision, forward kinematic, and contact factors.","type":"content","url":"/content/blogs/2019/2019-09-18-legged-robot-factors-part-i#factor-graph-formulation","position":5},{"hierarchy":{"lvl1":"So, what makes legged robots different?","lvl2":"Forward Kinematic Factor"},"type":"lvl2","url":"/content/blogs/2019/2019-09-18-legged-robot-factors-part-i#forward-kinematic-factor","position":6},{"hierarchy":{"lvl1":"So, what makes legged robots different?","lvl2":"Forward Kinematic Factor"},"content":"The forward kinematics factor relates the base pose to the current contact pose using noisy encoder measurements. This is a simple relative pose factor which means we can use GTSAM’s built in BetweenFactor<Pose3> factor to implement it. We just need to determine what the factor’s covariance will be.\n\nAssuming the encoder noise is gaussian, we can map the encoder covariance to the contact pose covariance using the body manipulator Jacobian of the forward kinematics function. In general, the manipulator Jacobian maps joint angle rates to end effector twist, so it makes sense that it can be used to approximate the mapping of encoder uncertainty through the non-linearities of the robot’s kinematics.\n\nFor example, if H_BC is pose of the contact frame relative to the base frame and J_BC is the corresponding body manipulator Jacobian, the forward kinematic factor can be implemented using the following GTSAM code:Matrix6 FK_Cov = J_BC * encoder_covariance_matrix * J_BC.transpose();\nBetweenFactor<Pose3> forward_kinematics_factor(X(node), C(node), H_BC, noiseModel::Gaussian::Covariance(FK_cov));","type":"content","url":"/content/blogs/2019/2019-09-18-legged-robot-factors-part-i#forward-kinematic-factor","position":7},{"hierarchy":{"lvl1":"So, what makes legged robots different?","lvl2":"Rigid Contact Factor"},"type":"lvl2","url":"/content/blogs/2019/2019-09-18-legged-robot-factors-part-i#rigid-contact-factor","position":8},{"hierarchy":{"lvl1":"So, what makes legged robots different?","lvl2":"Rigid Contact Factor"},"content":"Contact factors come in a number of different flavors depending on the assumptions we want to make about the contact sensor measurements. However, they will all provide a measurement of contact frame odometry (i.e. how to foot will move over time).\n\nFor example, in the simplest case, perhaps measuring contact implies that the entire pose of the foot remains fixed. We can call this rigid contact, and it may be a good assumption for many humanoid robots that have large, flat feet. In contrast, we could alternatively assume a point contact, where the position of the contact frame remains fixed, but the foot is free to rotate.\n\nFor now, lets look at how we can implement a rigid contact factor.  Again, when contact is measured, we assume there is no relative change in the contact frame pose. In other words, the contact frame velocity is zero. This can be implemented in GTSAM using a BetweenFactor<Pose3> factor, where the measurement is simply the identity element.BetweenFactor<Pose3> contact_factor(C(node-1), C(node), Pose3::identity(), noiseModel::Gaussian::Covariance(Sigma_ij);\n\nSome potential foot slip can be accommodated through the factor’s covariance, Sigma_ij. If we are highly confident that the foot remained fixed on the ground, this covariance should be small. A large covariance implies less confidence in this assumption.\n\nOne idea is simply assuming Gaussian noise on the contact frame velocity. In this case, the factor’s covariance will grow with the length of time between the graph nodes. Another idea is to use contact force information to model the factor’s covariance.","type":"content","url":"/content/blogs/2019/2019-09-18-legged-robot-factors-part-i#rigid-contact-factor","position":9},{"hierarchy":{"lvl1":"So, what makes legged robots different?","lvl3":"What happens when contact is made/broken?","lvl2":"Rigid Contact Factor"},"type":"lvl3","url":"/content/blogs/2019/2019-09-18-legged-robot-factors-part-i#what-happens-when-contact-is-made-broken","position":10},{"hierarchy":{"lvl1":"So, what makes legged robots different?","lvl3":"What happens when contact is made/broken?","lvl2":"Rigid Contact Factor"},"content":"Using the formulation above, every time contact is made or broken, a new node has to be added to the factor graph. This stems from our contact measurement assumption. When the robot loses contact, we have no way to determine the foot movement using contact sensors alone. Our only choice is to add a new node in the graph and omit the contact factor until contact is regained.\n\nThis may not be an issue for some slow walking robots where contact states change infrequently, but what about a running hexapod? In that case, the numerous contact changes will lead to an explosion in the number of nodes (and optimization variables) needed in the graph. This ultimately affects the performance of the state estimator by increasing the time it takes to solve the underlying optimization problem.\n\nThankfully, there is a way around this problem.","type":"content","url":"/content/blogs/2019/2019-09-18-legged-robot-factors-part-i#what-happens-when-contact-is-made-broken","position":11},{"hierarchy":{"lvl1":"So, what makes legged robots different?","lvl2":"Hybrid Rigid Contact Factor"},"type":"lvl2","url":"/content/blogs/2019/2019-09-18-legged-robot-factors-part-i#hybrid-rigid-contact-factor","position":12},{"hierarchy":{"lvl1":"So, what makes legged robots different?","lvl2":"Hybrid Rigid Contact Factor"},"content":"During many types of walking gaits, when one contact is broken, another is made. For example, with bipedal walking, left stance is followed by right stance, then left, then right, and so on. The order on a hexapod might be more complicated, but one fact remains: there is often (at least) one foot on the ground at all times. We can use this knowledge to improve performance in our factor graph by limiting the insertion rate of new graph nodes.\n\nIf we choose two arbitrary times (potentially far apart), any single contact is likely to have been broken at some point between them. However, there may be a chain of contact frames that we can swap through to maintain the notion of contact with the environment. Each consecutive pair of contact frames in this chain are related to each other through forward kinematics.\n\nFor example, lets say our biped robot switched from left stance (L1), to right stance (R1), back to left stance (L2) again. During the L1 phase, we can assume the contact frame pose remained fixed (zero velocity). When the robot switches from L1 to R1, we can map this contact frame from the left to the right foot using the encoder measurements and forward kinematics (since both feet are on the ground at this time). During the R1 phase, we again assume that the contact pose remains fixed. When the second swap happens, R1 to L2, we map the contact frame back to left foot using the encoder measurements and a (different) forward kinematics function. So in effect, we have tracked the movement of the contact frame across two contact switches. This relative pose measurement provides odometry for the contact frame and can be used to create a hybrid rigid contact factor. Using this method, nodes can now be added to the graph at arbitrary times, and do not have to be added when contact is made/broken (unless all feet come off the ground).\n\nLike the original rigid contact factor, the hybrid version can be created using GTSAM’s BetweenFactor<Pose3> factor, where delta_Hij is the cumulative change in contact pose across all contact switches. The factor’s covariance is typically larger as it needs to account for the uncertainty in each swap’s forward kinematics.BetweenFactor<Pose3> hybrid_contact_factor(C(node-1), C(node), delta_Hij, noiseModel::Gaussian::Covariance(Sigma_ij);","type":"content","url":"/content/blogs/2019/2019-09-18-legged-robot-factors-part-i#hybrid-rigid-contact-factor","position":13},{"hierarchy":{"lvl1":"So, what makes legged robots different?","lvl2":"Coming soon..."},"type":"lvl2","url":"/content/blogs/2019/2019-09-18-legged-robot-factors-part-i#coming-soon","position":14},{"hierarchy":{"lvl1":"So, what makes legged robots different?","lvl2":"Coming soon..."},"content":"I briefly discussed two types of factors that can be used to improve legged robot state estimation: the forward kinematic factor and the (hybrid) rigid contact factor. Combining these two factors allows for kinematic odometry to be added alongside other measurements (inertial, vision, etc.) when building up a factor graph.\n\nIn particular, when developing the rigid contact factor, we made the strong assumption that a contact measurement implies zero angular and linear velocity of the contact frame. The factor tries to keep the entire pose of the foot fixed across two timesteps. This assumption may not be valid for all types of walking robots. In fact, it doesn’t even hold for the Cassie robot! The roll angle about Cassie’s foot is unactuated and free to move during walking. In the next post, I will discuss the (hybrid) point contact factor which makes no assumptions about the angular velocity of the contact frame.","type":"content","url":"/content/blogs/2019/2019-09-18-legged-robot-factors-part-i#coming-soon","position":15},{"hierarchy":{"lvl1":"Look Ma, No RANSAC"},"type":"lvl1","url":"/content/blogs/2019/2019-09-20-robust-noise-model","position":0},{"hierarchy":{"lvl1":"Look Ma, No RANSAC"},"content":"Author: Varun AgrawalWebsite: \n\nvarunagrawal​.github​.io\n\n(Psst, be sure to clone/download GTSAM 4.0.2 which resolves a bug in the Huber model discussed below, for correct weight behavior)","type":"content","url":"/content/blogs/2019/2019-09-20-robust-noise-model","position":1},{"hierarchy":{"lvl1":"Look Ma, No RANSAC","lvl2":"Introduction"},"type":"lvl2","url":"/content/blogs/2019/2019-09-20-robust-noise-model#introduction","position":2},{"hierarchy":{"lvl1":"Look Ma, No RANSAC","lvl2":"Introduction"},"content":"\n\nFigure 1 Two books on a plane separated by an SE(2) transform, and some manually selected feature matches between them. There are some clearly incorrect matches, which are outliers.\n\nRobust error models are powerful tools for supplementing parameter estimation algorithms with the added capabilities of modeling outliers. Parameter estimation is a fundamental tool used in many fields, especially in perception and robotics, and thus performing robust parameter estimation across a wide range of applications and scenarios is crucial to strong performance for many applications. This necessitates the need to manage outliers in our measurements, and robust error models provide us the means to do so. Robust error models are amenable to easy plug-and-play use in pre-existing optimization frameworks, requiring minimal changes to existing pipelines.\n\nUsing robust error models can even obviate the need for more complex, non-deterministic algorithms such as Random Sample Consensus (a.k.a. RANSAC), a fundamental tool for parameter estimation in many a roboticist’s toolbox for years. While RANSAC has proven to work well in practice, it might need a high number of runs to converge to a good solution. Robust error models are conceptually easy and intuitive and can be used by themselves or in conjunction with RANSAC.\n\nIn this blog post, we demonstrate the capabilities of robust error models which downweigh outliers to provide better estimates of the parameters of interest. To illustrate the benefits of robust error models, take a look at Figure 1. We show two books next to each other, transformed by an SE(2) transform, with some manually labeled point matches between the two books. We have added some outliers to more realistically model the problem.\nAs we will see later in the post, the estimates from RANSAC and a robust estimation procedure are quite similar, even though the ratio of inliers to outliers is 2:1.","type":"content","url":"/content/blogs/2019/2019-09-20-robust-noise-model#introduction","position":3},{"hierarchy":{"lvl1":"Look Ma, No RANSAC","lvl2":"Parameter Estimation - A Short Tutorial"},"type":"lvl2","url":"/content/blogs/2019/2019-09-20-robust-noise-model#parameter-estimation-a-short-tutorial","position":4},{"hierarchy":{"lvl1":"Look Ma, No RANSAC","lvl2":"Parameter Estimation - A Short Tutorial"},"content":"We begin by reviewing techniques for parameter estimation as outlined by the \n\ntutorial by Zhengyou Zhang, as applied to our SE(2). Given some matches \\{(s,s')\\} between the two images, we want to estimate the the SE(2) parameters θ that transform a feature s in the first image to a feature s' in the  second image:\n\n\\[ s’ = f(\\theta ; s) \\]\n\nOf course, this generalizes to other transformations, including 3D-2D problems. This is a ubiquitous problem seen in multiple domains of perception and robotics and is referred to as parameter estimation.\n\nA standard framework to estimate the parameters is via the Least-Squares formulation. We usually have many more observations than we have parameters, i.e., the problem is now overdetermined. To handle this, we minimize the sum of square residuals f(\\theta ; s_i) - s'_i, for i\\in1\\ldots N:\n\n\\[ E_{LS}(\\theta) =  \\sum_i \\vert\\vert f(\\theta ; s_i) - s’_i \\vert\\vert^2 \\]\n\nwhich we refer to as the cost function or the objective function.\nIn the case of SE(2) the parameters θ should be some parameterization of a transformation matrix, having three degrees of freedom (DOF). A simple way to accomplish this is to have \\theta=(x,y,\\alpha).\n\nOur measurement functions are generally non-linear, and hence we need to linearize the measurement function around an estimate of θ. GTSAM will iteratively do so via optimization procedures such as Gauss-Newton, Levenberg-Marquardt, or Dogleg. Linearization is done via the Taylor expansion around a linearization point \\theta_0:\n\n\\[ f(\\theta + \\Delta\\theta; s) = f(\\theta; s) + J(\\theta; s)\\Delta\\theta \\]\n\nThis gives us the following linearized least squares objective function:\n\n\\[ E_{LS, \\theta_0} = \\sum_i \\vert\\vert f(\\theta; s_i) + J(\\theta; s_i)\\Delta\\theta - s_i’ \\vert\\vert^2 \\]\n\nSince the above is now linear in \\Delta\\theta, GTSAM can solve it using either sparse Cholesky or QR factorization.","type":"content","url":"/content/blogs/2019/2019-09-20-robust-noise-model#parameter-estimation-a-short-tutorial","position":5},{"hierarchy":{"lvl1":"Look Ma, No RANSAC","lvl2":"Robust Error Models"},"type":"lvl2","url":"/content/blogs/2019/2019-09-20-robust-noise-model#robust-error-models","position":6},{"hierarchy":{"lvl1":"Look Ma, No RANSAC","lvl2":"Robust Error Models"},"content":"We have derived the basic parameter optimization approach in the previous section and seen how the choice of the optimization function affects the optimality of our solution. However, another aspect we need to take into account is the effect of outliers on our optimization and final parameter values.\n\nBy default, the optimization objectives outlined above try to model all measurements equally. This means that in the presence of outliers, the optimization process might give us parameter estimates that try to fit these outliers, sacrificing accuracy on the inliers. More formally, given the residual r_i of the i^{th} match, i.e. the difference between the i^{th} observation and the fitted value, the standard least squares approach attempts to optimize the sum of all the squared residuals. This can lead to the estimated parameters being distorted due to the equal weighting of all data points. Surely, there must be a way for the objective to model inliers and outliers in a clever way based on the residual errors?\n\nOne way to tackle the presence of outliers is a family of models known as Robust Error Models or M-estimators. The M-estimators try to reduce the effect of outliers by replacing the squared residuals with a function of the residuals ρ that weighs each residual term by some value:\n\n\\[ p = min \\sum_i^n \\rho(r_i) \\]\n\nTo allow for optimization, we define ρ to be a symmetric, positive-definite function, thus it has a unique minimum at zero, and is less increasing than square.\n\nThe benefit of this formulation is that we can now solve the above minimization objective as an Iteratively Reweighted Least Squares problem. The M-estimator of the parameter vector p based on ρ is the value of the parameters which solves\n\n\\[ \\sum_i \\psi(r_i)\\frac{\\delta r_i}{\\delta p_j} = 0 \\]\n\nfor j = 1, ..., m (recall that the maximum/minimum of a function is at the point its derivative is equal to zero). Above, \\psi(x) = \\frac{\\delta \\rho(x)}{\\delta x} is called the influence function, which we can use to define a weight function w(x) = \\frac{\\psi{x}}{x} giving the original derivative as\n\n\\[ \\sum_i w(r_i) r_i \\frac{\\delta r_i}{\\delta p_j} = 0 \\]\n\nwhich is exactly the system of equations we obtain from iterated reweighted least squares.\n\nIn layman’s terms, the influence function \\psi(x) measures the influence of a data point on the value of the parameter estimate. This way, the estimated parameters are intelligent to outliers and only sensitive to inliers, since the are no longer susceptible to being significantly modified by a single match, thus making them robust.","type":"content","url":"/content/blogs/2019/2019-09-20-robust-noise-model#robust-error-models","position":7},{"hierarchy":{"lvl1":"Look Ma, No RANSAC","lvl3":"M-estimator Constraints","lvl2":"Robust Error Models"},"type":"lvl3","url":"/content/blogs/2019/2019-09-20-robust-noise-model#m-estimator-constraints","position":8},{"hierarchy":{"lvl1":"Look Ma, No RANSAC","lvl3":"M-estimator Constraints","lvl2":"Robust Error Models"},"content":"While M-estimators provide us with significant benefits with respect to outlier modeling, they do come with some constraints which are required to enable their use in a wide variety of optimization problems.\n\nThe influence function should be bounded.\n\nThe robust estimator should be unique, i.e. it should have a unique minimum. This implies that the individual ρ-function is convex in variable p.\n\nThe objective should have a gradient, even when the 2nd derivative is singular.","type":"content","url":"/content/blogs/2019/2019-09-20-robust-noise-model#m-estimator-constraints","position":9},{"hierarchy":{"lvl1":"Look Ma, No RANSAC","lvl3":"Common M-estimators","lvl2":"Robust Error Models"},"type":"lvl3","url":"/content/blogs/2019/2019-09-20-robust-noise-model#common-m-estimators","position":10},{"hierarchy":{"lvl1":"Look Ma, No RANSAC","lvl3":"Common M-estimators","lvl2":"Robust Error Models"},"content":"Below we list some of the common estimators from the literature and which are available out of the box in GTSAM. We also provide accompanying graphs of the corresponding ρ function, the influence function, and the weight function in order, allowing one to visualize the differences and effects of each estimators influence function.\n\nFair\n\n\nHuber\n\n\nCauchy\n\n\nGeman-McClure\n\n\nWelsch\n\n\nTukey\n","type":"content","url":"/content/blogs/2019/2019-09-20-robust-noise-model#common-m-estimators","position":11},{"hierarchy":{"lvl1":"Look Ma, No RANSAC","lvl2":"Example with Huber Noise Model"},"type":"lvl2","url":"/content/blogs/2019/2019-09-20-robust-noise-model#example-with-huber-noise-model","position":12},{"hierarchy":{"lvl1":"Look Ma, No RANSAC","lvl2":"Example with Huber Noise Model"},"content":"Now it’s time for the real deal. So far we’ve spoken about how great robust estimators are, and how they can be easily modeled in a least squares objective, but having a concrete example and application can really help illuminate these concepts and demonstrate the power of a robust error model. In this specific case we use the Huber M-estimator, though any other provided M-estimator can be used depending on the application or preference.\n\nFor our example application, the estimation of an SE(2) transformation between two objects (a scenario commonly seen in PoseSLAM applications), we go back to our image of the two books from the introduction, which we have manually labeled with matches and outliers. A RANSAC estimate using the matches gives us the SE(2) paramters (347.15593, 420.31040, 0.39645).\n\n\n\nFigure 2: Matches between the 2 books.\n\nTo begin, we apply a straightforward optimization process based on Factor Graphs. Using GTSAM, this can be achieved in a few lines of code. We show the core part of the example below, omitting the housekeeping and data loading for brevity.// This is the value we wish to estimate\nPose2_ pose_expr(0);\n\n// Set up initial values, and Factor Graph\nValues initial;\nExpressionFactorGraph graph;\n\n// provide an initial estimate which is pretty much random\ninitial.insert(0, Pose2(1, 1, 0.01));\n\n// We assume the same noise model for all points (since it is the same camera)\nauto measurementNoise = noiseModel::Isotropic::Sigma(2, 1.0);\n\n// Now we add in the factors for the measurement matches.\n// Matches is a vector of 4 tuples (index1, keypoint1, index2, keypoint2)\nint index_i, index_j;\nPoint2 p, measurement;\nfor (vector<tuple<int, Point2, int, Point2>>::iterator it = matches.begin();\n    it != matches.end(); ++it) {\n\n    std::tie(index_i, measurement, index_j, p) = *it;\n\n    Point2_ predicted = transformTo(pose_expr, p);\n    \n    // Add the Point2 expression variable, an initial estimate, and the measurement noise.\n    graph.addExpressionFactor(predicted, measurement, measurementNoise);\n}\n\n// Optimize and print basic result\nValues result = LevenbergMarquardtOptimizer(graph, initial).optimize();\nresult.print(\"Final Result:\\n\");\n\nIt is important to note that our initial estimate for the transform values is pretty arbitrary.\nRunning the above code give us the transform values (305.751, 520.127, 0.284743), which when compared to the RANSAC estimate doesn’t look so good.\n\nNow how about we try using M-estimators via the built-in robust error models? This is a two line change as illustrated below:// This is the value we wish to estimate\nPose2_ pose_expr(0);\n\n// Set up initial values, and Factor Graph\nValues initial;\nExpressionFactorGraph graph;\n\n// provide an initial estimate which is pretty much random\ninitial.insert(0, Pose2(1, 1, 0.01));\n\n// We assume the same noise model for all points (since it is the same camera)\nauto measurementNoise = noiseModel::Isotropic::Sigma(2, 1.0);\n\n/********* First change *********/\n// We define our robust error model here, providing the default parameter value for the estimator.\nauto huber = noiseModel::Robust::Create(noiseModel::mEstimator::Huber::Create(1.345), measurementNoise);\n\n// Now we add in the factors for the measurement matches.\n// Matches is a vector of 4 tuples (index1, keypoint1, index2, keypoint2)\nint index_i, index_j;\nPoint2 p, measurement;\nfor (vector<tuple<int, Point2, int, Point2>>::iterator it = matches.begin();\n    it != matches.end(); ++it) {\n\n    std::tie(index_i, measurement, index_j, p) = *it;\n\n    Point2_ predicted = transformTo(pose_expr, p);\n\n    // Add the Point2 expression variable, an initial estimate, and the measurement noise.\n    // The graph takes in factors with the robust error model.\n    /********* Second change *********/\n    graph.addExpressionFactor(predicted, measurement, huber);\n}\n\n// Optimize and print basic result\nValues result = LevenbergMarquardtOptimizer(graph, initial).optimize();\nresult.print(\"Final Result:\\n\");\n\nThis version of the parameter estimation gives us the SE(2) transform (363.76, 458.966, 0.392419). Quite close for only 15 total matches, especially considering 5 of the 10 were outliers! The estimates are expected to improve with more matches to further constrain the problem.\n\nAs an alternate example, we look at an induced SE(2) transform and try to estimate it from scratch (no manual match labeling. Our source image is one of crayon boxes retrieved from the internet, since this image lends itself to good feature detection.\n\nTo model an SE(2) transformation, we apply a perspective transform to the image. The transformation applied is (x, y, theta) = (4.711, 3.702, 0.13963). This gives us a ground truth value to compare our methods against. The transformed image can be seen below.\n\nWe run the standard pipeline of SIFT feature extraction and FLANN+KDTree based matching to obtain a set of matches. At this point we are ready to evaluate the different methods of estimating the transformation.\n\nThe vanilla version of the parameter estimation gives us (6.26294, -14.6573, 0.153888) which is pretty bad. However, the version with robust error models gives us (4.75419, 3.60199, 0.139674), which is a far better estimate when compared to the ground truth, despite the initial estimate for the transform being arbitrary. This makes it apparent that the use of robust estimators and subsequently robust error models is the way to go for use cases where outliers are a concern. Of course, providing better initial estimates will only improve the final estimate.\n\nYou may ask how does this compare to our dear old friend RANSAC? Using the OpenCV implementation of RANSAC, a similar pipeline gives use the following SE(2) values: (4.77360, 3.69461, 0.13960).\n\nWe show, in order, the original image, its warped form, and the recovered image from the SE(2) transformation estimation. The first image is from the ground truth transform, the second one is using RANSAC, the next one is using a vanilla parameter estimation approach, and the last one uses robust error models. As you can see, while the vanilla least-squares optimization result is poor compared to the ground truth, the RANSAC and the robust error model recover the transformation correctly, with the robust error model’s result being comparable to the RANSAC one.\n\n\n\nImage warping and recovery using ground truth SE(2) transform. The first image is the original image, the 2nd image is the transformed image, and the last one is the image we get on applying the reverse $SE(2)$ transform.\n\n\n\nImage warping and recovery using RANSAC.\n\n\n\nImage warping and recovery using plain old parameter estimation. You can see that the 3rd (recovered) image does not line up correctly.\n\n\n\nImage warping and recovery using robust error models with parameter estimation. These results are comparable to the ones from RANSAC, demonstrating the promise of robust error models. That's pretty close too, so why go through the headache of using robust error models? For one, unlike RANSAC, robust error models are deterministic and possess defined behavior. Moreover, one does not need to run multiple runs of optimization to obtain a consistent result, compared to RANSAC which may require hundreds of runs to converge to a good result.  ## Robust Error Models + RANSAC ","type":"content","url":"/content/blogs/2019/2019-09-20-robust-noise-model#example-with-huber-noise-model","position":13},{"hierarchy":{"lvl1":"Look Ma, No RANSAC","lvl2":"Conclusion"},"type":"lvl2","url":"/content/blogs/2019/2019-09-20-robust-noise-model#conclusion","position":14},{"hierarchy":{"lvl1":"Look Ma, No RANSAC","lvl2":"Conclusion"},"content":"In this post, we have seen the basics of parameter estimation, a ubiquitous mathematical framework for many perception and robotics problems, and we have seen how this framework is susceptible to perturbations from outliers which can throw off the final estimate. More importantly, we have seen how a simple tool called the Robust M-estimator can easily help us deal with these outliers and their effects. An example case for SE(2) transform estimation demonstrates not only their ease of use with GTSAM, but also the efficacy of the solution generated, especially when compared to widely used alternatives such as RANSAC.\n\nFurthermore, robust estimators are deterministic, ameliorating the need for the complexity that comes inherent with RANSAC. While RANSAC is a great tool, robust error models provide us with a solid alternative to be considered. With the benefits of speed, accuracy, and ease of use, robust error models make a strong case for their adoption for many related problems and we hope you will give them a shot the next time you use GTSAM.","type":"content","url":"/content/blogs/2019/2019-09-20-robust-noise-model#conclusion","position":15},{"hierarchy":{"lvl1":"LQR Control Using Factor Graphs"},"type":"lvl1","url":"/content/blogs/2019/2019-11-07-lqr-control","position":0},{"hierarchy":{"lvl1":"LQR Control Using Factor Graphs"},"content":" horizontal scrolling  - TOC\n{:toc} ","type":"content","url":"/content/blogs/2019/2019-11-07-lqr-control","position":1},{"hierarchy":{"lvl1":"LQR Control Using Factor Graphs","lvl2":"Introduction"},"type":"lvl2","url":"/content/blogs/2019/2019-11-07-lqr-control#introduction","position":2},{"hierarchy":{"lvl1":"LQR Control Using Factor Graphs","lvl2":"Introduction"},"content":"\n\nFigure 1:Factor graph structure for an LQR problem with 3 time steps. The cost factors are marked with dashed lines and the dynamics constraint factors are marked with solid lines.\n\nIn this post we explain how optimal control problems can be formulated as factor graphs and solved\nby performing variable elimination on the factor graph.\n\nSpecifically, we will show the factor graph formulation and solution for the\nLinear Quadratic Regulator (LQR).  LQR is a state feedback controller which derives the optimal gains\nfor a linear system with quadratic costs on control effort and state error.\n\nWe consider the \n\nfinite-horizon, discrete LQR\nproblem.\nThe task is to find the optimal controls u_k at time instances t_k\nso that a total cost is minimized.  Note that we will later see the optimal controls can be represented in the form u^*_k = K_kx_k for some optimal gain matrices K_k.  The LQR problem can be represented as a constrained optimization\nproblem where the costs of control and state error are represented by the\nminimization objective \\eqref{eq:cost}, and the system dynamics are represented by the\nconstraints \\eqref{eq:dyn_model}.\\def\\argmin{\\mathop{\\mathrm{argmin}}\\limits}\n\\def\\coloneqq{\\mathrel{\\mathop:}=}\n\n\\begin{equation} \\argmin\\limits_{u_{1\\sim N}} ~~x_N^T Q x_N + \\sum\\limits_{k=1}^{N-1} x_k^T Q x_k + u_k^T R u_k  \\end{equation}\n\\begin{equation} s.t. ~~ x_{k+1}=Ax_k+Bu_k ~~\\text{for } k=1 \\text{ to } N-1  \\end{equation}\n\nWe can visualize the objective function and constraints in the form of a factor\ngraph as shown in \n\nFigure 1. This is a simple Markov chain, with the oldest\nstates and controls on the left, and the newest states and controls on the right. The\nternary factors represent the dynamics model constraints and the unary\nfactors represent the state and control costs we seek to minimize via least-squares.","type":"content","url":"/content/blogs/2019/2019-11-07-lqr-control#introduction","position":3},{"hierarchy":{"lvl1":"LQR Control Using Factor Graphs","lvl2":"Variable Elimination"},"type":"lvl2","url":"/content/blogs/2019/2019-11-07-lqr-control#variable-elimination","position":4},{"hierarchy":{"lvl1":"LQR Control Using Factor Graphs","lvl2":"Variable Elimination"},"content":"To optimize the factor graph, which represents minimizing the least squares objectives above, we can simply eliminate the factors from right\nto left.  In this section we demonstrate the variable elimination graphically and algebraically, but the matrix elimination is also\nprovided in the \n\nAppendix. ********************** BEGIN VARIABLE ELIMINATION SLIDESHOW **********************  Slideshow container, based on https://www.w3schools.com/howto/howto_js_slideshow.asp <div class=\"slideshow-container\">\n  <div class=\"mySlides 0\" style=\"text-align: center;\">\n    <!-- <div class=\"numbertext\">2 / 3</div> -->\n    <a name=\"fig_eliminate_x_a\"></a>\n    <figure class=\"center\">\n    <img src=\"/_static/lqr_control/Elimination/cropped_Slide1.png\"\n        alt=\"Elimination of state $x_2$\" />\n        <figcaption><b>Figure 2a</b> Elimination of state $x_2$</figcaption>\n    </figure>\n  </div>\n  <div class=\"mySlides 0\" style=\"text-align: center;\">\n    <!-- <div class=\"numbertext\">2 / 3</div> -->\n    <a name=\"fig_eliminate_x_b\"></a>\n    <figure class=\"center\">\n    <img src=\"/_static/lqr_control/Elimination/cropped_Slide2.png\"\n        alt=\"Elimination of state $x_2$\" />\n        <figcaption><b>Figure 2b</b> Elimination of state $x_2$</figcaption>\n    </figure>\n  </div>\n  <div class=\"mySlides 0\" style=\"text-align: center;\">\n    <a name=\"fig_eliminate_u_a\"></a>\n    <figure class=\"center\">\n    <img src=\"/_static/lqr_control/Elimination/cropped_Slide3.png\"\n        alt=\"Elimination of state $u_1$\" />\n        <figcaption><b>Figure 3a</b> Elimination of state $u_1$</figcaption>\n    </figure>\n  </div>\n  <div class=\"mySlides 0\" style=\"text-align: center;\">\n    <a name=\"fig_eliminate_u_b\"></a>\n    <figure class=\"center\">\n    <img src=\"/_static/lqr_control/Elimination/cropped_Slide4.png\"\n        alt=\"Elimination of state $u_1$\" />\n        <figcaption><b>Figure 3b</b> Elimination of state $u_1$</figcaption>\n    </figure>\n  </div>\n  <div class=\"mySlides 0\" style=\"text-align: center;\">\n    <a name=\"fig_merge_factor\"></a>\n    <figure class=\"center\">\n    <img src=\"/_static/lqr_control/Elimination/cropped_Slide5.png\"\n        alt=\"Elimination of state $u_1$\" />\n        <figcaption><b>Figure 3c</b> Cost-to-go at $x_1$ is the sum of the two unary factors on $x_1$ (green)</figcaption>\n    </figure>\n  </div>\n  <div class=\"mySlides 0\" style=\"text-align: center;\">\n    <a name=\"fig_bayes_net\"></a>\n    <figure class=\"center\">\n    <img src=\"/_static/lqr_control/Elimination/cropped_Slide6.png\"\n        alt=\"Bayes net\" />\n        <figcaption><b>Figure 4a</b> Repeat elimination until the graph is reduced to a Bayes net</figcaption>\n    </figure>\n  </div>\n  <!-- <div class=\"mySlides 0\" style=\"text-align: center;\">\n    <a name=\"fig_bayes_net\"></a>\n    <figure class=\"center\">\n    <img src=\"/_static/lqr_control/Elimination/cropped_Slide7.png\"\n        alt=\"Bayes net\" />\n        <figcaption><b>Figure 4b</b> Repeat elimination until the graph is reduced to a Bayes net</figcaption>\n    </figure>\n  </div> -->\n  <div class=\"mySlides 0\" style=\"text-align: center;\">\n    <a name=\"fig_bayes_net\"></a>\n    <figure class=\"center\">\n    <img src=\"/_static/lqr_control/Elimination/cropped_Slide8.png\"\n        alt=\"Bayes net\" />\n        <figcaption><b>Figure 4b</b> Repeat elimination until the graph is reduced to a Bayes net</figcaption>\n    </figure>\n  </div>\n  <div class=\"mySlides 0\" style=\"text-align: center;\">\n    <a name=\"fig_bayes_net\"></a>\n    <figure class=\"center\">\n    <img src=\"/_static/lqr_control/Elimination/cropped_Slide10.png\"\n        alt=\"Bayes net\" />\n        <figcaption><b>Figure 4c</b> Repeat elimination until the graph is reduced to a Bayes net</figcaption>\n    </figure>\n  </div>\n  <div class=\"mySlides 0\" style=\"text-align: center;\">\n    <a name=\"fig_bayes_net\"></a>\n    <figure class=\"center\">\n    <img src=\"/_static/lqr_control/Elimination/cropped_Slide11.png\"\n        alt=\"Bayes net\" />\n        <figcaption><b>Figure 4d</b> Completed Bayes net</figcaption>\n    </figure>\n  </div>\n  <!-- Next and previous buttons -->\n  <a class=\"prev\" onclick=\"plusSlides(-1,0)\">&#10094;</a>\n  <a class=\"next\" onclick=\"plusSlides(1,0)\">&#10095;</a>\n\n</div> <!-- slideshow-container -->\n\n<!-- The dots/circles -->\n<div style=\"text-align:center\">\n  <span class=\"dot 0\" onclick=\"currentSlide(1,0)\"></span>\n  <span class=\"dot 0\" onclick=\"currentSlide(2,0)\"></span>\n  <span class=\"dot 0\" onclick=\"currentSlide(3,0)\"></span>\n  <span class=\"dot 0\" onclick=\"currentSlide(4,0)\"></span>\n  <span class=\"dot 0\" onclick=\"currentSlide(5,0)\"></span>\n  <span class=\"dot 0\" onclick=\"currentSlide(6,0)\"></span>\n  <span class=\"dot 0\" onclick=\"currentSlide(7,0)\"></span>\n  <span class=\"dot 0\" onclick=\"currentSlide(8,0)\"></span>\n  <span class=\"dot 0\" onclick=\"currentSlide(9,0)\"></span>\n  <!-- <span class=\"dot 0\" onclick=\"currentSlide(10,0)\"></span> -->\n</div>\n\n<!-- this css is to make the scroll bar disappear when the mouse isn't over the scrollable div...\nTaken from my website: https://github.com/gchenfc/gerrysworld2/blob/master/css/activity.css -->\n<style>\n.scrollablecontent::-webkit-scrollbar {\n    width: 5px;\n    height: 12px;\n}\n.scrollablecontent::-webkit-scrollbar-track {\n    background: transparent;\n}\n.scrollablecontent::-webkit-scrollbar-thumb {\n    background: #ddd;\n    visibility:hidden;\n}\n.scrollablecontent:hover::-webkit-scrollbar-thumb {\n    visibility:visible;\n}\n/* .slideout {\n  width: 400px;\n  height: auto;\n  overflow: hidden;\n  background: orange;\n  margin: 0 auto;\n  transition: height 0.4s linear;\n}\n.hide {\n    height: 0;\n} */\n</style>\n<!-- ********************** END VARIABLE ELIMINATION SLIDESHOW ********************** -->\n\n<!-- ************************ BEGIN SCROLLABLE ELIMINATION DESCRIPTION ************************ -->\n<div class=\"scrollablecontent\" markdown=\"1\" id=\"sec:elim_scrollable\"\n    style=\"overflow-x: hidden; background-color:rgba(0,0,0,0.05); padding:0 8px; margin-bottom: 10px;\">\n<!-- ************** STATE ************** -->\n<div markdown=\"1\" id=\"sec:elim_state_div\" class=\"slideout\">\n<a id=\"sec:elim_state\"></a>\n### Eliminate a State\nLet us start at the last state, $x_2$. Gathering the two factors (marked in\nred [Figure 2a](#fig_eliminate_x_a){:onclick=\"currentSlide(1,0)\"}), we have \\eqref{eq:potential} the objective function $\\phi_1$, and \\eqref{eq:constrain} the constraint equation on $x_2$, $u_1$ and $x_1$:\n\n\\begin{equation} \\phi_1(x_2) = x_2^T Q x_2 \\label{eq:potential} \\end{equation}\n\n\\begin{equation} x_2 = Ax_1 + Bu_1 \\label{eq:constrain} \\end{equation}\n\nBy substituting $x_2$ from the dynamics constraint \\eqref{eq:constrain} into the objective function\n\\eqref{eq:potential}, we create a new factor representing\nthe cost of state $x_2$ as a function of $x_1$ and $u_1$:\n\n\\begin{equation} \\phi_2(x_1, u_1) = (Ax_1 + Bu_1)^T Q (Ax_1 + Bu_1)\n\\label{eq:potential_simplified} \\end{equation}\n\nThe resulting factor graph is illustrated in [Figure 2b](#fig_eliminate_x_b){:onclick=\"currentSlide(2,0)\"}.  Note that the \ndynamics constraint is now represented by the bayes net factors shown as gray arrows.\n\nTo summarize, we used the dynamics constraint to eliminate variable\n$x_2$ and the two factors marked in red, and we replaced them with a new binary cost factor on $x_1$\nand $u_1$, marked in blue. \n<p align=\"right\" style=\"background-color: rgba(0,0,0,0.1);\n    margin: 0px -8px 0 -8px;\n    padding: 0 8px;\"><a onclick=\"currentSlide(3,0)\">next >></a>\n</p>\n\n</div>\n<!-- ************** CONTROL ************** -->\n<div markdown=\"1\" id=\"sec:elim_ctrl_div\" class=\"slideout\">\n<a id=\"sec:elim_ctrl\"></a>\n### Eliminate a Control\n<!-- Now \\eqref{eq:potential_simplified} defines an (unnormalized) joint\nGaussian density on variables $x_1$ and $u_1$.  -->\nTo eliminate $u_1$, we seek to replace the two factors marked red in [Figure 3a](#fig_eliminate_u_a){:onclick=\"currentSlide(3,0)\"}\nwith a new cost factor on $x_1$ and an equation for the optimal control $$u_1^*(x_1)$$.\n\nAdding the control cost to \\eqref{eq:potential_simplified}, the combined cost of the\ntwo red factors in [Figure 3a](#fig_eliminate_u_a){:onclick=\"currentSlide(3,0)\"} is given by:\n\n\\begin{equation} \\phi_3(x_1, u_1) = u_1^TRu_1 + (Ax_1 + Bu_1)^T Q (Ax_1 + Bu_1)\n\\label{eq:potential_u1} \\end{equation}\n\n$\\phi_3$ is sometimes referred to as the *optimal action value function* and we seek to minimize it over $u_1$.\nWe do so by\nsetting the derivative of \\eqref{eq:potential_u1} wrt $u_1$ to zero\n<!-- (detailed calculation in the [Appendix](#eliminate-u_1)),  -->\nyielding the expression for the optimal control input $u_1^*$ as \n\n\\\\[ \\begin{align} \nu_1^*(x_1) &= \\argmin\\limits_{u_1}\\phi_3(x_1, u_1) \\nonumber \\\\\\\\ \n&= -(R+B^TQB)^{-1}B^TQAx_1 \\label{eq:control_law} \\\\\\\\ \n&= K_1x_1 \\nonumber\n\\end{align} \\\\]\n\nwhere $K_1\\coloneqq -(R+B^TQB)^{-1}B^TQA$.\n\nFinally, we substitute the expression of our optimal control, $$u_1^* = K_1x_1$$,\ninto our potential \\eqref{eq:potential_u1}\n<!-- (detailed calculation in the [Appendix](#marginalization-cost-on-x_1)) -->\nto obtain a new unary cost factor on $x_1$:\n\n\\begin{align}\n    \\phi_4(x_1) &= \\phi_3(x_1, u_1^*(x_1)) \\nonumber \\\\\\\\ \n        &= (K_1x_1)^T RK_1x_1 + (Ax_1 + BKx_1)^T Q (Ax_1 + BKx_1) \\nonumber \\\\\\\\ \n        &= x_1^T(A^TQA-K_1^TB^TQA)x_1 \\label{eq:potential_x1}\n\\end{align}\nNote that we simplified $K_1^TRK_1 + K_1^TB^TQBK_1 = -K_1^TB^TQA$ by substituting in for $K_1$ using\n\\eqref{eq:control_law}.\n\nThe resulting factor graph is illustrated in [Figure 3b](#fig_eliminate_u_b){:onclick=\"currentSlide(4,0)\"}.\n\nFor convenience, we will also define $P_k$ where $x_k^TP_kx_k$ represents the aggregate of the two unary costs on $x_k$.  In the case of $P_1$,\n\\begin{align}\n    x_1^TP_1x_1 &= x_1^TQx_1 + \\phi_4(x_1) \\nonumber\n\\end{align}\nis the aggregation of the two unary factors labeled in green in [Figure 3c](#fig_merge_factor){:onclick=\"currentSlide(5,0)\"}. \n<p style=\"background-color: rgba(0,0,0,0.1);\n    margin: 0px -8px 0 -8px;\n    padding: 0 8px;\">\n<span align=\"left\"><a onclick=\"currentSlide(2,0)\"><< prev</a><span style=\"float:right\"><a onclick=\"currentSlide(6,0)\">next >></a></span></span>\n</p>\n</div>\n<!-- ************** BAYES NET ************** -->\n<div markdown=\"1\" id=\"sec:elim_bayes_div\" class=\"slideout\">\n<a id=\"sec:elim_bayes\"></a>\n### Turning into a Bayes Network\nBy eliminating all the variables from right to left, we can get a Bayes network\nas shown in [Figure 4d](#fig_bayes_net){:onclick=\"currentSlide(9,0)\"}. Each time we eliminate a state\nand control, we simply repeat the steps in [Eliminate a state](#eliminate-a-state) and [Eliminate a control](#eliminate-a-control): we express the state $x_{k+1}$ with the dynamics model, then find the optimal control $u_k$ as\na function of state $x_k$.\n\nEliminating a general state, $x_{k+1}$, and control $u_k$, we obtain the recurrence relations:\n\n\\begin{equation} \\boxed{K_k = -(R+B^TP_{k+1}B)^{-1}B^TP_{k+1}A} \\label{eq:control_update_k} \\end{equation}\n\n\\begin{equation} \\boxed{P_k = Q+A^TP_{k+1}A - K_k^TB^TP_{k+1}A} \\label{eq:cost_update_k} \\end{equation}\n\nwith $P_{N}=Q$ is the cost at the last time step.\n\nThe final Bayes net in [Figure 4d](#fig_bayes_net){:onclick=\"currentSlide(9,0)\"} shows graphically the optimal control law:\n\\begin{equation} \\boxed{u^*_k = K_k x_k} \\end{equation}\n<p style=\"background-color: rgba(0,0,0,0.1);\n    margin: 0px -8px 0 -8px;\n    padding: 0 8px;\">\n<span align=\"left\"><a onclick=\"currentSlide(5,0)\"><< prev</a></span>\n</p>\n</div>\n</div> <!-- scrollablecontent -->\n<!-- ************************ END SCROLLABLE ELIMINATION DESCRIPTION ************************ -->\n\n<div class=\"slideshow-container\">\n  <div class=\"mySlides 0\" style=\"text-align: center;\">\n    <!-- <div class=\"numbertext\">2 / 3</div> -->\n    <a name=\"fig_eliminate_x_a\"></a>\n    <figure class=\"center\">\n    <img src=\"/_static/lqr_control/Elimination/cropped_Slide1.png\"\n        alt=\"Elimination of state $x_2$\" />\n        <figcaption><b>Figure 2a</b> Elimination of state $x_2$</figcaption>\n    </figure>\n  </div>\n  <div class=\"mySlides 0\" style=\"text-align: center;\">\n    <!-- <div class=\"numbertext\">2 / 3</div> -->\n    <a name=\"fig_eliminate_x_b\"></a>\n    <figure class=\"center\">\n    <img src=\"/_static/lqr_control/Elimination/cropped_Slide2.png\"\n        alt=\"Elimination of state $x_2$\" />\n        <figcaption><b>Figure 2b</b> Elimination of state $x_2$</figcaption>\n    </figure>\n  </div>\n  <div class=\"mySlides 0\" style=\"text-align: center;\">\n    <a name=\"fig_eliminate_u_a\"></a>\n    <figure class=\"center\">\n    <img src=\"/_static/lqr_control/Elimination/cropped_Slide3.png\"\n        alt=\"Elimination of state $u_1$\" />\n        <figcaption><b>Figure 3a</b> Elimination of state $u_1$</figcaption>\n    </figure>\n  </div>\n  <div class=\"mySlides 0\" style=\"text-align: center;\">\n    <a name=\"fig_eliminate_u_b\"></a>\n    <figure class=\"center\">\n    <img src=\"/_static/lqr_control/Elimination/cropped_Slide4.png\"\n        alt=\"Elimination of state $u_1$\" />\n        <figcaption><b>Figure 3b</b> Elimination of state $u_1$</figcaption>\n    </figure>\n  </div>\n  <div class=\"mySlides 0\" style=\"text-align: center;\">\n    <a name=\"fig_merge_factor\"></a>\n    <figure class=\"center\">\n    <img src=\"/_static/lqr_control/Elimination/cropped_Slide5.png\"\n        alt=\"Elimination of state $u_1$\" />\n        <figcaption><b>Figure 3c</b> Cost-to-go at $x_1$ is the sum of the two unary factors on $x_1$ (green)</figcaption>\n    </figure>\n  </div>\n  <div class=\"mySlides 0\" style=\"text-align: center;\">\n    <a name=\"fig_bayes_net\"></a>\n    <figure class=\"center\">\n    <img src=\"/_static/lqr_control/Elimination/cropped_Slide6.png\"\n        alt=\"Bayes net\" />\n        <figcaption><b>Figure 4a</b> Repeat elimination until the graph is reduced to a Bayes net</figcaption>\n    </figure>\n  </div>\n  <!-- <div class=\"mySlides 0\" style=\"text-align: center;\">\n    <a name=\"fig_bayes_net\"></a>\n    <figure class=\"center\">\n    <img src=\"/_static/lqr_control/Elimination/cropped_Slide7.png\"\n        alt=\"Bayes net\" />\n        <figcaption><b>Figure 4b</b> Repeat elimination until the graph is reduced to a Bayes net</figcaption>\n    </figure>\n  </div> -->\n  <div class=\"mySlides 0\" style=\"text-align: center;\">\n    <a name=\"fig_bayes_net\"></a>\n    <figure class=\"center\">\n    <img src=\"/_static/lqr_control/Elimination/cropped_Slide8.png\"\n        alt=\"Bayes net\" />\n        <figcaption><b>Figure 4b</b> Repeat elimination until the graph is reduced to a Bayes net</figcaption>\n    </figure>\n  </div>\n  <div class=\"mySlides 0\" style=\"text-align: center;\">\n    <a name=\"fig_bayes_net\"></a>\n    <figure class=\"center\">\n    <img src=\"/_static/lqr_control/Elimination/cropped_Slide10.png\"\n        alt=\"Bayes net\" />\n        <figcaption><b>Figure 4c</b> Repeat elimination until the graph is reduced to a Bayes net</figcaption>\n    </figure>\n  </div>\n  <div class=\"mySlides 0\" style=\"text-align: center;\">\n    <a name=\"fig_bayes_net\"></a>\n    <figure class=\"center\">\n    <img src=\"/_static/lqr_control/Elimination/cropped_Slide11.png\"\n        alt=\"Bayes net\" />\n        <figcaption><b>Figure 4d</b> Completed Bayes net</figcaption>\n    </figure>\n  </div>\n  <!-- Next and previous buttons -->\n  <a class=\"prev\" onclick=\"plusSlides(-1,0)\">&#10094;</a>\n  <a class=\"next\" onclick=\"plusSlides(1,0)\">&#10095;</a>\n\n</div> <!-- slideshow-container -->\n\n<!-- The dots/circles -->\n<div style=\"text-align:center\">\n  <span class=\"dot 0\" onclick=\"currentSlide(1,0)\"></span>\n  <span class=\"dot 0\" onclick=\"currentSlide(2,0)\"></span>\n  <span class=\"dot 0\" onclick=\"currentSlide(3,0)\"></span>\n  <span class=\"dot 0\" onclick=\"currentSlide(4,0)\"></span>\n  <span class=\"dot 0\" onclick=\"currentSlide(5,0)\"></span>\n  <span class=\"dot 0\" onclick=\"currentSlide(6,0)\"></span>\n  <span class=\"dot 0\" onclick=\"currentSlide(7,0)\"></span>\n  <span class=\"dot 0\" onclick=\"currentSlide(8,0)\"></span>\n  <span class=\"dot 0\" onclick=\"currentSlide(9,0)\"></span>\n  <!-- <span class=\"dot 0\" onclick=\"currentSlide(10,0)\"></span> -->\n</div>\n\n<!-- this css is to make the scroll bar disappear when the mouse isn't over the scrollable div...\nTaken from my website: https://github.com/gchenfc/gerrysworld2/blob/master/css/activity.css -->\n<style>\n.scrollablecontent::-webkit-scrollbar {\n    width: 5px;\n    height: 12px;\n}\n.scrollablecontent::-webkit-scrollbar-track {\n    background: transparent;\n}\n.scrollablecontent::-webkit-scrollbar-thumb {\n    background: #ddd;\n    visibility:hidden;\n}\n.scrollablecontent:hover::-webkit-scrollbar-thumb {\n    visibility:visible;\n}\n/* .slideout {\n  width: 400px;\n  height: auto;\n  overflow: hidden;\n  background: orange;\n  margin: 0 auto;\n  transition: height 0.4s linear;\n}\n.hide {\n    height: 0;\n} */\n</style>\n<!-- ********************** END VARIABLE ELIMINATION SLIDESHOW ********************** -->\n\n<!-- ************************ BEGIN SCROLLABLE ELIMINATION DESCRIPTION ************************ -->\n<div class=\"scrollablecontent\" markdown=\"1\" id=\"sec:elim_scrollable\"\n    style=\"overflow-x: hidden; background-color:rgba(0,0,0,0.05); padding:0 8px; margin-bottom: 10px;\">\n<!-- ************** STATE ************** -->\n<div markdown=\"1\" id=\"sec:elim_state_div\" class=\"slideout\">\n<a id=\"sec:elim_state\"></a>\n### Eliminate a State\nLet us start at the last state, $x_2$. Gathering the two factors (marked in\nred [Figure 2a](#fig_eliminate_x_a){:onclick=\"currentSlide(1,0)\"}), we have \\eqref{eq:potential} the objective function $\\phi_1$, and \\eqref{eq:constrain} the constraint equation on $x_2$, $u_1$ and $x_1$:\n\n\\begin{equation} \\phi_1(x_2) = x_2^T Q x_2 \\label{eq:potential} \\end{equation}\n\n\\begin{equation} x_2 = Ax_1 + Bu_1 \\label{eq:constrain} \\end{equation}\n\nBy substituting $x_2$ from the dynamics constraint \\eqref{eq:constrain} into the objective function\n\\eqref{eq:potential}, we create a new factor representing\nthe cost of state $x_2$ as a function of $x_1$ and $u_1$:\n\n\\begin{equation} \\phi_2(x_1, u_1) = (Ax_1 + Bu_1)^T Q (Ax_1 + Bu_1)\n\\label{eq:potential_simplified} \\end{equation}\n\nThe resulting factor graph is illustrated in [Figure 2b](#fig_eliminate_x_b){:onclick=\"currentSlide(2,0)\"}.  Note that the \ndynamics constraint is now represented by the bayes net factors shown as gray arrows.\n\nTo summarize, we used the dynamics constraint to eliminate variable\n$x_2$ and the two factors marked in red, and we replaced them with a new binary cost factor on $x_1$\nand $u_1$, marked in blue. \n<p align=\"right\" style=\"background-color: rgba(0,0,0,0.1);\n    margin: 0px -8px 0 -8px;\n    padding: 0 8px;\"><a onclick=\"currentSlide(3,0)\">next >></a>\n</p>\n\n</div>\n<!-- ************** CONTROL ************** -->\n<div markdown=\"1\" id=\"sec:elim_ctrl_div\" class=\"slideout\">\n<a id=\"sec:elim_ctrl\"></a>\n### Eliminate a Control\n<!-- Now \\eqref{eq:potential_simplified} defines an (unnormalized) joint\nGaussian density on variables $x_1$ and $u_1$.  -->\nTo eliminate $u_1$, we seek to replace the two factors marked red in [Figure 3a](#fig_eliminate_u_a){:onclick=\"currentSlide(3,0)\"}\nwith a new cost factor on $x_1$ and an equation for the optimal control $$u_1^*(x_1)$$.\n\nAdding the control cost to \\eqref{eq:potential_simplified}, the combined cost of the\ntwo red factors in [Figure 3a](#fig_eliminate_u_a){:onclick=\"currentSlide(3,0)\"} is given by:\n\n\\begin{equation} \\phi_3(x_1, u_1) = u_1^TRu_1 + (Ax_1 + Bu_1)^T Q (Ax_1 + Bu_1)\n\\label{eq:potential_u1} \\end{equation}\n\n$\\phi_3$ is sometimes referred to as the *optimal action value function* and we seek to minimize it over $u_1$.\nWe do so by\nsetting the derivative of \\eqref{eq:potential_u1} wrt $u_1$ to zero\n<!-- (detailed calculation in the [Appendix](#eliminate-u_1)),  -->\nyielding the expression for the optimal control input $u_1^*$ as \n\n\\\\[ \\begin{align} \nu_1^*(x_1) &= \\argmin\\limits_{u_1}\\phi_3(x_1, u_1) \\nonumber \\\\\\\\ \n&= -(R+B^TQB)^{-1}B^TQAx_1 \\label{eq:control_law} \\\\\\\\ \n&= K_1x_1 \\nonumber\n\\end{align} \\\\]\n\nwhere $K_1\\coloneqq -(R+B^TQB)^{-1}B^TQA$.\n\nFinally, we substitute the expression of our optimal control, $$u_1^* = K_1x_1$$,\ninto our potential \\eqref{eq:potential_u1}\n<!-- (detailed calculation in the [Appendix](#marginalization-cost-on-x_1)) -->\nto obtain a new unary cost factor on $x_1$:\n\n\\begin{align}\n    \\phi_4(x_1) &= \\phi_3(x_1, u_1^*(x_1)) \\nonumber \\\\\\\\ \n        &= (K_1x_1)^T RK_1x_1 + (Ax_1 + BKx_1)^T Q (Ax_1 + BKx_1) \\nonumber \\\\\\\\ \n        &= x_1^T(A^TQA-K_1^TB^TQA)x_1 \\label{eq:potential_x1}\n\\end{align}\nNote that we simplified $K_1^TRK_1 + K_1^TB^TQBK_1 = -K_1^TB^TQA$ by substituting in for $K_1$ using\n\\eqref{eq:control_law}.\n\nThe resulting factor graph is illustrated in [Figure 3b](#fig_eliminate_u_b){:onclick=\"currentSlide(4,0)\"}.\n\nFor convenience, we will also define $P_k$ where $x_k^TP_kx_k$ represents the aggregate of the two unary costs on $x_k$.  In the case of $P_1$,\n\\begin{align}\n    x_1^TP_1x_1 &= x_1^TQx_1 + \\phi_4(x_1) \\nonumber\n\\end{align}\nis the aggregation of the two unary factors labeled in green in [Figure 3c](#fig_merge_factor){:onclick=\"currentSlide(5,0)\"}. \n<p style=\"background-color: rgba(0,0,0,0.1);\n    margin: 0px -8px 0 -8px;\n    padding: 0 8px;\">\n<span align=\"left\"><a onclick=\"currentSlide(2,0)\"><< prev</a><span style=\"float:right\"><a onclick=\"currentSlide(6,0)\">next >></a></span></span>\n</p>\n</div>\n<!-- ************** BAYES NET ************** -->\n<div markdown=\"1\" id=\"sec:elim_bayes_div\" class=\"slideout\">\n<a id=\"sec:elim_bayes\"></a>\n### Turning into a Bayes Network\nBy eliminating all the variables from right to left, we can get a Bayes network\nas shown in [Figure 4d](#fig_bayes_net){:onclick=\"currentSlide(9,0)\"}. Each time we eliminate a state\nand control, we simply repeat the steps in [Eliminate a state](#eliminate-a-state) and [Eliminate a control](#eliminate-a-control): we express the state $x_{k+1}$ with the dynamics model, then find the optimal control $u_k$ as\na function of state $x_k$.\n\nEliminating a general state, $x_{k+1}$, and control $u_k$, we obtain the recurrence relations:\n\n\\begin{equation} \\boxed{K_k = -(R+B^TP_{k+1}B)^{-1}B^TP_{k+1}A} \\label{eq:control_update_k} \\end{equation}\n\n\\begin{equation} \\boxed{P_k = Q+A^TP_{k+1}A - K_k^TB^TP_{k+1}A} \\label{eq:cost_update_k} \\end{equation}\n\nwith $P_{N}=Q$ is the cost at the last time step.\n\nThe final Bayes net in [Figure 4d](#fig_bayes_net){:onclick=\"currentSlide(9,0)\"} shows graphically the optimal control law:\n\\begin{equation} \\boxed{u^*_k = K_k x_k} \\end{equation}\n<p style=\"background-color: rgba(0,0,0,0.1);\n    margin: 0px -8px 0 -8px;\n    padding: 0 8px;\">\n<span align=\"left\"><a onclick=\"currentSlide(5,0)\"><< prev</a></span>\n</p>\n</div>\n</div> <!-- scrollablecontent -->\n<!-- ************************ END SCROLLABLE ELIMINATION DESCRIPTION ************************ -->","type":"content","url":"/content/blogs/2019/2019-11-07-lqr-control#variable-elimination","position":5},{"hierarchy":{"lvl1":"LQR Control Using Factor Graphs","lvl2":"Intuition"},"type":"lvl2","url":"/content/blogs/2019/2019-11-07-lqr-control#intuition","position":6},{"hierarchy":{"lvl1":"LQR Control Using Factor Graphs","lvl2":"Intuition"},"content":" ************** Value Function ************** \n\n\n\nFigure 5 Example LQR control solutions as solved by factor graphs (middle) and the traditional Discrete Algebraic Ricatti Equations (right). The optimal control gains and cost-to-go factors are compared (left). All plots show exact agreement between factor graph and Ricatti equation solutions.\n\n\n\nWe introduce the cost-to-go (also known as return cost, optimal state value function, or simply value function) as V_k(x) \\coloneqq x^TP_kx which intuitively represents the total cost that will be accrued from here on out, assuming optimal control.\n\nIn our factor graph representation, it is becomes obvious that V_k(x) corresponds to the total cost at and after the state x_k assuming optimal control because we eliminate variables backwards in time with the objective of minimizing cost.\nEliminating a state just re-expresses the future cost in terms of prior states/controls.  Each time we eliminate a control, u, the future cost is recalculated assuming optimal control (i.e. \\phi_4(x) = \\phi_3(x, u^*)).\n\nThis “cost-to-go” is depicted as a heatmap in \n\nFigure 5.\nThe heat maps depict the V_k showing that the cost is high when x is far from 0, but also showing that after iterating sufficient far backwards in time, V_k(x) begins to converge.  That is to say, the V_0(x) is very similar for N=30 and N=100.\nSimilarly, the leftmost plot of \n\nFigure 5 depicts K_k and P_k and shows that they (predictably) converge as well.\n\nThis convergence allows us to see that we can extend to the \n\ninfinite horizon LQR problem (continued in the next section). The factor graph representation also gives us insight to the equation for the optimal gain matrix $K_k$ from\n\\eqref{eq:control_update_k}.\nThe optimal control, $K_k$, should attempt to balance (a) the unary factor $u_k^TRu_k$ representing the cost of executing a control action and (b) the binary factor $(Ax_k+Bu_k)^TP_{k+1}(Ax_k+Bu_k)$ representing the future cost of the control action.\n\nThe binary factor consists of two terms\nrepresents a balance between achieving a small \"cost-to-go\" next time step ($B^TP_{k+1}B$) and exerting a small\namount of control this time step ($R$). ","type":"content","url":"/content/blogs/2019/2019-11-07-lqr-control#intuition","position":7},{"hierarchy":{"lvl1":"LQR Control Using Factor Graphs","lvl2":"Equivalence to the Ricatti Equation"},"type":"lvl2","url":"/content/blogs/2019/2019-11-07-lqr-control#equivalence-to-the-ricatti-equation","position":8},{"hierarchy":{"lvl1":"LQR Control Using Factor Graphs","lvl2":"Equivalence to the Ricatti Equation"},"content":"In traditional descriptions of discrete, finite-horizon LQR (i.e. \n\nChow, \n\nKirk, \n\nStanford), the control law and cost function are given by\n\n\\[ u_k = K_kx_k \\]K_k = -(R+B^TP_{k+1}B)^{-1}B^TP_{k+1}AP_k = Q+A^TP_{k+1}A - K_k^TB^TP_{k+1}A\n\nwith P_k commonly referred to as the solution to the dynamic Ricatti equation and P_N=Q is the\nvalue of the Ricatti function at the final time step.\n\\eqref{eq:control_update_k_ricatti} and \\eqref{eq:cost_update_k_ricatti} correspond to\nthe same results as we derived in \\eqref{eq:control_update_k} and \\eqref{eq:cost_update_k}\nrespectively.\n\nRecall that P_0 and K_0 appear to converge as the number of time steps grows.  They will approach a stationary solution to the equations\\begin{align}\nK &= -(R+B^TPB)^{-1}B^TPA \\nonumber \\\\\\\\ \nP &= Q+A^TPA - K^TB^TPA \\nonumber\n\\end{align}\n\nas N\\to\\infty.  This is the \n\nDiscrete Algebraic Ricatti Equations (DARE) and \\lim_{N\\to\\infty}V_0(x) and \\lim_{N\\to\\infty}K_0 are the cost-to-go and optimal control gain respectively for the \n\ninfinite horizon LQR problem.  Indeed, one way to calculate the solution to the DARE is to iterate on the dynamic Ricatti equation.","type":"content","url":"/content/blogs/2019/2019-11-07-lqr-control#equivalence-to-the-ricatti-equation","position":9},{"hierarchy":{"lvl1":"LQR Control Using Factor Graphs","lvl2":"Implementation using GTSAM"},"type":"lvl2","url":"/content/blogs/2019/2019-11-07-lqr-control#implementation-using-gtsam","position":10},{"hierarchy":{"lvl1":"LQR Control Using Factor Graphs","lvl2":"Implementation using GTSAM"},"content":"**Edit (Apr 17, 2021): Code updated to new Python wrapper as of GTSAM 4.1.0.\n\nYou can view an example Jupyter notebook on \n\ngoogle colab{:target=“_blank”} or\n\n\ndownload the modules/examples\nthat you can use in your\nprojects to:\n\nCalculate the closed loop gain matrix, K, using GTSAM\n\nCalculate the “cost-to-go” matrix, P (which is equivalent to the solutions to\nthe dynamic Ricatti equation), using GTSAM\n\nCalculate the LQR solution for a non-zero, non-constant goal position, using GTSAM\n\nVisualize the cost-to-go and how it relates to factor graphs and the Ricatti\nequation\n\nand more!\n\nA brief example of the open-loop finite horizon LQR problem using\nfactor graphs is shown below:\n\ndef solve_lqr(A, B, Q, R, X0=np.array([0., 0.]), num_time_steps=500):\n    '''Solves a discrete, finite horizon LQR problem given system dynamics in\n    state space representation.\n    Arguments:\n        A, B: nxn state transition matrix and nxp control input matrix\n        Q, R: nxn state cost matrix and pxp control cost matrix\n        X0: initial state (n-vector)\n        num_time_steps: number of time steps, T\n    Returns:\n        x_sol, u_sol: Txn array of states and Txp array of controls\n    '''\n    n = np.size(A, 0)\n    p = np.size(B, 1)\n\n    # noise models\n    prior_noise = gtsam.noiseModel.Constrained.All(n)\n    dynamics_noise = gtsam.noiseModel.Constrained.All(n)\n    q_noise = gtsam.noiseModel.Gaussian.Information(Q)\n    r_noise = gtsam.noiseModel.Gaussian.Information(R)\n\n    # Create an empty Gaussian factor graph\n    graph = gtsam.GaussianFactorGraph()\n\n    # Create the keys corresponding to unknown variables in the factor graph\n    X = []\n    U = []\n    for k in range(num_time_steps):\n        X.append(gtsam.symbol('x', k))\n        U.append(gtsam.symbol('u', k))\n\n    # set initial state as prior\n    graph.add(X[0], np.eye(n), X0, prior_noise)\n\n    # Add dynamics constraint as ternary factor\n    #   A.x1 + B.u1 - I.x2 = 0\n    for k in range(num_time_steps-1):\n        graph.add(X[k], A, U[k], B, X[k+1], -np.eye(n),\n                  np.zeros((n)), dynamics_noise)\n\n    # Add cost functions as unary factors\n    for x in X:\n        graph.add(x, np.eye(n), np.zeros(n), q_noise)\n    for u in U:\n        graph.add(u, np.eye(p), np.zeros(p), r_noise)\n\n    # Solve\n    result = graph.optimize()\n    x_sol = np.zeros((num_time_steps, n))\n    u_sol = np.zeros((num_time_steps, p))\n    for k in range(num_time_steps):\n        x_sol[k, :] = result.at(X[k])\n        u_sol[k] = result.at(U[k])\n    \n    return x_sol, u_sol","type":"content","url":"/content/blogs/2019/2019-11-07-lqr-control#implementation-using-gtsam","position":11},{"hierarchy":{"lvl1":"LQR Control Using Factor Graphs","lvl2":"Future Work"},"type":"lvl2","url":"/content/blogs/2019/2019-11-07-lqr-control#future-work","position":12},{"hierarchy":{"lvl1":"LQR Control Using Factor Graphs","lvl2":"Future Work"},"content":"The factor graph \n\n(Figure 1) for our finite horizon discrete LQR problem can be readily extended to LQG, iLQR, DDP, and reinforcement\nlearning using non-deterministic dynamics factors, nonlinear factors, discrete factor graphs, and other features of GTSAM (stay tuned for future posts). ********************************** APPENDIX ********************************** ","type":"content","url":"/content/blogs/2019/2019-11-07-lqr-control#future-work","position":13},{"hierarchy":{"lvl1":"LQR Control Using Factor Graphs","lvl2":"Appendix"},"type":"lvl2","url":"/content/blogs/2019/2019-11-07-lqr-control#appendix","position":14},{"hierarchy":{"lvl1":"LQR Control Using Factor Graphs","lvl2":"Appendix"},"content":" ### Marginalization Cost on $x_1$\nBy substituting \\eqref{eq:control_law} into \\eqref{eq:potential_simplified}, we have the updated\npotential function as a function of only $x_1$:\n\\\\[ \\begin{aligned} \n    \\phi_1(x_1) &= x_1^T Q x_1 + (K_1x_1)^T RK_1x_1 + (Ax_1 + BKx_1)^T Q (Ax_1 + BKx_1) \\\\\\\\ \n    &= x_1^T(Q+ K_1^TRK_1 + A^TQA + K_1^TB^TQB - K_1^TB^TQA - A^TQBK_1)x_1  \\\\\\\\ \n    &= x_1^T[Q + A^TQA + K_1^T(R+B^TQB)K_1 - K_1^TB^TQA - A^TQBK_1]x_1 \\\\\\\\ \n    &= x_1^T(Q + A^TQA + A^TQBK_1 - K_1^TB^TQA - A^TQBK_1)x_1 \\\\\\\\ \n    &= x_1^T(Q + A^TQA - K_1^TB^TQA)x_1 \n\\end{aligned} \\\\] ","type":"content","url":"/content/blogs/2019/2019-11-07-lqr-control#appendix","position":15},{"hierarchy":{"lvl1":"LQR Control Using Factor Graphs","lvl3":"Least Squares Implementation in GTSAM","lvl2":"Appendix"},"type":"lvl3","url":"/content/blogs/2019/2019-11-07-lqr-control#least-squares-implementation-in-gtsam","position":16},{"hierarchy":{"lvl1":"LQR Control Using Factor Graphs","lvl3":"Least Squares Implementation in GTSAM","lvl2":"Appendix"},"content":"GTSAM can be specified to use either of two methods for solving the least squares problems that\nappear in eliminating factor graphs: Cholesky Factorization or QR Factorization.  Both arrive at the same result, but we will take a look at QR since it more immediately illustrates the elimination algorithm at work. plain table for formatting purposes ","type":"content","url":"/content/blogs/2019/2019-11-07-lqr-control#least-squares-implementation-in-gtsam","position":17},{"hierarchy":{"lvl1":"LQR Control Using Factor Graphs","lvl4":"QR Factorization","lvl3":"Least Squares Implementation in GTSAM","lvl2":"Appendix"},"type":"lvl4","url":"/content/blogs/2019/2019-11-07-lqr-control#qr-factorization","position":18},{"hierarchy":{"lvl1":"LQR Control Using Factor Graphs","lvl4":"QR Factorization","lvl3":"Least Squares Implementation in GTSAM","lvl2":"Appendix"},"content":"\n\n\n\nFigure 6a Initial factor graph and elimination matrix\n\n\\\\( \\begin{array}{cc} \\text{NM} & \\text{Elimination Matrix} \\\\\\\\ \\left[ \\begin{array}{c} \\vphantom{Q^{1/2} | } I\\\\\\\\ \\vphantom{I | } 0\\\\\\\\ \\vphantom{R^{1/2} | } I\\\\\\\\ \\vphantom{Q^{1/2} | } I\\\\\\\\ \\vphantom{-A | } 0\\\\\\\\ \\vphantom{R^{1/2} | } I\\\\\\\\ \\vphantom{Q^{1/2} | } I \\end{array} \\right] & \\left[ \\begin{array}{ccccc|c} Q^{1/2} & & & & & 0\\\\\\\\ I & -B & -A & & & 0\\\\\\\\ & R^{1/2} & & & & 0\\\\\\\\ & & Q^{1/2}& & & 0\\\\\\\\ & & I & -B & -A & 0\\\\\\\\ & & & R^{1/2}& & 0\\\\\\\\ & & & & Q^{1/2}& 0 \\end{array} \\right] \\end{array} \\\\)\n\n\n\nFigure 6b Eliminate $x_2$: the two factors to replace are highlighted in red\n\n\\\\( \\begin{array}{cc} \\text{NM} & \\text{Elimination Matrix} \\\\\\\\ \\begin{bmatrix} \\vphantom{Q^{1/2} | } I\\\\\\\\ \\vphantom{I | } 0\\\\\\\\ \\vphantom{R^{1/2} | } I\\\\\\\\ \\vphantom{Q^{1/2} | } I\\\\\\\\ \\vphantom{I | } 0\\\\\\\\ \\vphantom{R^{1/2} | } I\\\\\\\\ \\vphantom{Q^{1/2} | } I \\end{bmatrix} & \\left[ \\begin{array}{ccccc|c} \\color{red} {Q^{1/2}} & & & & & 0\\\\\\\\ \\color{red} I & \\color{red} {-B} & \\color{red} {-A} & & & 0\\\\\\\\ & R^{1/2} & & & & 0\\\\\\\\ & & Q^{1/2}& & & 0\\\\\\\\ & & I & -B & -A & 0\\\\\\\\ & & & R^{1/2}& & 0\\\\\\\\ & & & & Q^{1/2}& 0 \\end{array} \\right] \\end{array} \\\\)\n\n\n\nFigure 6c Eliminated $x_2$: the resulting binary cost factor is highlighted in blue\n\n\\\\( \\begin{array}{cc} \\text{NM} & \\text{Elimination Matrix} \\\\\\\\ \\begin{bmatrix} \\vphantom{I | } 0\\\\\\\\ \\vphantom{Q^{1/2} | } I\\\\\\\\ \\vphantom{R^{1/2} | } I\\\\\\\\ \\vphantom{Q^{1/2} | } I\\\\\\\\ \\vphantom{I | } 0\\\\\\\\ \\vphantom{R^{1/2} | } I\\\\\\\\ \\vphantom{Q^{1/2} | } I\\\\\\\\ \\end{bmatrix} & \\left[ \\begin{array}{c:cccc|c} I & -B & -A & & & 0\\\\\\\\ \\hdashline & \\color{blue} {Q^{1/2}B} & \\color{blue} {Q^{1/2}A} & & & 0\\\\\\\\ & R^{1/2} & & & & 0\\\\\\\\ & & Q^{1/2}& & & 0\\\\\\\\ & & I & -B & -A & 0\\\\\\\\ & & & R^{1/2}& & 0\\\\\\\\ & & & & Q^{1/2}& 0 \\end{array} \\right] \\end{array} \\\\)\n\n\n\nFigure 6d Eliminate $u_1$: the two factors to replace are highlighted in red\n\n\\\\( \\begin{array}{cc} \\text{NM} & \\text{Elimination Matrix} \\\\\\\\ \\begin{bmatrix} \\vphantom{I | } 0\\\\\\\\ \\vphantom{Q^{1/2} | } I\\\\\\\\ \\vphantom{R^{1/2} | } I\\\\\\\\ \\vphantom{Q^{1/2} | } I\\\\\\\\ \\vphantom{I | } 0\\\\\\\\ \\vphantom{R^{1/2} | } I\\\\\\\\ \\vphantom{Q^{1/2} | } I\\\\\\\\ \\end{bmatrix} & \\left[ \\begin{array}{c:cccc|c} I & -B & -A & & & 0\\\\\\\\ \\hdashline & \\color{red} {Q^{1/2}B} & \\color{red} {Q^{1/2}A} & & & 0\\\\\\\\ & \\color{red} {R^{1/2}} & & & & 0\\\\\\\\ & & {Q^{1/2}} & & & 0\\\\\\\\ & & I & -B & -A & 0\\\\\\\\ & & & R^{1/2}& & 0\\\\\\\\ & & & & Q^{1/2}& 0 \\end{array} \\right] \\end{array} \\\\)\n\n\n\nFigure 6e Eliminated $u_1$: the resulting unary cost factor on $x_1$ is shown in blue\n\n\\\\( \\begin{array}{cc} \\text{NM} & \\text{Elimination Matrix} \\\\\\\\ \\begin{bmatrix} \\vphantom{I | } 0\\\\\\\\ \\vphantom{D_1^{1/2} | } I\\\\\\\\ \\vphantom{(P_1-Q)^{1/2} | } I\\\\\\\\ \\vphantom{Q^{1/2} | } I\\\\\\\\ \\vphantom{I | } 0\\\\\\\\ \\vphantom{R^{1/2} | } I\\\\\\\\ \\vphantom{Q^{1/2} | } I\\\\\\\\ \\end{bmatrix} & \\left[ \\begin{array}{cc:ccc|c} I & -B & -A & & & 0\\\\\\\\ & D_1^{1/2} & -D_1^{1/2}K_1 & & & 0\\\\\\\\ \\hdashline & & \\color{blue} {(P_1-Q)^{1/2}} & & & 0\\\\\\\\ & & Q^{1/2} & & & 0\\\\\\\\ & & I & -B & -A & 0\\\\\\\\ & & & R^{1/2}& & 0\\\\\\\\ & & & & Q^{1/2}& 0 \\end{array} \\right] \\end{array} \\\\)\n\n\n\nFigure 6f Eliminate $x_1$: the three factors to replace are highlighted in red\n\n\\\\( \\begin{array}{cc} \\text{NM} & \\text{Elimination Matrix} \\\\\\\\ \\begin{bmatrix} \\vphantom{I | } 0\\\\\\\\ \\vphantom{D_1^{1/2} | } I\\\\\\\\ \\vphantom{(P_1-Q)^{1/2} | } I\\\\\\\\ \\vphantom{Q^{1/2} | } I\\\\\\\\ \\vphantom{I | } 0\\\\\\\\ \\vphantom{R^{1/2} | } I\\\\\\\\ \\vphantom{Q^{1/2} | } I\\\\\\\\ \\end{bmatrix} & \\left[ \\begin{array}{cc:ccc|c} I & -B & -A & & & 0\\\\\\\\ & D_1^{1/2} & -D_1^{1/2}K_1 & & & 0\\\\\\\\ \\hdashline & & \\color{red} {(P_1-Q)^{1/2}} & & & 0\\\\\\\\ & & \\color{red} {Q^{1/2}} & & & 0\\\\\\\\ & & \\color{red} I & \\color{red} {-B} & \\color{red} {-A} & 0\\\\\\\\ & & & R^{1/2}& & 0\\\\\\\\ & & & & Q^{1/2}& 0 \\end{array} \\right] \\end{array} \\\\)\n\n\n\nFigure 6g Eliminated $x_1$: the resulting binary cost factor is shown in blue\n\n\\\\( \\begin{array}{cc} \\text{NM} & \\text{Elimination Matrix} \\\\\\\\ \\begin{bmatrix} \\vphantom{I | } 0\\\\\\\\ \\vphantom{D_1^{1/2}K_1 | } I\\\\\\\\ \\vphantom{I | } 0\\\\\\\\ \\vphantom{D_0^{1/2} | } I\\\\\\\\ \\vphantom{P^{1/2} | } I\\\\\\\\ \\vphantom{Q^{1/2} | } I\\\\\\\\ \\end{bmatrix} & \\left[ \\begin{array}{ccc:cc|c} I & -B & -A & & & 0\\\\\\\\ & D_1^{1/2} & -D_1^{1/2}K_1& & & 0\\\\\\\\ & & I & -B & -A & 0\\\\\\\\ \\hdashline & & &\\color{blue} {P_1^{1/2}B} & \\color{blue} {P_1^{1/2}A} & 0\\\\\\\\ & & & R^{1/2}& & 0\\\\\\\\ & & & & Q^{1/2}& 0 \\end{array} \\right] \\end{array} \\\\)\n\n\n\nFigure 6h Eliminate $u_0$: the two cost factors to replace are shown in red\n\n\\\\( \\begin{array}{cc} \\text{NM} & \\text{Elimination Matrix} \\\\\\\\ \\begin{bmatrix} \\vphantom{I | } 0\\\\\\\\ \\vphantom{D_1^{1/2}K_1 | } I\\\\\\\\ \\vphantom{I | } 0\\\\\\\\ \\vphantom{D_0^{1/2} | } I\\\\\\\\ \\vphantom{P^{1/2} | } I\\\\\\\\ \\vphantom{Q^{1/2} | } I\\\\\\\\ \\end{bmatrix} & \\left[ \\begin{array}{ccc:cc|c} I & -B & -A & & & 0\\\\\\\\ & D_1^{1/2} & -D_1^{1/2}K_1& & & 0\\\\\\\\ & & I & -B & -A & 0\\\\\\\\ \\hdashline & & &\\color{red} {P_1^{1/2}B} &\\color{red} {P_1^{1/2}A} & 0\\\\\\\\ & & &\\color{red} {R^{1/2}} & & 0\\\\\\\\ & & & & Q^{1/2} & 0 \\end{array} \\right] \\end{array} \\\\)\n\n\n\nFigure 6i Eliminated $u_0$: the resulting unary cost factor on $x_0$ is shown in blue.\n\n\\\\( \\begin{array}{cc} \\text{NM} & \\text{Elimination Matrix} \\\\\\\\ \\begin{bmatrix} \\vphantom{I | } 0\\\\\\\\ \\vphantom{D_1^{1/2}K_1 | } I\\\\\\\\ \\vphantom{I | } 0\\\\\\\\ \\vphantom{D_0^{1/2} | } I\\\\\\\\ \\vphantom{(P_0-Q)^{1/2} | } I\\\\\\\\ \\vphantom{P^{1/2} | } I\\\\\\\\ \\end{bmatrix} & \\left[ \\begin{array}{cccc:c|c} I & -B & -A & & & 0\\\\\\\\ & D_1^{1/2} & -D_1^{1/2}K_1 & & & 0\\\\\\\\ & & I & -B & -A & 0\\\\\\\\ & & & D_0^{1/2} & -D_0^{1/2}K_0 & 0\\\\\\\\ \\hdashline & & & & \\color{blue} {(P_0-Q)^{1/2}} & 0\\\\\\\\ & & & & Q^{1/2} & 0\\\\\\\\ \\end{array} \\right] \\end{array} \\\\)\n\n\n\nFigure 6j Eliminate $x_0$: the final two factors to eliminate are shown in red.\n\n\\\\( \\begin{array}{cc} \\text{NM} & \\text{Elimination Matrix} \\\\\\\\ \\begin{bmatrix} \\vphantom{I | } 0\\\\\\\\ \\vphantom{D_1^{1/2}K_1 | } I\\\\\\\\ \\vphantom{I | } 0\\\\\\\\ \\vphantom{D_0^{1/2} | } I\\\\\\\\ \\vphantom{(P_0-Q)^{1/2} | } I\\\\\\\\ \\vphantom{P^{1/2} | } I\\\\\\\\ \\end{bmatrix} & \\left[ \\begin{array}{cccc:c|c} I & -B & -A & & & 0\\\\\\\\ & D_1^{1/2} & -D_1^{1/2}K_1 & & & 0\\\\\\\\ & & I & -B & -A & 0\\\\\\\\ & & & D_0^{1/2} & -D_0^{1/2}K_0 & 0\\\\\\\\ \\hdashline & & & & \\color{red} {(P_0-Q)^{1/2}} & 0\\\\\\\\ & & & & \\color{red} {Q^{1/2}} & 0\\\\\\\\ \\end{array} \\right] \\end{array} \\\\)\n\n\n\nFigure 6k Final result: after eliminating $x_0$, the elimination matrix is upper-triangular and we can read off the control laws.\n\n\\\\( \\begin{array}{cc} \\text{NM} & \\text{Elimination Matrix} \\\\\\\\ \\begin{bmatrix} \\vphantom{I | } 0\\\\\\\\ \\vphantom{D_1^{1/2}K_1 | } I\\\\\\\\ \\vphantom{I | } 0\\\\\\\\ \\vphantom{D_0^{1/2} | } I\\\\\\\\ \\vphantom{P_0^{1/2} | } I\\\\\\\\ \\end{bmatrix} & \\left[ \\begin{array}{cccc:c|c} I & -B & -A & & & 0\\\\\\\\ & D_1^{1/2} & -D_1^{1/2}K_1 & & & 0\\\\\\\\ & & I & -B & -A & 0\\\\\\\\ & & & D_0^{1/2} & -D_0^{1/2}K_0 & 0\\\\\\\\ \\hdashline & & & & \\color{blue} {P_0^{1/2}} & 0\\\\\\\\ \\end{array} \\right] \\end{array} \\\\)\n\n❮ \n\n❯ slideshow-container  ************************ QR block elimination ************************  Slideshow container, based on https://www.w3schools.com/howto/howto_js_slideshow.asp  Next and previous buttons  The dots/circles \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwhere\n\n$P_{k}$\n\n$=$\n\n$Q + A^TP_{k+1}A - K_k^TB^TP_{k+1}A$\n\n($P_2=Q$)\n\n\n\n$D_{k}$\n\n$=$\n\n$R + B^TP_{k+1}B$\n\n\n\n$K_k$\n\n$=$\n\n$-D_{k}^{-1/2}(R + B^TP_{k+1}B)^{-T/2}B^TP_{k+1}A$\n\n\n\n\n\n$=$\n\n$-(R + B^TP_{k+1}B)^{-1}B^TP_{k+1}A$ ************************ end QR block elimination ************************ \n\nThe factorization process is illustrated in \n\nFigure 6 for a 3-time step factor graph, where the noise matrices and elimination\nmatrices are shown with the corresponding states of the graph.  The noise matrix (NM) is 0 for a\nhard constraint and I for a minimization objective.  The elimination matrix is formatted as an\naugmented matrix [A|b] for the linear least squares problem \\argmin\\limits_x\\|\\|Ax-b\\|\\|_2^2\nwith {x=[x_2;u_1;x_1;u_0;x_0]} is the vertical concatenation of all state and control vectors.\nThe recursive expressions for P, D, and K when eliminating control variables (i.e. u_1 in \n\nFigure 6e{:onclick=“currentSlide(5,1)”}) are derived from block QR Factorization.\n\nNote that all b_i=0 in the augmented matrix for the LQR problem of finding minimal control to\nreach state 0, but simply changing values of b_i intuitively extends GTSAM to solve\nLQR problems whose objectives are to reach different states or even follow trajectories. ### Final Symbolic Expressions of Factor Graph Evaluation\nIn the above solution, we have\n\\\\[ \\begin{aligned} \nK_1 &= -(R+B^TQB)^{-1}B^TQA\\\\\\\\ \nP_1 &= Q+A^TQA + A^TQBK_1\\\\\\\\ \nK_0 &= -(R+B^TV_1B)^{-1}B^TV_1A\\\\\\\\ \nP_0 &= Q + A^T V_1 A + A^T V_1 B K_0\n\\end{aligned} \\\\]\n\nIn general, the above factor graph and solution method can be expanded for an arbitrary number of time steps, $T$, arising in the iterative equations\n\\\\[ \\begin{aligned} \n    V_T &= Q \\\\\\\\ \n    K_t &= -( R + B^T V_{t+1} B )^{-1} B^T V_{t+1} A \\\\\\\\ \n    P_t &= Q + A^T V_{t+1} A + A^T V_{t+1} B K_t \n\\end{aligned} \\\\]\nand\n\\\\[ \\begin{aligned} \n    u_t &= K_t x_t\n\\end{aligned} \\\\]\nwhich match the traditional algorithm using the Ricatti Equation for solving the finite-horizon discrete-time LQR problem.  As the number\nof time steps grows, the solution for $V_0$ approaches the stationary solution to the algebraic\nRicatti equation and the solution for $K_0$ approaches the solution to the infinite-horizon\ndiscrete-time LQR problem.  **************** JAVASCRIPT FOR SLIDESHOWS **************** ","type":"content","url":"/content/blogs/2019/2019-11-07-lqr-control#qr-factorization","position":19},{"hierarchy":{"lvl1":"What are Factor Graphs?"},"type":"lvl1","url":"/content/blogs/2020/2020-06-01-factor-graphs","position":0},{"hierarchy":{"lvl1":"What are Factor Graphs?"},"content":"","type":"content","url":"/content/blogs/2020/2020-06-01-factor-graphs","position":1},{"hierarchy":{"lvl1":"What are Factor Graphs?","lvl3":"By Frank Dellaert,  on Twitter"},"type":"lvl3","url":"/content/blogs/2020/2020-06-01-factor-graphs#by-frank-dellaert-on-twitter","position":2},{"hierarchy":{"lvl1":"What are Factor Graphs?","lvl3":"By Frank Dellaert,  on Twitter"},"content":"[Cross-posting from our new sister-site, \n\nOpenSAM.org]\n\nMany computational problems in robotics have an optimization problem at their core. For example, in simultaneous localization and mapping (SLAM) and many other estimation problems we are after a maximum a posteriori estimate, i.e., we try to maximize posterior probability of the variables given a set of measurements. When attempting to act optimally, we try to maximize a performance index, or conversely minimize a penalty function. And even in classical planning, we are trying to find an assignment to a set of discrete variables that minimizes the plan length or optimizes for some other desirable property of the plan.\n\nIn most of these optimization problems, the objective to be maximized or minimized is composed of many different factors or terms that typically are local in nature, i.e., they only depend on a small subset of the entire set of variables. For example, in a tracking application, a particular video frame only provides information about the position of a target at a particular time. At the next time step, a different variable is associated with the target. Of course, this depends on the parametrization chosen: if a track is globally parametrized, for example as a polynomial, this locality is destroyed.\n\nA particularly insightful way of modeling this locality structure is using the concept of factor graphs. Factor graphs are a class of graphical models in which there are variables and factors. The variables represent unknown quantities in the problem, and the factors represent functions on subsets of the variables. Edges in the factor graph are always between factors and variables, and indicate that a particular factor depends on a particular variable.\n\nThere are three main advantages to using factor graphs when designing algorithms for robotics applications:\n\nThey can represent a wide variety of problems across robotics.\n\nBy laying bare the compositional structure of the problem, they expose opportunities to improve computational performance.\n\nThey are beneficial in designing and thinking about modelling your problem, even aside from performance considerations.\n\nBecause many optimization problems in robotics have the locality property, factor graphs can model a wide variety of problems across AI and robotics. Some of these are illustrated below:","type":"content","url":"/content/blogs/2020/2020-06-01-factor-graphs#by-frank-dellaert-on-twitter","position":3},{"hierarchy":{"lvl1":"What are Factor Graphs?","lvl2":"Boolean Satisfiability:"},"type":"lvl2","url":"/content/blogs/2020/2020-06-01-factor-graphs#boolean-satisfiability","position":4},{"hierarchy":{"lvl1":"What are Factor Graphs?","lvl2":"Boolean Satisfiability:"},"content":"In Boolean Satisfiability, we are looking for an assignment to Boolean variables that make a set of Boolean formulas true. In the example below, the rather boring looking Boolean equations is represented by the Boolean factor graph below:","type":"content","url":"/content/blogs/2020/2020-06-01-factor-graphs#boolean-satisfiability","position":5},{"hierarchy":{"lvl1":"What are Factor Graphs?","lvl2":"Constraint Satisfaction:"},"type":"lvl2","url":"/content/blogs/2020/2020-06-01-factor-graphs#constraint-satisfaction","position":6},{"hierarchy":{"lvl1":"What are Factor Graphs?","lvl2":"Constraint Satisfaction:"},"content":"When we generalize from Boolean to discrete variables, we obtain the class of Constraint Satisfaction Problems or CSPs. For example, the image below shows a graph coloring problem where the goal is to assign a color (chosen from a finite set of colors) to the Swiss cantons such that no neighboring canton has the same color. In the graph below, these pairiwse constraints are indicated by the square factors.","type":"content","url":"/content/blogs/2020/2020-06-01-factor-graphs#constraint-satisfaction","position":7},{"hierarchy":{"lvl1":"What are Factor Graphs?","lvl2":"Bayes Networks:"},"type":"lvl2","url":"/content/blogs/2020/2020-06-01-factor-graphs#bayes-networks","position":8},{"hierarchy":{"lvl1":"What are Factor Graphs?","lvl2":"Bayes Networks:"},"content":"If we relax the hard constraints to real-valued preferences, we switch to a Constraint Optimization Problem. This can be used to express that a particular color is preferred over another one. It can be shown that COPs are also the main optimization problem to solve in the venerable AI technique of Bayes networks. Below an example where a Bayes network with given evidence (the square nodes) is converted to a COP factor graph, which can then give the maximum probable explanation for the remaining (non-evidence) variables.","type":"content","url":"/content/blogs/2020/2020-06-01-factor-graphs#bayes-networks","position":9},{"hierarchy":{"lvl1":"What are Factor Graphs?","lvl2":"Polynomial Equations:"},"type":"lvl2","url":"/content/blogs/2020/2020-06-01-factor-graphs#polynomial-equations","position":10},{"hierarchy":{"lvl1":"What are Factor Graphs?","lvl2":"Polynomial Equations:"},"content":"The constraints can also be other functions of the variables. For example, we can represent a system of polynomial equations by a factor graph. The example below, adapted from Gim Hee Lee’s Ph.D. thesis, encodes a minimal geometry problem from computer vision, useful for autonomous driving:","type":"content","url":"/content/blogs/2020/2020-06-01-factor-graphs#polynomial-equations","position":11},{"hierarchy":{"lvl1":"What are Factor Graphs?","lvl2":"Simultaneous Localization and Mapping:"},"type":"lvl2","url":"/content/blogs/2020/2020-06-01-factor-graphs#simultaneous-localization-and-mapping","position":12},{"hierarchy":{"lvl1":"What are Factor Graphs?","lvl2":"Simultaneous Localization and Mapping:"},"content":"Speaking of autonomous driving, perhaps the most famous application of factor graphs is in SLAM, or Simultaneous Localization and Mapping. This is illustrated in the example below, where the location of a vehicle over time (the cyan variables) as well as the location of a set of landmarks (the dark blue nodes) are solved for. In this case we have a plethora of factors associated with vehicle odometry, landmark sightings, and GPS priors. The example in question is a small excerpt from a real robot experiment in Sydney’s Victoria Park.","type":"content","url":"/content/blogs/2020/2020-06-01-factor-graphs#simultaneous-localization-and-mapping","position":13},{"hierarchy":{"lvl1":"What are Factor Graphs?","lvl2":"Structure from Motion:"},"type":"lvl2","url":"/content/blogs/2020/2020-06-01-factor-graphs#structure-from-motion","position":14},{"hierarchy":{"lvl1":"What are Factor Graphs?","lvl2":"Structure from Motion:"},"content":"Finally, we can also represent 3D mapping or 3D reconstruction problems with factor graphs. Below we show just the edges in the factor graph connecting one camera (yellow) with all points visible from that camera (black) in a large-scale 3D reconstruction of downtown Chicago.","type":"content","url":"/content/blogs/2020/2020-06-01-factor-graphs#structure-from-motion","position":15},{"hierarchy":{"lvl1":"What are Factor Graphs?","lvl2":"Conclusion"},"type":"lvl2","url":"/content/blogs/2020/2020-06-01-factor-graphs#conclusion","position":16},{"hierarchy":{"lvl1":"What are Factor Graphs?","lvl2":"Conclusion"},"content":"For more information about how factor graphs are typically used to solve perception problems in robotics, see the following booklet: \n\nFactor graphs for robot perception, by \n\nFrank Dellaert and \n\nMichael Kaess, which appeared in 2017 in Foundations and Trends in Robotics.\n\nHowever, this is just the tip of the iceberg. Factor graphs can be used to model a much wider variety of problems across robotics domains, such as Tracking, Inertial Navigation, Mapping with LIDARs, Classical Planning, Reinforcement Learning and Optimal Control, Motion Planning etc...","type":"content","url":"/content/blogs/2020/2020-06-01-factor-graphs#conclusion","position":17},{"hierarchy":{"lvl1":"Geometry and Variable Naming Conventions"},"type":"lvl1","url":"/content/blogs/2020/2020-06-28-gtsam-conventions","position":0},{"hierarchy":{"lvl1":"Geometry and Variable Naming Conventions"},"content":"Author: \n\nSamarth Brahmbhatt\n\nI am a post-doc at \n\nIntel ISL. In my\n\n\nPhD work, I used GTSAM for object pose\nestimation and 3D reconstruction of hand joints to study hand-object interaction.\nI love GTSAM for its ability to optimize geometric quantities and deal with\nuncertainty in observations. However, it is easy to get lost in the web of\ninverse transformations while optimizing complex geometric systems. I’ve found\nthat naming code variables consistently with textbook notation conventions can\nhelp mitigate this. This post describes some suggestions for doing that.","type":"content","url":"/content/blogs/2020/2020-06-28-gtsam-conventions","position":1},{"hierarchy":{"lvl1":"Geometry and Variable Naming Conventions","lvl2":"Points"},"type":"lvl2","url":"/content/blogs/2020/2020-06-28-gtsam-conventions#points","position":2},{"hierarchy":{"lvl1":"Geometry and Variable Naming Conventions","lvl2":"Points"},"content":"Always name your 3D points like how you would on paper. A point cX in the\ncamera coordinate system c is named cX.\n\n3D points use uppercase letters, 2D points use lowercase letters.","type":"content","url":"/content/blogs/2020/2020-06-28-gtsam-conventions#points","position":3},{"hierarchy":{"lvl1":"Geometry and Variable Naming Conventions","lvl2":"Pose"},"type":"lvl2","url":"/content/blogs/2020/2020-06-28-gtsam-conventions#pose","position":4},{"hierarchy":{"lvl1":"Geometry and Variable Naming Conventions","lvl2":"Pose"},"content":"Name your pose variables like how you would write them on paper.\nThe pose ^wT_c of camera coordinate frame c in the world coordinate frame\nw is named wTc. In GTSAM jargon, c is the pose coordinate frame,\nand w is the world coordinate frame.","type":"content","url":"/content/blogs/2020/2020-06-28-gtsam-conventions#pose","position":5},{"hierarchy":{"lvl1":"Geometry and Variable Naming Conventions","lvl2":"Composing Poses"},"type":"lvl2","url":"/content/blogs/2020/2020-06-28-gtsam-conventions#composing-poses","position":6},{"hierarchy":{"lvl1":"Geometry and Variable Naming Conventions","lvl2":"Composing Poses"},"content":"Math: ^oT_c =~^oT_w~\\cdot~^wT_c.\n\nGTSAM code:Pose3 oTw = Pose3(...);\nPose3 wTc = Pose3(...);\nPose3 oTc = oTw * wTc;\n// same as Pose3 oTc = oTw.compose(wTc);","type":"content","url":"/content/blogs/2020/2020-06-28-gtsam-conventions#composing-poses","position":7},{"hierarchy":{"lvl1":"Geometry and Variable Naming Conventions","lvl2":"Transforming Points From Pose Coordinates"},"type":"lvl2","url":"/content/blogs/2020/2020-06-28-gtsam-conventions#transforming-points-from-pose-coordinates","position":8},{"hierarchy":{"lvl1":"Geometry and Variable Naming Conventions","lvl2":"Transforming Points From Pose Coordinates"},"content":"This operation uses the camera pose in the world coordinate frame\n(^wT_c) to bring points from the camera coordinate frame (^c\\homo{X}) to\nthe world coordinate frame (^w\\homo{X}).\n\nMath: ^w\\homo{X} =~^wT_c~\\cdot~^c\\homo{X}\n\nGTSAM Code:Point3 wX = wTc.transformFrom(cX);\n// same as Point3 wX = wTc * cX;","type":"content","url":"/content/blogs/2020/2020-06-28-gtsam-conventions#transforming-points-from-pose-coordinates","position":9},{"hierarchy":{"lvl1":"Geometry and Variable Naming Conventions","lvl2":"Transforming Points To Pose Coordinates"},"type":"lvl2","url":"/content/blogs/2020/2020-06-28-gtsam-conventions#transforming-points-to-pose-coordinates","position":10},{"hierarchy":{"lvl1":"Geometry and Variable Naming Conventions","lvl2":"Transforming Points To Pose Coordinates"},"content":"This operation uses the inverse of the camera pose \\left(^wT_c\\right)^{-1}\nto bring points from the world coordinate frame (^w\\homo{X}) to\nthe camera coordinate frame (^c\\homo{X}).\n\nMath: ^c\\homo{X} =~\\left(^wT_c\\right)^{-1}~\\cdot~^w\\homo{X}\n\nGTSAM Code:Point3 cX = wTc.transformTo(wX);","type":"content","url":"/content/blogs/2020/2020-06-28-gtsam-conventions#transforming-points-to-pose-coordinates","position":11},{"hierarchy":{"lvl1":"Geometry and Variable Naming Conventions","lvl2":"Cameras"},"type":"lvl2","url":"/content/blogs/2020/2020-06-28-gtsam-conventions#cameras","position":12},{"hierarchy":{"lvl1":"Geometry and Variable Naming Conventions","lvl2":"Cameras"},"content":"The GTSAM pinhole camera classes\n(e.g. \n\nPinholeBase)\ninternally use transformTo() to transform 3D points into the camera\ncoordinates, so you should use the pose of the camera w.r.t. world while\nconstructing the object:Pose3 wTc = Pose3(...);\nSimpleCamera cam(wTc, K);\n\nnow you can use cam in the TriangulationFactor for example. Other factors\nlike the\n\n\nGenericProjectionFactor\nalso use the same convention:\n$$\\begin{align*}\n^{sensor}\\homo{X}\n&=~^{sensor}T_{world}~\\cdot~^{world}\\homo{X}\\\\\n&=~^{sensor}T_{body}~\\cdot~^{body}T_{world}~\\cdot~^{world}\\homo{X}\\\\\n&= \\left(^{world}T_{body}~\\cdot~^{body}T_{sensor}\\right)^{-1}~\\cdot~^{world}\\homo{X}\n\\end{align*}\n\n$$\n\nExample: In a mobile robot SLAM scenario, wheel odometry might tell you where\nthe robot body is in the world (^{world}T_{body}), and the robot URDF spec\nmight tell you where the camera is on the robot body (^{body}T_{sensor}).\nTogether, these allow you to situate camera observations in the world coordinate\nframe.\n\nGTSAM Code:Pose3 body_T_sensor = ...\nPoint2 sensor_p = ...  // 2D point in the image\n// in the following factor,\n// Symbol('T', i) is world_T_body for the i'th frame\n// Symbol('X', j) is the j'th 3D point in world coordinates i.e. world_Xj\nauto f = GenericProjectionFactor<Pose3, Point3, Cal3_S2>(sensor_p, noise, Symbol('T', i), Symbol('X', j), K, body_T_sensor);\n\nIt will project the world 3D point ^{world}\\homo{X} into the sensor coordinates like so:Pose3 world_T_sensor = world_T_body * body_T_sensor;\nPoint3 sensor_X = world_T_sensor.transformTo(world_X);\n\nand then project it to the image using instrinsics and then compare it to the detection sensor_p.","type":"content","url":"/content/blogs/2020/2020-06-28-gtsam-conventions#cameras","position":13},{"hierarchy":{"lvl1":"Releasing GTSAM 4.0.3"},"type":"lvl1","url":"/content/blogs/2020/2020-07-16-new-release-gtsam","position":0},{"hierarchy":{"lvl1":"Releasing GTSAM 4.0.3"},"content":"Author: \n\nFan Jiang","type":"content","url":"/content/blogs/2020/2020-07-16-new-release-gtsam","position":1},{"hierarchy":{"lvl1":"Releasing GTSAM 4.0.3","lvl2":"Introduction"},"type":"lvl2","url":"/content/blogs/2020/2020-07-16-new-release-gtsam#introduction","position":2},{"hierarchy":{"lvl1":"Releasing GTSAM 4.0.3","lvl2":"Introduction"},"content":"You are probably here because you do optimization, like everyone here on the GTSAM team. As fellow roboticists, we know how frustrating it be when your problem does not converge.\n\nTo further optimize your optimization experience, we are excited to announce this new release of GTSAM, GTSAM 4.0.3, where we incorporated a lot of new features and bugfixes, as well as substantial improvements in convergence for Pose3-related problems.\n\nNote that GTSAM already provides the excellent Pose3 initialization module by Luca Carlone, in InitializePose3, which we always recommend if your pipeline does not provide a good initial estimate out of the box.","type":"content","url":"/content/blogs/2020/2020-07-16-new-release-gtsam#introduction","position":3},{"hierarchy":{"lvl1":"Releasing GTSAM 4.0.3","lvl2":"Major Changes"},"type":"lvl2","url":"/content/blogs/2020/2020-07-16-new-release-gtsam#major-changes","position":4},{"hierarchy":{"lvl1":"Releasing GTSAM 4.0.3","lvl2":"Major Changes"},"content":"","type":"content","url":"/content/blogs/2020/2020-07-16-new-release-gtsam#major-changes","position":5},{"hierarchy":{"lvl1":"Releasing GTSAM 4.0.3","lvl3":"Switching Away from Cayley","lvl2":"Major Changes"},"type":"lvl3","url":"/content/blogs/2020/2020-07-16-new-release-gtsam#switching-away-from-cayley","position":6},{"hierarchy":{"lvl1":"Releasing GTSAM 4.0.3","lvl3":"Switching Away from Cayley","lvl2":"Major Changes"},"content":"TL;DR: GTSAM will now default to using the full \\mathrm{SE(3)} exponential map, instead of using the Cayley map, which should give better convergence for most problems without performance impact.\n\nIn nonlinear optimization, one important factor affecting the convergence is the mathematical structure of the object we are optimizing on. In many practical 3D robotics problems this is the \\mathrm{SE(3)} manifold describing the structure of 3D Poses.\n\nIt is not easy to directly operate on nonlinear manifolds like \\mathrm{SE(3)}, so libraries like GTSAM uses the following strategy:\n\nLinearize the error manifold at the current estimate\n\nCalculate the next update in the associated tangent space\n\nMap the update back to the manifold with a retract map\n\nWe used two distinct but equally important concepts above: 1) the error metric, which is in a PoseSLAM problems is the measure of error between two poses; and 2) the retract operation, which is how we apply a computed linear update back to the nonlinear error manifold.\n\nIn GTSAM, you can choose, at compile time, between four different choices for the retract map on the \\mathrm{SE(3)} manifold:\n\nFull: Exponential map on \\mathrm{SE(3)}\n\nDecomposed retract, which uses addition for translation and:\n\nExponential map \\mathrm{SO(3)} with Rotation Matrix\n\nExponential map \\mathrm{SO(3)} with Quaternions\n\nCayley map on \\mathrm{SO(3)} with Rotation Matrix\n\nPreviously in GTSAM, we used the Cayley map by default, which is an approximation of the \\mathrm{SO(3)} exponential map when the tangent vector (rotation error) is small. This is perfectly fine locally, if we have a relatively good initial estimate.\n\nHowever, since we are also using the inverse of the retract as the error metric, a different choice for the retract map could give better convergence.\nAs you can see in the following figure, the Cayley local map is unbounded when θ is large, and thus negatively impacts convergence when the initialization is not good enough.\n\nBased on careful benchmarking, in the new release, we will not use the Cayley approximation by default, which should give you a better convergence for most applications. This is  especially true if your initial estimate can be far away from the global optimum: the impact on well-initialized problems is minimal. You can look at the benchmark yourself in the next section, if you are interested.","type":"content","url":"/content/blogs/2020/2020-07-16-new-release-gtsam#switching-away-from-cayley","position":7},{"hierarchy":{"lvl1":"Releasing GTSAM 4.0.3","lvl3":"Can we still use Cayley and Friends?","lvl2":"Major Changes"},"type":"lvl3","url":"/content/blogs/2020/2020-07-16-new-release-gtsam#can-we-still-use-cayley-and-friends","position":8},{"hierarchy":{"lvl1":"Releasing GTSAM 4.0.3","lvl3":"Can we still use Cayley and Friends?","lvl2":"Major Changes"},"content":"Yes, just not by default. Historically, the Cayley approximation was chosen as a cheaper alternative to the full exponential map, and our intention is not to tell you that you should not use it, rather to inform you that without properly initializing your initial estimate, the result of Cayley could be inferior to those obtained with the full \\mathrm{SE(3)} retract.\n\nIn order to give you an intuitive understanding of the situation we made a benchmark where the four configurations by:\n\nasking GTSAM to solve 6 benchmark optimization datasets, with the Chordal initialization as initial estimate (from InitializePose3);\n\nasking GTSAM to solve 6 benchmark optimization datasets, this time with 100 random initial estimates, sampled around the ground truth by a Gaussian distribution of 1 sigma, and observe the convergence metrics.\n\n\n\na) With Chordal Initialization\n\n\n\nb) Without Chordal Initialization\n\nPerformance for different retract variants.\n\nNote that with proper initialization, all 4 configurations achieved convergence without issue. However, the full \\mathrm{SE(3)} retract exhibited much better convergence with randomly initialized estimates.\n\nFor a visual reference, here are 3D scatter plots of samples from the random benchmark results that you can zoom in and see the difference:\n\n\n\n\n\nThe results can be reproduced with this repo: https://github.com/ProfFan/expmap-benchmark","type":"content","url":"/content/blogs/2020/2020-07-16-new-release-gtsam#can-we-still-use-cayley-and-friends","position":9},{"hierarchy":{"lvl1":"Releasing GTSAM 4.0.3","lvl2":"Important New Features & Bugfixes"},"type":"lvl2","url":"/content/blogs/2020/2020-07-16-new-release-gtsam#important-new-features-bugfixes","position":10},{"hierarchy":{"lvl1":"Releasing GTSAM 4.0.3","lvl2":"Important New Features & Bugfixes"},"content":"In addition to the change in default Pose3 retract, which will now be the full exponential map, GTSAM has seen a steady stream of commits since the last release, 4.0.2, which has been there for more than 6 months. A summary of the most important issues and features is below:\n\nRobust noise model is ready for general usage\n\nIt can be used to replace RANSAC for some applications\n\nFor a gentle introduction, see \n\nthis awesome tutorial by Varun Agrawal\n\nCombinedImuFactor serialization is now fixed\n\nThe ISAM2 KITTI example has a C++ port, thanks Thomas Jespersen for the help!\n\nNow you can choose arbitrary MATLAB install prefix for the toolbox build\n\nNow you can make python-install to install the Python toolbox\n\nNow you can use the Conjugate Gradient solver in Python\n\nNow you can install GTSAM with pip if you only use the Python interface\n\nAdded FrobeniusFactor and FrobeniusWormholeFactor for robust SFM applications\n\nSwitched to in-place update of the diagonal Hessian in LM\n\nexpect a 3%-5% speedup, YMMV\n\nThe Cython wrapper now can be built on Windows :)\n\nKudos @tuwuhs for the help!\n\nFixed a few memory-related bugs detected by the LLVM sanitizer\n\nGreatly improved stability","type":"content","url":"/content/blogs/2020/2020-07-16-new-release-gtsam#important-new-features-bugfixes","position":11},{"hierarchy":{"lvl1":"Releasing GTSAM 4.0.3","lvl2":"Finale"},"type":"lvl2","url":"/content/blogs/2020/2020-07-16-new-release-gtsam#finale","position":12},{"hierarchy":{"lvl1":"Releasing GTSAM 4.0.3","lvl2":"Finale"},"content":"With over a hundred merged pull requests, we welcome you again on board the new release of GTSAM, GTSAM 4.0.3. We would like to thank all our contributors for their precious commits and bug reports. Finally, thank you for using GTSAM and please don’t hesitate to open an issue on GitHub if you found a bug!","type":"content","url":"/content/blogs/2020/2020-07-16-new-release-gtsam#finale","position":13},{"hierarchy":{"lvl1":"Mount Rainier's Eigenvectors"},"type":"lvl1","url":"/content/blogs/2020/2020-08-30-laplacian","position":0},{"hierarchy":{"lvl1":"Mount Rainier's Eigenvectors"},"content":"","type":"content","url":"/content/blogs/2020/2020-08-30-laplacian","position":1},{"hierarchy":{"lvl1":"Mount Rainier's Eigenvectors","lvl3":"Frank Dellaert, August 30, 2020"},"type":"lvl3","url":"/content/blogs/2020/2020-08-30-laplacian#frank-dellaert-august-30-2020","position":2},{"hierarchy":{"lvl1":"Mount Rainier's Eigenvectors","lvl3":"Frank Dellaert, August 30, 2020"},"content":"In this post I’ll talk a bit about estimating absolute quantities from relative measurements, using the reconstruction of \n\nMount Rainier as a motivating example. I’ll show how the Hessian of that problem is exactly the “Graph Laplacian” from graph theory, and relate the eigen-decomposition of that graph with the properties of the reconstruction.\n\nThere is an accompaning \n\nYoutube video.\n\nAlso, this post was created as a Colab notebook, and you can run it yourself here: \n\n#@Libraries we need\nimport networkx as nx\nimport matplotlib.pyplot as plt\nfrom matplotlib import image\nfrom mpl_toolkits.mplot3d import Axes3D\n%matplotlib inline\nimport numpy as np\nimport scipy\nfrom sklearn.neighbors import NearestNeighbors","type":"content","url":"/content/blogs/2020/2020-08-30-laplacian#frank-dellaert-august-30-2020","position":3},{"hierarchy":{"lvl1":"Mount Rainier's Eigenvectors","lvl2":"Reconstructing Mount Rainier"},"type":"lvl2","url":"/content/blogs/2020/2020-08-30-laplacian#reconstructing-mount-rainier","position":4},{"hierarchy":{"lvl1":"Mount Rainier's Eigenvectors","lvl2":"Reconstructing Mount Rainier"},"content":"Imagine you and your friends have climbed Mount Rainier, and you were able to obtain a large number of relative height measurements: can we reconstruct the shape of the mountain from those? It turns out the answer is yes, modulo an unknown height offset.\n\nTo simulate this scenario, I will use a \n\nDigital Elevation Model (DEM). It’s not super-easy to get one’s hands on digital elevation data, but luckily \n\nMatt Booty has done it for us for Mount Rainier. Below we download the image and load it into this notebook.!wget -q https://mattbooty.files.wordpress.com/2019/01/mr-dem-height-map-output-03.jpg\ndem = image.imread('mr-dem-height-map-output-03.jpg')\nfig, ax = plt.subplots(figsize=(6, 6))\nax.imshow(dem);\n\nFirst, we’ll get some sample points, corresponding to where you and your friends would have taken measurements. In this example we assume we know the 2D locations of the points, although it is not hard to extend the example beyond that assumption.\n\nThe code below samples 500 different locations and shows them on the DEM:coordinates = np.random.multivariate_normal(mean=[420,800], cov=100*100*np.eye(2),size=500)\nfig, ax = plt.subplots(figsize=(6, 6))\nax.imshow(dem);\nax.scatter(coordinates[:,0], coordinates[:,1], marker='.')\nplt.show()\n\nLet’s collect all 2D and their associated ground-truth heights into a numpy array, filtering out samples from the black border.truth = np.array([(x,y,dem[y,x,0]) for x,y in np.round(coordinates).astype(np.int) if dem[y,x,0]>0])\nN = truth.shape[0]\nz = truth[:,2]\n\nTo create relative measurements, we’ll use a graph derived from the K-nearest neighbors.\n\nThe associated estimation problem is quite simple, and we can see it as a factor graph with just pairwise linear factors \\|z_i -z_j - \\delta_{ij}\\|^2. In this linear case, the factor graph is completely equivalent to a sparse rectangular matrix H, with entries 1 and -1. For every edge (i,j) in the nearest neighbor graph, there is one row/factor.\n\nFrom the graph G we can easily generate this sparse measurement matrix H using the (oriented) incidence_matrix function in networkx. The relative measurements d are then obtained as d=H z where z are the ground truth height values.# Find nearest Neighbors\nneigh = NearestNeighbors(n_neighbors=6, radius=100)\nneigh.fit(truth)\nA = neigh.kneighbors_graph()# Create graph\nG0 = nx.from_scipy_sparse_matrix(A)\n\n# Create measurement matrix H\nH = nx.incidence_matrix(G0, oriented=True).transpose()\nM, N = H.shape\n\n# Simulate measurements d\nd = H @ z + np.random.normal(size=(M,),scale=1)\nplt.hist(d);\n\nUnfortunately relative measurements alone can never tell us about absolute height. To remedy this we add one row to H to clamp \\hat{z}[0] to 100, and we then use the lsqr method in scipy to obtain estimated heights \\hat{z}:H1 = scipy.sparse.bmat([[H], [scipy.sparse.eye(1,N)]])\nd1 = np.concatenate((d,[truth[0,2]]))\nz, istop, itn, r1norm = scipy.sparse.linalg.lsqr(H1, d1)[:4]\n\nFinally, let’s render the result in 3D:fig = plt.figure(figsize=(12,8))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(truth[:,0], truth[:,1], z, marker='.');\n\nThat does not look half-bad ;-)","type":"content","url":"/content/blogs/2020/2020-08-30-laplacian#reconstructing-mount-rainier","position":5},{"hierarchy":{"lvl1":"Mount Rainier's Eigenvectors","lvl2":"Of Jacobians, Hessians, and Graph Laplacians"},"type":"lvl2","url":"/content/blogs/2020/2020-08-30-laplacian#of-jacobians-hessians-and-graph-laplacians","position":6},{"hierarchy":{"lvl1":"Mount Rainier's Eigenvectors","lvl2":"Of Jacobians, Hessians, and Graph Laplacians"},"content":"The large (but sparse) measurement matrix H we used above is also called the Jacobian, and the lsqr method solved the linear least-squares problem \\hat{z} = \\arg\\min_z \\|H z - d \\|^2. This is really just a multivariate quadratic with its curvature given by the Hessian Q\\doteq H^TH. The Hessian is also known as the information matrix, and is equal to the inverse of the covariance matrix P\\doteq Q^{-1}.\n\nThe significance of all this that the Hessian Q tells us exactly how much we know about the estimated heights given the measurements and the uncertainty associated with the measurements (which here we silently assumed to be zero-mean Gaussian additive noise with unit standard deviation \\sigma=1).\n\nTo explore this in a bit more depth, let’s simplify the problem even more and only consider 1D mountains. Below we walk up a mountain and record  relative height measurements between 6 subsequent points:z = np.array([100,120,130,120,110,110])\nplt.plot(z,'-*');\n\nLet us take measurements both between neighboring points as well as points one hop away, which should intuitively improve our estimates:G = nx.Graph()\nG.add_edges_from([(0,1),(0,2),(1,2),(1,3),(2,3),(2,4),(3,4),(3,5),(4,5)])\nH = nx.incidence_matrix(G, oriented=True).transpose()\nM, N = H.shape\nd = H @ z + np.random.normal(size=(M,),scale=1)\n\nIf we calculate at the Hessian Q=H^T H for this simpler problem, we see that it is a 6\\times 6 matrix:Q = H.T@ H\nprint(Q.toarray().astype(np.int))[[ 2 -1 -1  0  0  0]\n [-1  3 -1 -1  0  0]\n [-1 -1  4 -1 -1  0]\n [ 0 -1 -1  4 -1 -1]\n [ 0  0 -1 -1  3 -1]\n [ 0  0  0 -1 -1  2]]\n\nGraph theory also defines the \n\nLaplacian Matrix L=D-A of an undirected graph G, where D is a diagonal degree matrix and A is the adjacency matrix of the graph.\n\nThe Laplacian L associated with this matrix is easy to calculate, but we’ll use networkx to do it for us:L = nx.linalg.laplacian_matrix(G).toarray()\nprint(L)[[ 2 -1 -1  0  0  0]\n [-1  3 -1 -1  0  0]\n [-1 -1  4 -1 -1  0]\n [ 0 -1 -1  4 -1 -1]\n [ 0  0 -1 -1  3 -1]\n [ 0  0  0 -1 -1  2]]\n\nAs we can see, for this problem the Hessian and the Graph Laplacian are identical.\n\nThis nicely connects estimation and graph theory. In particular, in graph theory the eigenvalues and eigenvectors of the graph Laplacian L are of particular interest, and we will be able to connect them to the SVD of the measurement matrix H and by extension Q.\n\nFirst, let us calculate the eigen-decomposition of L. We will use the \n\nsingular value decomposition or SVD to do this as that nicely sorts the eigenvalues for us in decreasing magnitude order:eigenvectors, eigenvalues, _ = np.linalg.svd(L)\nprint(np.round(eigenvalues,2))\nprint(np.round(eigenvectors,2))[5.34 5.   3.47 3.   1.19 0.  ]\n[[ 0.23  0.   -0.29  0.58  0.6   0.41]\n [-0.1  -0.5   0.62 -0.29  0.33  0.41]\n [-0.66  0.5  -0.19 -0.29  0.16  0.41]\n [ 0.66  0.5   0.19 -0.29 -0.16  0.41]\n [ 0.1  -0.5  -0.62 -0.29 -0.33  0.41]\n [-0.23  0.    0.29  0.58 -0.6   0.41]]\n\nHowever, because we have L=Q=H^TH, we can also do obtain this by taking the SVD of H:_, S, V = np.linalg.svd(H.toarray())\nprint(np.round(S**2,2)) # square to get eigenvalues of Q!\nprint(np.round(V.transpose(),2))[5.34 5.   3.47 3.   1.19 0.  ]\n[[ 0.23 -0.   -0.29  0.58  0.6  -0.41]\n [-0.1  -0.5   0.62 -0.29  0.33 -0.41]\n [-0.66  0.5  -0.19 -0.29  0.16 -0.41]\n [ 0.66  0.5   0.19 -0.29 -0.16 -0.41]\n [ 0.1  -0.5  -0.62 -0.29 -0.33 -0.41]\n [-0.23 -0.    0.29  0.58 -0.6  -0.41]]\n\nBased on this, we can start to understand some properties that are often discussed in the context of the graph Laplacian:\n\n1. The smallest eigenvalue of L is zero, and the associated eigenvector is all ones, up to a scale.print(\"Eigenvalues=\\n  {}\".format(np.round(eigenvalues,2)), \n      \"\\nEigenvector associated with first zero=\\n  {}\".format(np.round(eigenvectors[:,-1],2))\n      )Eigenvalues=\n  [5.34 5.   3.47 3.   1.19 0.  ] \nEigenvector associated with first zero=\n  [0.41 0.41 0.41 0.41 0.41 0.41]\n\nLet’s unpack this: if we interpret the graph Laplacian L as an the information matrix Q the result above says that there is a direction in the space of unknowns in which we have zero information. This makes total sense: since we only have relative measurements, we can add a constant without changing the objective function: that corresponds to the first eigenvector.\n\n2. If the graph G is disconnected, the second eigenvalue is also zero.\n\nWe can examine this by looking at two mountaintops, by just replicating H:H2 = np.zeros((2*M,2*N))\nH2[:M,:N]=H.toarray()\nH2[M:,N:]=H.toarray()\nQ2 = H2.T @ H2\neigenvectors2, eigenvalues2, _ = np.linalg.svd(Q2)\nprint(\"Eigenvalues=\\n  {}\".format(np.round(eigenvalues2)), \n      \"\\nEigenvector associated with first zero=\\n  {}\".format(np.round(eigenvectors2[:,-1],2)),\n      \"\\nEigenvector associated with second zero=\\n  {}\".format(np.round(eigenvectors2[:,-2],2))\n      )Eigenvalues=\n  [5. 5. 5. 5. 3. 3. 3. 3. 1. 1. 0. 0.] \nEigenvector associated with first zero=\n  [-0.   -0.   -0.   -0.   -0.   -0.    0.41  0.41  0.41  0.41  0.41  0.41] \nEigenvector associated with second zero=\n  [ 0.41  0.41  0.41  0.41  0.41  0.41 -0.    0.    0.    0.    0.    0.  ]\n\nThis is also not surprising: even though this simulates scaling the two mountain tops and recording our elevation changes, we have no measurements connecting the two mountains, and hence we can independently vary the height of either mountain, without violating any measurement constraint. That corresponds to the two eigenvectors associated with the two zero eigenvalues.\n\nThis is also the basis for \n\nspectral partitioning/clustering mtehods: the eigenvectors neatly separate the graph into two components, and this can be generalized to more sophisticated clustering methods.\n\nThis is even better appreciated by plotting both eigenvectors:fig=plt.plot(eigenvectors2[:,-2:],'--*')\n\n3. The second-smallest eigenvalue, called the \n\nAlgebraic Connectivity or Fiedler value is greater than zero if the graph is connected, and measures the global connectivity in the graph.\n\nIn our original 6-node mountain top problem, the Fiedler value is approximatey 1.19:print(\"lambda = {}\".format(round(eigenvalues[-2],2)))\nprint(\"with networkx: {}\".format(round(nx.algebraic_connectivity(G),2)))lambda = 1.19\nwith networkx: 1.19\n\n4. The eigenvector associated with the Fiedler value, also called the “Fiedler vector”, is the direction in space that incurs the least error in the scalar synchronization problem.\n\nIndeed: the direction that incurs the least error is also the one we have the least information about: highly informative measurements will incur a high error if they are violated. Note that by “error” we mean the quadratic objective function, not the difference between our solution and the ground truth, which we in general do not have access to.\n\nPer the spectral clustering idea, it can also be used to partition the graph into two parts, even if the graph is not disconnected. This can be appreciated by plotting the Fiedler vector for the original mountain top:fig=plt.plot(eigenvectors[:,-2], '-*')\nplt.legend([\"$\\lambda = {}$\".format(round(eigenvalues[-2],2))],loc='upper right');\n\nJust by thresholding on zero we can neatly partition the mountain in the left half, and the right half!\n\n5. The remaining eigenvectors of L (or Q) are increasingly “high frequency” modes that identify directions in space that incur high costs.fig, ax = plt.subplots(figsize=(10, 5))\nax.plot(eigenvectors[:,:-2])\nplt.legend([\"$\\lambda_{} = {}$\".format(5-i, round(e,2)) for i,e in enumerate(eigenvalues[:-2])],loc='lower left');\n\nFor example, the blue “mode” associated with \\lambda_5 shows that modifying the values of neighboring nodes 2 and 3 in opposite ways will be quite expensive.\n\nThe SVD is a generalization of the Fourier transform, in that the SVD of the Laplacian of a ring will in fact yield the DFT:G3 = nx.watts_strogatz_graph(64, 2, 0) # ring is simply special case of WS graph\nfig, ax = plt.subplots(ncols=3,figsize=(13, 4))\nnx.draw_circular(G3, node_size=25, ax=ax[0])\nL3 = nx.linalg.laplacian_matrix(G3).toarray()\neigenvectors3, eigenvalues3, _ = np.linalg.svd(L3)\nax[1].plot(eigenvectors3[:,-3:]);\nax[2].plot(eigenvectors3[:,-5:-3]);\n\nJust to close the thread, as we saw before in the Mount Rainier example, if we have an absolute measurement then we can simply add a “prior” to our factor graph H and RHS d, and we will get a definite answer:H1 = np.concatenate((H.toarray(), [[1,0,0,0,0,0]]), axis=0)\nd1 = np.concatenate((d,[100]))\nQ1 = H1.T @ H1\nz = np.linalg.inv(Q1) @ H1.T @ d1\nprint(z)[100.         121.27268328 130.98429722 120.60191527 111.20576812\n 111.48345044]\n\nBecause there is no contradictory information regarding the value of z_0, the solution adopts that prior exactly.\n\nIt is instructive to abandon the “pure Laplacian” view here and look at the eigenvalues and eigenvectors of the Hessian of this new problem, which is no longer indeterminate:eigenvectors, eigenvalues, _ = np.linalg.svd(Q1)\nfig, ax = plt.subplots(figsize=(10, 5))\nax.plot(eigenvectors)\nax.legend([\"$\\lambda_{} = {}$\".format(5-i, round(e,2)) for i,e in enumerate(eigenvalues)],loc='lower right');\n\nAs you can see:\n\nthe smallest eigenvalue is no longer zero: we now incur a definite cost of moving up and down, but the eigenvector associated with \\lambda_1 suggests that it this is less so for nodes far away from the “anchor node”.\n\nThe eigenvectors associated with \\lambda_5 has a zero in the front and its eigenvalue is the same as before.\n\nThe remaining eigenvectors have larger eigenvalues as they also change the anchor node, which now has higher cost.","type":"content","url":"/content/blogs/2020/2020-08-30-laplacian#of-jacobians-hessians-and-graph-laplacians","position":7},{"hierarchy":{"lvl1":"Mount Rainier's Eigenvectors","lvl2":"Mount Rainier’s Eigenvectors"},"type":"lvl2","url":"/content/blogs/2020/2020-08-30-laplacian#mount-rainiers-eigenvectors","position":8},{"hierarchy":{"lvl1":"Mount Rainier's Eigenvectors","lvl2":"Mount Rainier’s Eigenvectors"},"content":"We will use the sparse/fast methods from networkx to calculate the Fiedler value and Fiedler vector:fiedler_value = nx.algebraic_connectivity(G0)\nprint(fiedler_value)\nfiedler_vector = nx.fiedler_vector(G0)\nplt.plot(fiedler_vector);0.053047495666829085\n\nThis does not look very good in 1D, so let’s plot it in 3D, using a color map to encode the Fiedler vector’s values:fig = plt.figure(figsize=(12,8))\nax = fig.add_subplot(111, projection='3d')\nz = truth[:,2]\nax.scatter(truth[:,0], truth[:,1], z, marker='.', c=fiedler_vector,cmap='RdYlGn');\n\nIt is even clearer from the top: below we plot an overhead view, in which we can clearly see that the two “halves” of Mount Rainier can move more or less independently without incurring much error. Hence, this is the lowest frequency mode over which we do have some information (but not much).\n\nIf you are running the notebook, you could try playing with the number of neighbors in the KNN calculation, and see how the Fiedler vector changes as a result.fig = plt.figure(figsize=(12,8))\nax = fig.add_subplot(111)\nz = truth[:,2]\nax.scatter(truth[:,0], truth[:,1], marker='.', c=fiedler_vector,cmap='RdYlGn');","type":"content","url":"/content/blogs/2020/2020-08-30-laplacian#mount-rainiers-eigenvectors","position":9},{"hierarchy":{"lvl1":"Mount Rainier's Eigenvectors","lvl2":"Other Synchronization Problems"},"type":"lvl2","url":"/content/blogs/2020/2020-08-30-laplacian#other-synchronization-problems","position":10},{"hierarchy":{"lvl1":"Mount Rainier's Eigenvectors","lvl2":"Other Synchronization Problems"},"content":"The “mountain shape” problems we discussed above are instances of a class of estimation problems called synchronization problems, in which we are given a set of pairwise relative measurements, and our goal is to estimate a set of absolute quantities.\n\nArguably the most studied synchronization problem in computer vision is rotation averaging, where we are given a set of relative rotations R_{ij} between cameras and the goal is to estimate the absolute orientation R_i for every camera. This is a crucial step in 3D reconstruction algorithms.\n\nWe have recently published a paper on \n\nShonan Rotation Averaging, where the eigenvalue and eigenvectors of the graph Laplacian are used in both the analysis and convergence certificates.","type":"content","url":"/content/blogs/2020/2020-08-30-laplacian#other-synchronization-problems","position":11},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 1: Linear and nonlinear"},"type":"lvl1","url":"/content/blogs/2021/2021-02-23-uncertainties-part1","position":0},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 1: Linear and nonlinear"},"content":" custom latex commands here $ \\usepackage{amsmath} \\usepackage[makeroom]{cancel} \\DeclareMathOperator*{\\argmin}{argmin} \\newcommand{\\coloneqq}{\\mathrel{:=}} $ horizontal scrolling ","type":"content","url":"/content/blogs/2021/2021-02-23-uncertainties-part1","position":1},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 1: Linear and nonlinear","lvl2":"Introduction"},"type":"lvl2","url":"/content/blogs/2021/2021-02-23-uncertainties-part1#introduction","position":2},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 1: Linear and nonlinear","lvl2":"Introduction"},"content":"In these posts I will review some general aspects of optimization-based state estimation methods, and how to input and output consistent quantities and uncertainties, i.e, covariance matrices, from them. We will take a (hopefully) comprehensive tour that will cover why we do the things the way we do, aiming to clarify some uncertain things about working with covariances. We will see how most of the explanations naturally arise by making explicit the definitions and conventions that sometimes we implicitly assume when using these tools.\n\nThey summarize and extend some of the interesting discussions we had in the \n\ngtsam-users group. We hope that such space will continue to bring to the table relevant questions shared by all of us.","type":"content","url":"/content/blogs/2021/2021-02-23-uncertainties-part1#introduction","position":3},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 1: Linear and nonlinear","lvl2":"A simple example: a pose graph"},"type":"lvl2","url":"/content/blogs/2021/2021-02-23-uncertainties-part1#a-simple-example-a-pose-graph","position":4},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 1: Linear and nonlinear","lvl2":"A simple example: a pose graph"},"content":"As a motivation, we will use a similar pose graph to those used in other GTSAM examples:\n\n\n\n\n\nWe consider a robot moving on the 2D plane (top), which has an odometer that provides relative measurements of the robot's displacement. The graph at the bottom represent the factor graph model. Modeling the odometry factor in a consistent way is the main topic we will cover in these posts.\n\nTo start, we will consider that the variables in the graph \\mathbf{x}_i\ncorrespond to positions {\\mathbf{x}_{i}} = (x,y) \\in \\mathbb{R}^2. The variables are related by a linear model given by a transition matrix \\mathbf{A}_{i} and a constant term \\mathbf{b}_{i} = (b_x, b_y), which can be obtained from some sensor such as a wheel odometer. We can then establish the following relationship between variables i and i+1:\\mathbf{x}_{i+1} = \\mathbf{A}_{i} \\mathbf{x}_i + \\mathbf{b}_i\n\nHowever, we know that in reality things do not work that way, and we will usually have errors produced by noise in our sensors or actuators. The most common way to address this problem is adding some zero-mean Gaussian noise \\eta_i\\sim Gaussian(\\mathbf{0}_{2\\times1}, \\Sigma_i) to our model to represent this uncertainty:\\mathbf{x}_{i+1} = \\mathbf{A}_{i}\\mathbf{x}_i + \\mathbf{b}_i + \\eta_i\n\nWe can recognize here the typical motion or process model we use in Kalman filter for instance, that describes how our state evolves. We say that the noise we introduced on the right side states that our next state \\mathbf{x}_{i+1} will be around \\mathbf{A}_{i}\\mathbf{x}_i + \\mathbf{b}_i, and the covariance matrix \\Sigma_i describes the region where we expect \\mathbf{x}_{i+1} to lie.\n\nWe can also notice that with a bit of manipulation, it is possible to establish the following relationship:\\eta_i  = \\mathbf{x}_{i+1} - \\mathbf{A}_{i}\\mathbf{x}_i - \\mathbf{b}_i\n\nThis is an important expression because we know that the left-hand expression follows a Gaussian distribution. But since we have an equivalence, the right-hand term must do as well. We must note here that what distributes as a Gaussian is neither \\mathbf{x}_{i} nor \\mathbf{x}_{i+1}, but the difference (\\mathbf{x}_{i+1} - \\mathbf{A}_{i}\\mathbf{x}_i - \\mathbf{b}_i). This allows us to use the difference as an  odometry factor that relates \\mathbf{x}_i and \\mathbf{x}_{i+1} probabillistically in our factor graph:(\\mathbf{x}_{i+1} - \\mathbf{A}_{i}\\mathbf{x}_i - \\mathbf{b}_i) \\sim Gaussian(\\mathbf{0}_{2\\times1}, \\Sigma_i)","type":"content","url":"/content/blogs/2021/2021-02-23-uncertainties-part1#a-simple-example-a-pose-graph","position":5},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 1: Linear and nonlinear","lvl3":"Analyzing the solution","lvl2":"A simple example: a pose graph"},"type":"lvl3","url":"/content/blogs/2021/2021-02-23-uncertainties-part1#analyzing-the-solution","position":6},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 1: Linear and nonlinear","lvl3":"Analyzing the solution","lvl2":"A simple example: a pose graph"},"content":"Solving the factor graph using the previous expression for the odometry factors is equivalent to solve the following least squares problem under the assumption that all our factors are Gaussian and we use maximum-a-posteriori (MAP) estimation, which is fortunately our case:\\mathcal{X}^{*} = \\argmin\\displaystyle\\sum_{i} || \\mathbf{A}_i \\mathbf{x}_{i} + \\mathbf{b}_{i} - {\\mathbf{x}_{i+1}} ||^{2}_{\\Sigma_i}\n\nPlease note here that while we swapped the terms in the factor to be consistent with GTSAM’s documentation, it does not affect the formulation since the term is squared.\n\nThe previous optimization problem is linear with respect to the variables, hence solvable in closed form. By differentiating the squared cost, setting it to zero and doing some manipulation, we end up with the so-called normal equations, which are particularly relevant for our posterior analysis:=\n\\mathbf{A}^{T} \\Sigma^{-1} \\mathbf{A}\\ \\mathcal{X}^{*} = \\mathbf{A}^{T} \\Sigma^{-1} \\mathbf{b}\n\nwhere the vector \\mathcal{X}^{*} stacks all the variables in the problem, \\mathbf{A} is a matrix that stacks the linear terms in the factors, and \\mathbf{b} does the same for the constant vector terms.\n\nFirst point to note here is that finding the solution \\mathcal{X}^{*} requires to invert the matrix \\mathbf{A}^{T} \\Sigma^{-1} \\mathbf{A}, which in general is hard since it can be huge and dense in some parts. However we know that there are clever ways to solve it, such as iSAM and iSAM2 that GTSAM already implements, which are covered in \n\nthis comprehensive article by Frank Dellaert and Michael Kaess.\n\n\n\n\n\nWhen solving the normal equations, the left-hand side is known as the Fisher information matrix or Hessian. Its inverse approximates the covariance of the least squares solution.\n\nOur interest in this matrix, though, known as Fisher information matrix or Hessian (since it approximates the Hessian of the original quadratic cost), is that its inverse \\Sigma^{*} = (\\mathbf{A}^{T} \\Sigma^{-1} \\mathbf{A})^{-1} also approximates the covariance of our solution - known as Laplace approximation in machine learning (\n\nBishop (2006), Chap. 4.4). This is a quite important result because by solving the factor graph we are not only recovering an estimate of the mean of the solution but also a measure of its uncertainty. In GTSAM, the Marginals class implements this procedure and a example can be further explored in the \n\ntutorial.\n\nAs a result, we can say that after solving the factor graph the probability distribution of the solution is given by Gaussian(\\mathcal{X}^{*}, \\Sigma^{*}).","type":"content","url":"/content/blogs/2021/2021-02-23-uncertainties-part1#analyzing-the-solution","position":7},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 1: Linear and nonlinear","lvl2":"Getting nonlinear"},"type":"lvl2","url":"/content/blogs/2021/2021-02-23-uncertainties-part1#getting-nonlinear","position":8},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 1: Linear and nonlinear","lvl2":"Getting nonlinear"},"content":"The previous pose graph was quite simple and probably not applicable for most of our problems. Having linear factors as the previous one is an impossible dream for most of the applications. It is more realistic to think that our state i+1 will evolve as a nonlinear function of the state i and the measurements, which is more general than the formerly assummed linear model \\mathbf{A}_{i} \\mathbf{x}_i + \\mathbf{b}_i:\\mathbf{x}_{i+1} = f(\\mathbf{x}_i)\n\nDespite nonlinear, we can still say that our mapping has some errors involved, which are embeded into a zero-mean Gaussian noise that we can simply add as before:\\mathbf{x}_{i+1} = f(\\mathbf{x}_i) + \\eta_i\n\nFollowing a similar procedure as before isolating the noise, we can reformulate the problem as the following nonlinear least squares (NLS) problem:\\mathcal{X} = \\argmin \\displaystyle\\sum_i || f(\\mathbf{x}_i) - \\mathbf{x}_{i+1} ||^2_{\\Sigma_i}\n\nSince the system is not linear with respect to the variables anymore we cannot solve it in close form. We need to use nonlinear optimization algorithms such as \n\nGauss-Newton, \n\nLevenberg-Marquardt or \n\nDogleg. They will try to approximate our linear dream by linearizing the residual with respect to some linearization or operation point given by a guess \\mathcal{\\bar{X}}^{k} valid at iteration k:f(\\mathbf{x}_i) \\approx f(\\mathbf{\\bar{x}}^{k}) + \\mathbf{H}(\\mathbf{\\bar{x}}^{k})\\mathbf{x}_i = \\mathbf{b}_k + \\mathbf{H}^k \\mathbf{x}_i\n\nwhere \\mathbf{H}(\\mathbf{\\bar{x}}^{k}) denotes the Jacobian of f(\\cdot) evaluated at the linearization point \\mathbf{\\bar{x}}^{k}. Hence the problem becomes:\\delta\\mathcal{X}^{k}= \\argmin \\displaystyle\\sum_i || \\mathbf{H}^k\\mathbf{x}_i + \\mathbf{b}_i - \\mathbf{x}_{i+1}  ||^2_{\\Sigma_i^k}\n\nIt is important to observe here that we are not obtaining the global solution \\mathcal{X}, but just a small increment \\delta\\mathcal{X}^{k} that will allow us to move closer to some minima that depends on the initial values. This linear problem can be solved in closed form as we did before. In fact, the normal equations now become something similar to the linear case:(\\mathbf{H}^k)^{T} (\\Sigma^{k})^{-1}\\ \\mathbf{H}^k\\ \\delta\\mathcal{X} = (\\mathbf{H}^k)^{T} (\\Sigma^{k})^{-1} \\mathbf{b}\n\nwhere \\mathbf{H}^k is built by stacking the Jacobians at iteration k. The solution \\delta\\mathcal{X}^{k} will be used to update our current solution at iteration k using the update rule:\\mathcal{X}^{k+1} = \\mathcal{X}^k + \\delta\\mathcal{X}^{k}\n\nwhere \\mathcal{X}^{k+1} corresponds to the best estimate so far, and can be used as a new linearization point for a next iteration.\n\nSimilarly, the expression on the left-hand side \\Sigma^{k+1} = ((\\mathbf{H}^k)^{T} (\\Sigma^{k})^{-1} \\mathbf{H}^k)^{-1} also corresponds to the Fisher information or Hessian as before, but with respect to the linearization point. This is quite important because both the best solution so far \\mathcal{X}^{k+1} and its associated covariance \\Sigma^{k+1} will be valid only at this linearization point and will change with every iteration of the nonlinear optimization. But understanding this, we still can say that at iteration k+1 our best solution will follow a distribution Gaussian(\\mathcal{X}^{k+1}, \\Sigma^{k+1}).","type":"content","url":"/content/blogs/2021/2021-02-23-uncertainties-part1#getting-nonlinear","position":9},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 1: Linear and nonlinear","lvl2":"Conclusions"},"type":"lvl2","url":"/content/blogs/2021/2021-02-23-uncertainties-part1#conclusions","position":10},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 1: Linear and nonlinear","lvl2":"Conclusions"},"content":"In this first post we reviewed the basics ideas to solve factor graphs and extract estimates from them. We showed how the linear problem described by the normal equations not only allows us to find the solution, but also encodes information about its covariance - given by the inverse of the Fisher information matrix. Additionally, in the context of nonlinear problems, we showed how such covariance is only valid around the linearization point.\n\nIn our \n\nnext post we will review how these ideas generalize when we are not estimating vector variables anymore but other objects such as rotation matrices and rigid-body transformations - generally known as manifolds. While this will introduce a bit of complexity not only in the maths but also the notation, by being explicit about our conventions we will be able to consistently solve a wider set of problems usually found in robotics and computer vision.","type":"content","url":"/content/blogs/2021/2021-02-23-uncertainties-part1#conclusions","position":11},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 2: Frames and manifolds"},"type":"lvl1","url":"/content/blogs/2021/2021-02-23-uncertainties-part2","position":0},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 2: Frames and manifolds"},"content":"Author: \n\nMatias Mattamala\n\n custom latex commands here $ \\DeclareMathOperator*{\\argmin}{argmin} \\newcommand{\\coloneqq}{\\mathrel{:=}} $ horizontal scrolling  - TOC   \n\n{:toc}","type":"content","url":"/content/blogs/2021/2021-02-23-uncertainties-part2","position":1},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 2: Frames and manifolds","lvl2":"Introduction"},"type":"lvl2","url":"/content/blogs/2021/2021-02-23-uncertainties-part2#introduction","position":2},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 2: Frames and manifolds","lvl2":"Introduction"},"content":"In our \n\nprevious post we discussed some basic concepts to solve linear and nonlinear factor graphs and to extract uncertainty estimates from them. We reviewed that the Fisher information matrix corresponds to an approximation of the inverse covariance of the solution, and that in the nonlinear case, both the solution and its covariance estimate are valid for the current linearization point only.\n\nWhile the nonlinear case effectively allows us to model a bunch of problems, we need to admit we were not too honest when we said that it was everything we needed to model real problems. Our formulation so far assumed that the variables in our factor graph are vectors, which is not the case for robotics and computer vision at least.\n\nThis second part will review the concept of manifold and how the reference frames affect the formulation of our factor graph. The tools we cover here will be also useful to define probability distributions in a more general way, as well as to extend our estimation framework to more general problems.","type":"content","url":"/content/blogs/2021/2021-02-23-uncertainties-part2#introduction","position":3},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 2: Frames and manifolds","lvl2":"Getting non-Euclidean"},"type":"lvl2","url":"/content/blogs/2021/2021-02-23-uncertainties-part2#getting-non-euclidean","position":4},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 2: Frames and manifolds","lvl2":"Getting non-Euclidean"},"content":"In robotics we do not estimate the state of a robot only by their position but also its orientation. Then we say that is its pose, i.e position and orientation together, what matters to define its state.\n\nRepresenting pose is a tricky thing. We could say “ok, but let’s just append the orientation to the position vector and do everything as before” but that does not work in practice. Problem arises when we want to compose two poses \\mathbf{T}_1 = (x_1, y_1, \\theta_1) and \\mathbf{T}_2 = (x_2, y_2, \\theta_2). Under the vector assumption, we can write the following expression as the composition of two poses:\\mathbf{T}_1 + \\mathbf{T}_2 = \\begin{bmatrix} x_1\\\\ y_1 \\\\ \\theta_1 \\end{bmatrix}  +  \\begin{bmatrix} x_2 \\\\ y_2 \\\\ \\theta_2\\end{bmatrix}  = \\begin{bmatrix} x_1 + x_2 \\\\ y_1+y_2 \\\\ \\theta_1 + \\theta_2\\end{bmatrix}\n\nThis is basically saying that it does not matter if we start in pose \\mathbf{T}_1 or \\mathbf{T}_2, we will end up at the same final pose by composing both, because in vector spaces we can commute the elements. But this does not work in reality, because rotations and translations do not commute. A simple example is that if you are currently sitting at your desk,  and you stand up, rotate 180 degrees and walk a step forward, is completely different to stand up, walk a step forward and then rotate 180 degrees (apart from the fact that you will hit your desk if you do the latter).\n\nSo we need a different representation for poses that allow us to describe accurately what we observe in reality. Long story short, we rather have to represent planar poses as 3\\times3 matrices known as rigid-body transformations:\\mathbf{T}_1 = \\left[\\begin{matrix} \\cos{\\theta_1} && -\\sin{\\theta_1} && x_1 \\\\ \\sin{\\theta_1} && \\cos{\\theta_1} && y_1 \\\\ 0 && 0 && 1\\end{matrix} \\right] = \\left[\\begin{matrix} \\mathbf{R}_1 && \\mathbf{t}_1 \\\\ 0 && 1\\end{matrix}\\right]\n\nHere \\mathbf{R}_1 = \\left[ \\begin{matrix} \\cos{\\theta_1} && -\\sin{\\theta_1}\\\\ \\sin{\\theta_1} && \\cos{\\theta_1}\\end{matrix} \\right]\n\n is a 2D rotation matrix, while \\mathbf{t}_1 = \\left[ \\begin{matrix}x_1 \\\\ y_1 \\end{matrix}\\right] is a translation vector.\nWhile we are using a 3\\times3 matrix now to represent the pose, its degrees of freedom are still 3, since it is a function of (x_1, y_1, \\theta_1).\n\nWorking with transformation matrices is great, because we can now describe the behavior we previously explained with words using matrix operations. If we start in pose \\mathbf{T}_1 and we apply the transformation \\mathbf{T}_2:\\mathbf{T}_1 \\mathbf{T}_2 = \\left[\\begin{matrix} \\mathbf{R}_1 && \\mathbf{t}_1 \\\\ 0 && 1\\end{matrix}\\right] \\left[\\begin{matrix} \\mathbf{R}_2 && \\mathbf{t}_2 \\\\ 0 && 1\\end{matrix}\\right] = \\left[\\begin{matrix} \\mathbf{R}_1 \\mathbf{R}_2 && \\mathbf{R}_1 \\mathbf{t}_2 +  \\mathbf{t}_1 \\\\ 0 && 1\\end{matrix}\\right]\n\nWe can now rewrite the same problem as before but now using our transformation matrices:\\mathbf{T}_{i+1} = \\mathbf{T}_i \\ \\Delta\\mathbf{T}_i\n\nHere we are saying that the next state \\mathbf{T}_{i+1}\n\n is gonna be the previous state \\mathbf{T}_{i}\n\n plus some increment \\Delta\\mathbf{T}_i\n\n given by a sensor -odometry in this case. Please note that since we are now working with transformation matrices, \\Delta\\mathbf{T}_i\n\n is almost everything we need to model the problem more accurately, since it will represent a change in both position and orientation.\n\nNow, there are two things here that we must discuss before moving on, which are fundamental to make everything consistent. We will review them carefully now.","type":"content","url":"/content/blogs/2021/2021-02-23-uncertainties-part2#getting-non-euclidean","position":5},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 2: Frames and manifolds","lvl2":"The importance of reference frames"},"type":"lvl2","url":"/content/blogs/2021/2021-02-23-uncertainties-part2#the-importance-of-reference-frames","position":6},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 2: Frames and manifolds","lvl2":"The importance of reference frames"},"content":"While some people (myself included) prefer to write the process as we did before, applying the increment on the right-hand side:\\mathbf{T}_{i+1} = \\mathbf{T}_i \\ \\Delta\\mathbf{T}_i\n\nOthers do it in a different way, on the left-hand side:\\mathbf{T}_{i+1} = \\Delta\\mathbf{T}_i \\ \\mathbf{T}_i\n\nThese expressions are not equivalent as we already discussed because rigid-body transformations do not commute. However, both make sense under specific conventions. We will be more explicit now by introducing the concept of reference frames in our notation. We will use the notation presented by Paul Furgale in his blog post \n\n“Representing Robot Pose: The good, the bad, and the ugly”.\n\nLet us say that the robot trajectory in space is expressed in a fixed frame; we will called it the world frame W. The robot itself defines a coordinate frame in its body, the body frame B.\n\nThen we can define the pose of the robot body B expressed in the world frame W by using the following notation:\\mathbf{T}_{WB}\n\nAnalogously, using we can express the pose of the origin/world W expressed in the robot frame B:\\mathbf{T}_{BW}\n\nAccording to this formulation, the first is a description from a fixed, world frame while the left-hand one does it from the robot’s perspective. The interesting thing is that we can always go from one to the other by inverting the matrices:\\mathbf{T}_{WB}^{-1} = \\mathbf{T}_{BW}\n\nSo the inverse swaps the subindices, effectively changing our point-of-view to describe the world.\n\nUsing one or the other does not matter. The important thing is to stay consistent.\nIn fact, by making the frames explicit in our convention we can always check if we are making mistakes. Let us say we are using the first definition, and our odometer is providing increments relative to the previous body state. Our increments \\Delta\\mathbf{T}_i\n\n will be \\Delta\\mathbf{T}_{B_{i} B_{i+1} }\n\n, i.e, it is a transformation that describes the pose of the body at time i+1 expressed in the previous one i.\n\nHence, the actual pose of the body at time i+1 expressed in the world frame W is given by matrix multiplication of the increment on the right side:\\mathbf{T}_{WB_{i+1}} = \\mathbf{T}_{WB_i} \\ \\Delta\\mathbf{T}_{B_{i} B_{i+1} }\n\nWe know this is correct because the inner indices cancel each other, effectively representing the pose at i+1 in the frame we want:\\mathbf{T}_{WB_{i+1}} = \\mathbf{T}_{W {\\bcancel{B_i}} } \\ \\Delta\\mathbf{T}_{ {\\bcancel{B_{i}}} B_{i+1} }\n\nSince we applied the increment from the right-hand side, we will refer to this as the right-hand convention.\n\n\n\n\n\nWhen using right-hand convention, the increments to describe the pose of the robot at instant $i$ are applied on the right-hand side. All our estimates are expressed with respect to a fixed world frame $W$ (in black).\n\nIf we want to apply the increment to \\mathbf{T}_{BW}, we would need to invert the measurement:\\Delta\\mathbf{T}_{B_{i+1} B_{i}} = \\Delta\\mathbf{T}_{B_{i} B_{i+1}}^{-1}\n\nso we can apply the transformation on the left-hand side:\\mathbf{T}_{B_{i+1}W} = \\Delta\\mathbf{T}_{B_{i+1} B_{i}} \\mathbf{T}_{B_i W}\n\nwhich we will refer onward as left-hand convention.\n\n\n\n\n\nWith left-hand convention the increments are applied from the left, changing our reference frame to the new robot pose $B_{i+1}$ (in black) .\n\nIn GTSAM we use the right-hand convention: we assume that the variables are expressed with respect to a fixed frame W, hence the increments are applied on the right-hand side.\n\nIt is important to have this in mind because all the operations that are implemented follow this. As a matter of fact, 3D computer vision has generally used left-hand convention because it is straightforward to apply the projection models from the left:{_I}\\mathbf{p} = \\mathbf{K}_{IC}\\ \\mathbf{T}_{CW}\\ {_{W}}\\mathbf{P}\n\nHere we made explicit all the frames involved in the transformations we usually can find in textbooks: An homogeneous point {_{W}}\\mathbf{P}\n\n expressed in the world frame W, is transformed by the extrinsic calibration matrix \\mathbf{T}_{CW}\n\n (a rigid body transformation ) which represents the world W in the camera frame C, producing a vector {_{C}}\\mathbf{P}\n\n in the camera frame (not shown). This is projected onto the image by means of the intrinsic calibration matrix \\mathbf{K}_{IC}\n\n, producing the vector _I\\mathbf{p}\n\n, expressed in the image frame I.\n\nSince in GTSAM the extrinsic calibration is defined the other way to be consistent with the right-hand convention, i.e \\mathbf{T}_{WC}, the implementation of the CalibratedCamera handles it by \n\nproperly inverting the matrix before doing the projection as we would expect:{_I}\\mathbf{p} = \\mathbf{K}_{IC}\\ (\\mathbf{T}_{WC})^{-1}\\ {_{W}}\\mathbf{P}","type":"content","url":"/content/blogs/2021/2021-02-23-uncertainties-part2#the-importance-of-reference-frames","position":7},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 2: Frames and manifolds","lvl2":"They are not simply matrices"},"type":"lvl2","url":"/content/blogs/2021/2021-02-23-uncertainties-part2#they-are-not-simply-matrices","position":8},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 2: Frames and manifolds","lvl2":"They are not simply matrices"},"content":"The second important point we need to discuss, is that while rigid-body transformations are nice, the new expression we have to represent the process presents some challenges when trying to use it in our factor graph using our previous method:\\mathbf{T}_{WB_{i+1}} = \\mathbf{T}_{WB_i} \\ \\Delta\\mathbf{T}_{B_{i} B_{i+1} }\n\nIn first place, we need to figure out a way to include the noise term \\eta_i\n\n we used before to handle the uncertainties about our measurement \\Delta\\mathbf{T}_{B_{i} B_{i+1} }\n\n, which was also the trick we used to generate the Gaussian factors we use in our factor graph. For now, we will say that the noise will be given by a matrix \\mathbf{N}_i \\in \\mathbb{R}^{4\\times4}\n\n, which holds the following:\\mathbf{T}_{WB_{i+1}} = \\mathbf{T}_{WB_i} \\ \\Delta\\mathbf{T}_{B_{i} B_{i+1}} \\mathbf{N}_i\n\nSecondly, assuming the noise \\mathbf{N}_i\n\n was defined somehow, we can isolate it as we did before. However, we need to use matrix operations now:\\mathbf{N}_i = (\\mathbf{T}_{WB_i} \\ \\Delta\\mathbf{T}_{B_{i} B_{i+1}})^{-1} \\mathbf{T}_{WB_{i+1}}\n\nand we can manipulate it a bit for clarity:\\mathbf{N}_i = (\\Delta\\mathbf{T}_{B_{i} B_{i+1}})^{-1} (\\mathbf{T}_{WB_i})^{-1} \\mathbf{T}_{WB_{i+1}}\n\nIf we do the subindex cancelation trick we did before, properly swapping the indices due to the inverses, we can confirm that the error is defined in frame B_{i+1}\n\n and expressed in frame B_{i+1}. This may seen counterintuitive, but in fact describes what we want: the degree of mismatch between our models and measurements to express the pose at frame B_{i+1}\n\n, which ideally should be zero.\n\nHowever, the error is still a matrix, which is impossible to include as a factor in the framework we have built so far. We cannot compute a vector residual as we did before, nor we are completely sure that the matrix \\mathbf{N}_i\n\n follows some sort of Gaussian distribution.\n\nWe need to define some important concepts used in GTSAM to get a better understanding on how the objects are manipulated.","type":"content","url":"/content/blogs/2021/2021-02-23-uncertainties-part2#they-are-not-simply-matrices","position":9},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 2: Frames and manifolds","lvl3":"Some objects are groups","lvl2":"They are not simply matrices"},"type":"lvl3","url":"/content/blogs/2021/2021-02-23-uncertainties-part2#some-objects-are-groups","position":10},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 2: Frames and manifolds","lvl3":"Some objects are groups","lvl2":"They are not simply matrices"},"content":"First of all, some objects used in GTSAM are \n\ngroups. Rigid-body transformations Pose3 and Pose2 in GTSAM), rotation matrices (Rot2 and Rot3) and quaternions, and of course vector spaces (Point2 and Point3) have some basic operations that allow us to manipulate them. These concepts \n\nhave been discussed in previous posts at the implementation level:\n\nComposition: How to compose 2 objects from the same manifold, with associativity properties. It is similar to the addition operation for vectors.\n\nIdentity: An identity operator under the composition/between.\n\nInverse: An element that when composed with its inverse becomes the identity.\n\nThe previous 3 operations allows us to define the operation between, which computes the difference between 2 objects from the same group, similarly to a subtraction operation.\n\nSince the operations can be defined in different ways for each kind of object (it is different to compose rotation matrices than quaternions or vectors for instance), some authors define special operators to refer to composition/between operations. Some examples are the box-plus \\boxplus for composition and box-minus \\boxminus for between as done by \n\nHertzberg et al., \n\nBlanco-Claraco, \n\nBloesch et al., and the \n\nKindr library.\n\nHowever, composition can be defined from the left or the right side because composition is associative but is not distributive. This is the same problem we described before when talking about reference frames and how to add small increments with the left and right hand conventions, since both are valid depending on our decisions or other authors’. While we can be clear about our own decisions, it is important to be aware of the definitions of each author because sometimes are not clearly stated. \n\nSolà et al for example make the difference explicit by defining left-\\oplus and right-\\oplus for composition using left-hand convention or right-hand convention respectively, but we need to be careful to recognize each other’s choices.\n\nWe recall again that in GTSAM and the rest of this post we use the right convention (pun intended), because we represent our poses with respect to a fixed world frame W and the increments are defined with respect to the body frame B.","type":"content","url":"/content/blogs/2021/2021-02-23-uncertainties-part2#some-objects-are-groups","position":11},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 2: Frames and manifolds","lvl3":"Some are manifolds","lvl2":"They are not simply matrices"},"type":"lvl3","url":"/content/blogs/2021/2021-02-23-uncertainties-part2#some-are-manifolds","position":12},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 2: Frames and manifolds","lvl3":"Some are manifolds","lvl2":"They are not simply matrices"},"content":"Additionally, rigid-body transformations, rotation matrices, quaternions and even vectors are differentiable manifolds. This means that even though they do not behave as Euclidean spaces at a global scale, they can be locally approximated as such by using local vector spaces called tangent spaces. The main advantage of analyzing all these objects from the manifold perspective is that we can build general algorithms based on common principles that apply to all of them.\n\n\n\n\n\nWhile a manifold have a non-Euclidean structure, it can be locally approximated by tangent spaces. In the figure, we illustrate different tangent spaces defined at the objects $\\mathbf{p}_1$ and $\\mathbf{p}_2$ on the manifold.\n\nAs we briefly mentioned before, objects such as rotation matrices are difficult to manipulate in the estimation framework because they are matrices. A 3D rotation matrix \\mathbf{R} represents 3 orientations with respect to a reference frame but, in raw terms, they are using 9 values to do so, which seems to overparametrize the object. However, the constraints that define a rotation matrix -and consequently the manifold- such as orthonormality \\mathbf{R}^{T}\\mathbf{R} = \\mathbf{I}\n\n and \\text{det}(\\mathbf{R}) = 1\n\n make the inherent dimensionality of the rotation still 3. Interestingly, this is exactly the dimensionality of the tangent spaces that can be defined over the manifold.\n\nThat is what makes working with manifolds so convenient: All the constraints that are part of the definition of the object are naturally handled, and we can work in tangent vector spaces using their inherent dimension. The same happens for rigid-body transformations (6 dimensions represented by a 16 elements matrix), quaternions (3 orientations represented by a 4D vector), and even objects that are not groups, such as unit vectors.\n\nIn order to work with manifolds, we need to define 2 operations, which are the key to transform objects between them and the tangent spaces:\n\nRetract or retraction: An operation that maps elements \\mathbf{\\xi} from the tangent space at \\mathbf{p}_1 to the manifold: \\mathbf{p} = \\text{retract}_{\\mathbf{p}_1}(\\mathbf{\\xi}).\n\nLocal: the opposite operation: mapping elements \\mathbf{p} from the manifold to the tangent space \\mathbf{\\xi} = \\text{local}_{\\mathbf{p}_1}(\\mathbf{p})\n\n\n\n\n\nThe retract operation maps an element $\\mathbf{\\xi}$ defined in the tangent space at $\\mathbf{p}_1$ back to the manifold.\n\n\n\n\n\nAn alternative figure to represent the retraction that will be used in the rest of the post, since it shows the interactions between the manifold and the tangent space more explicitly. We can clearly observe that the element $\\mathbf{p}$ generated by the retraction is also defined with respect to $\\mathbf{p}_1$ on the manifold.\n\n\n\n\n\nThe local operation does the opposite, and maps an element $\\mathbf{p}$ defined with respect to $\\mathbf{p}_1$ as an vector $\\mathbf{\\xi}$ in the tangent space.\n\nRetractions are the fundamental concept to \n\nsolve optimization problems and to define uncertainties on manifolds. While the former will be covered later in this post with an example, it is useful to explain the latter now. The general idea is that we can define distributions on the tangent space, and map them back on the manifold using the retraction. For instance, we can define a zero-mean Gaussian variable \\eta \\sim Gaussian(\\mathbf{0}_{n\\times1}, \\Sigma)\n\n in the tangent space centered at \\mathbf{p} and use the retraction:Gaussian(\\mathbf{p}_1, \\mathbf{\\eta}) := \\text{retract}_{\\mathbf{p}_1}(\\mathbf{\\eta})\n\nwhich graphically corresponds to:\n\n\n\n\n\nUsing the retraction, we can define Gaussians on the tangent space centered at some element $\\mathbf{p}_1$ and map them back on the manifold to construct Gaussians on the manifold with mean $\\mathbf{p}_1$ and covariance $\\text{Cov}(\\eta) = \\Sigma$.\n\nPlease note that we have defined the retraction from the right, since this matches the convention used by GTSAM, which coincidently also matches the composition operator for groups. However, in the literature we can find different definitions: \n\nBarfoot and Furgale (2014, left-hand convention), \n\nForster et al (2017, right-hand convention), and \n\nMangelson et al. (2020, left-hand convention) are some examples. Other definitions to define probability distributions on manifolds include \n\nCalinon (2020) and \n\nLee et al. (2008), please refer to their work for further details.","type":"content","url":"/content/blogs/2021/2021-02-23-uncertainties-part2#some-are-manifolds","position":13},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 2: Frames and manifolds","lvl3":"And others are both: Lie groups","lvl2":"They are not simply matrices"},"type":"lvl3","url":"/content/blogs/2021/2021-02-23-uncertainties-part2#and-others-are-both-lie-groups","position":14},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 2: Frames and manifolds","lvl3":"And others are both: Lie groups","lvl2":"They are not simply matrices"},"content":"In GTSAM we only need the objects to be differentiable manifolds in order to optimize them in the estimation framework, being groups is not a requirement at all. However, it is important to be aware that some of the objects we deal with are both groups and differentiable manifolds, known as \n\nLie groups, which is the formulation generally found in state estimation literature.\n\n\n\n\n\nLie groups are both groups and differentiable manifolds. They define a tangent space at the identity that allows to represent any object on the manifold through a retraction defined at the identity, known as exponential map, as well as a local operation also at the identity - the logarithm map.\n\nObjects such as rigid-body matrices and quaternions are Lie groups. 2D rigid-body transformations are elements of the Special Euclidean group \\text{SE(2)}, while 2D and 3D rotations are from the Special Orthogonal groups \\text{SO(2)} and \\text{SO(3)} respectively. 3D rigid-body transformations are objects of \\text{SE(3)} and we can use those definitions to define the operations we described before for groups and manifolds:\n\nComposition: Matrix multiplication \\mathbf{T}_{1} \\ \\mathbf{T}_{2}.\n\nIdentity: Identity matrix \\mathbf{I}.\n\nInverse: Matrix inverse (\\mathbf{T}_{1})^{-1}\n\nRetract: The retraction is defined at the tangent space at the identity, which is known as the exponential map of \\text{SE(3)}: \\text{retract}_{\\mathbf{I}}(\\mathbf{\\xi}) := \\text{Exp}(\\mathbf{\\xi}).\n\nLocal: It is also defined at the identity and known as the logarithm map of \\text{SE(3)}: \\text{local}_{\\mathbf{I}} := \\text{Log}(\\mathbf{T}_{1} ).\n\nPlease note here that we used capitalized \\text{Log}(\\cdot) := \\text{log}( \\cdot)^{\\vee}\n\n and \\text{Exp}(\\cdot):=\\text{exp}( (\\cdot)^{\\wedge})\n\n operators for simplicity as used by \n\nForster et al (2017), and \n\nSolà et al. (2020), since they are easy to understand under the retractions perspective. Refer to Solà et al. for a more detailed description, including the relationship between tangents spaces and the Lie algebra.\n\nIn GTSAM, 3D poses are implemented as Pose3 objects and we can think of them as \\text{SE(3)} elements. Therefore, to keep things simple, we will stay using the logarithm map and exponential map to talk about their retraction and local operators. However, GTSAM also allows us to use alternative retractions for some cases, which would be like using other Lie groups to represent poses, such as the \\mathbb{R}^{3}\\times\\text{SO}(3) group, which is explained \n\nhere.","type":"content","url":"/content/blogs/2021/2021-02-23-uncertainties-part2#and-others-are-both-lie-groups","position":15},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 2: Frames and manifolds","lvl3":"Reference frames on manifolds","lvl2":"They are not simply matrices"},"type":"lvl3","url":"/content/blogs/2021/2021-02-23-uncertainties-part2#reference-frames-on-manifolds","position":16},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 2: Frames and manifolds","lvl3":"Reference frames on manifolds","lvl2":"They are not simply matrices"},"content":"Reference frames are preserved when applying the local and retract operations. We will cover a few important ideas using \\text{SE(3)}\n\n, since it is related to our original problem of pose estimation.\n\nFirst of all, when we talked about the tangent space defined at the identity, in physical terms it refers to having a fixed, global frame, which we use to express our poses. Then, when using the local operation defined as the logarithm map of \\text{SE(3)}\n\n, we obtain a vector _W\\mathbf{\\xi}_{W} \\in \\mathbb{R}^{6}\n\n (sometimes also called tangent vector or twist), which is defined in the tangent space at the world frame:{_W}\\mathbf{\\xi} = \\text{Log}(\\mathbf{T}_{WB_i} )\n\n\n\n\n\nIn this example, the tangent space at the identity represents the world frame, on which any transformation $\\mathbf{T}_{WB_i}$ expressed in the world frame can be defined.\n\nAdditionally, we can add incremental changes to a transformation using the retraction, which for \\text{SE(3)}\n\n is done with the exponential map as follows:\\mathbf{T}_{WB_{i+1}} = \\mathbf{T}_{WB_i} \\text{Exp}( {_{B_i}}\\mathbf{\\xi})\n\nIn this case we added an increment from the body frame at time i, that represents the new pose at time i+1. Please note that the increments are defined with respect to a reference frame, but they do not require to specify the resulting frame. Their meaning (representing a new pose at time i+1) is something that we -as users- define but is not explicit in the formulation. (While we could do it, it can lead to confusions because in this specific case we are representing the pose at the next instant but we can also use retractions to describe corrections to the world frame as we will see in the final post.)\n\nThe graphical interpretation with the manifold is consistent with our general definition of retractions and frames. Using the exponential map on the right is defining a tangent space at \\mathbf{T}_{WB_i}\n\n, which can be interpreted as a new reference frame at B_i\n\n, which we used to define the increment:\n\n\n\n\n\nWhen we add a small increment ${_{B_i}}\\mathbf{\\xi}$ to the pose $\\mathbf{T}_{WB_i}$, the expression $\\mathbf{T}_{WB_i} \\text{Exp}( {_{B_i}}\\mathbf{\\xi})$ corresponds to the retraction defined for the tangent space at $\\mathbf{T}_{WB_i}$.\n\nThe incremental formulation via retractions is also convenient when we have local (body frame) velocity measurements -also known as twists- ({_{B_i}}\\omega, {_{B_i}}{v})\n\n, with {_{B_i}}\\omega \\in \\mathbb{R}^{3}, {_{B_i}}v \\in \\mathbb{R}^{3}\n\n  and we want to do \n\ndead reckoning:\\mathbf{T}_{WB_{i+1}} = {\\mathbf{T}_{WB_i}} \\text{Exp}\\left(\n  \\begin{bmatrix} _{B_i}\\omega \\\\ _{B_i} v \\end{bmatrix}\\ \n\\delta t \\right)\n\nThe product\\begin{bmatrix} _{B_i}\\omega \\ \\delta t \\\\ _{B_i} v \\ \\delta t\\end{bmatrix}\n\nrepresents the tangent vector resulting from time-integrating the velocity, which is map onto the manifold by means of the \\text{SE(3)} retraction:\n\n\n\n\n\nThe same formulation can be used to map velocities into increments via retractions/exponential map. In this case, the time increment $\\delta t$ controls the length of the curve that is projected on the manifold when we do dead-reckoning.\n\nThis last example is also useful to understand the physical meaning of the retraction/local operations in the case of 3D poses, which is not always clear. An interesting aspect of illustrating this with \\text{SE}(3) is that the meaning also applies if we use rotations only (\\text{SO}(3)\n\n, \\text{SO}(2)) or 2D poses (\\text{SE}(2)):\n\n\n\n\n\nThis figure illustrates the physical meaning of the $\\text{SE}(3)$ retraction of the dead-reckoning example. In light blue we show the increment applied to pose ${\\mathbf{T}_{WB_i}}$, which generates a new frame. At the right hand side we illustrate the decomposed increment: orientation at the top, and position at the bottom. Each component of the orientation increment represents a change with respect to each rotation axis defined by the body pose at ${\\mathbf{T}_{WB_i}}$ (rotations with respect to $X$, $Y$ and $Z$ axes, respectively, are shown). When combined, the vector represents an axis-angle representation of the rotation increment. Position increments (right-side, bottom) are easier to interpret because they only represent a shift in position with respect to ${\\mathbf{T}_{WB_i}}$'s origin. Since the retraction actually combines both orientation and position increments on the manifold, it generates a smooth trajectory as shown on the left-hand side (light blue).","type":"content","url":"/content/blogs/2021/2021-02-23-uncertainties-part2#reference-frames-on-manifolds","position":17},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 2: Frames and manifolds","lvl3":"Some remarks on the retract and local operations","lvl2":"They are not simply matrices"},"type":"lvl3","url":"/content/blogs/2021/2021-02-23-uncertainties-part2#some-remarks-on-the-retract-and-local-operations","position":18},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 2: Frames and manifolds","lvl3":"Some remarks on the retract and local operations","lvl2":"They are not simply matrices"},"content":"Lastly, we would like to share some important remarks that are not always stated in the math but can become real issues during the implementation. First, we need to be careful about the convention of the order of the components in the retraction/local operation (yes, more conventions again). Having clarity about the definition that every software or paper uses for these operations, even implicitly, is fundamental to make sense of the quantities we put into our estimation problems and the estimates we extract.\n\nFor instance, the definition of the \\text{SE(3)} retraction we presented, which matches Pose3 in GTSAM, uses an orientation-then-translation convention, i.e, the 6D tangent vector has orientation in the first three coordinates, and translation in the last three \\mathbf{\\xi} = (\\phi, \\rho), where \\phi \\in \\mathbb{R}^3 denotes the orientation components while \\rho \\in \\mathbb{R}^3 the translational ones. On the other hand, Pose2 uses translation-then-orientation (x, y, \\theta) for \n\nhistorical reasons.\n\nThis is also ties the definition of the covariances. For Pose3 objects, for instance, we must define covariance matrices that match the definition and meaning of the tangent vectors. In fact, the upper-left block must encode orientation covariances, i.e, our degree of uncertainty for each rotation axis, while the bottom-right encodes position covariances:\\left[\\begin{matrix} \\Sigma_{\\phi\\phi} & \\Sigma_{\\phi\\rho} \\\\ \\Sigma_{\\phi\\rho}^{T} & \\Sigma_{\\rho\\rho} \\end{matrix}\\right]\n\nThis is of paramount importance because, for example, the previous covariance matrix does not match the convention used in ROS to store the covariances in \n\nPoseWithCovariance. Similar issues can arise when using different libraries or software, so we must be aware of their conventions. Our advice is to always check the retraction/exponential map definition if not stated in the documentation.","type":"content","url":"/content/blogs/2021/2021-02-23-uncertainties-part2#some-remarks-on-the-retract-and-local-operations","position":19},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 2: Frames and manifolds","lvl2":"Bringing everything together"},"type":"lvl2","url":"/content/blogs/2021/2021-02-23-uncertainties-part2#bringing-everything-together","position":20},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 2: Frames and manifolds","lvl2":"Bringing everything together"},"content":"Now we can return to our original estimation problem using rigid-body transformations. Let us recall that we defined the following process model given by odometry measurements:\\mathbf{T}_{WB_{i+1}} = \\mathbf{T}_{WB_i} \\ \\Delta\\mathbf{T}_{B_{i} B_{i+1}} \\mathbf{N}_i\n\nWe reported two problems before:\n\nWe needed to define the noise \\mathbf{N}_i as a Gaussian noise, but it was a 4\\times4 matrix.\n\nWhen isolating the noise to create the residual, we ended up with a matrix expression, not vector one as we needed in our estimation framework.\n\nWe can identify now that our poses are objects of \\text{SE(3)}, hence we can use the tools we just defined.","type":"content","url":"/content/blogs/2021/2021-02-23-uncertainties-part2#bringing-everything-together","position":21},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 2: Frames and manifolds","lvl3":"Defining the noise","lvl2":"Bringing everything together"},"type":"lvl3","url":"/content/blogs/2021/2021-02-23-uncertainties-part2#defining-the-noise","position":22},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 2: Frames and manifolds","lvl3":"Defining the noise","lvl2":"Bringing everything together"},"content":"The first problem of defining the noise appropriately is solved by using probability distributions on manifolds as we described before. We define  a zero-mean Gaussian in the tangent space at \\Delta\\mathbf{T}_{B_{i} B_{i+1}} and retract it onto the manifold:\\mathbf{T}_{WB_{i+1}} = \\mathbf{T}_{WB_i} \\ \\Delta\\mathbf{T}_{B_{i} B_{i+1}} \\text{Exp}(_{B_{i+1}}\\mathbf{\\eta})\n\n\n\n\n\nGraphical interpretation of the definition of the noise, which is defined in frame $B_{i+1}$ and mapped using the retraction.\n\nwhere we have defined {_{B_{i+1}}}\\eta \\sim Gaussian(\\mathbf{0}_{6\\times1},\\ _{B_{i+1}}\\Sigma)\n\n. Please note that in order to match our right-hand convention, the covariance we use must be defined in the body frame at time i+1, i.e B_{i+1}\n\n. Additionally, the covariance matrix must follow the same ordering defined by the retraction as we mentioned before.","type":"content","url":"/content/blogs/2021/2021-02-23-uncertainties-part2#defining-the-noise","position":23},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 2: Frames and manifolds","lvl3":"Defining the residual","lvl2":"Bringing everything together"},"type":"lvl3","url":"/content/blogs/2021/2021-02-23-uncertainties-part2#defining-the-residual","position":24},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 2: Frames and manifolds","lvl3":"Defining the residual","lvl2":"Bringing everything together"},"content":"Having solved the first problem, we can now focus on the residual definition. We can isolate the noise as we did before, which holds:\\text{Exp}( {_{B_{i+1}}}\\mathbf{\\eta}) = \\Delta\\mathbf{T}_{B_{i} B_{i+1}}^{-1}\\ \\mathbf{T}_{WB_i}^{-1} \\ \\mathbf{T}_{WB_{i+1}}\n\nWe can now apply the local operator of the manifold on both sides to map the residual to the tangent space (recall we are using the logarithm map for \\text{SE(3)} elements for simplicity):_{B_{i+1}}\\mathbf{\\eta} = \\text{Log}\\left( \\Delta\\mathbf{T}_{B_{i} B_{i+1}}^{-1}\\ \\mathbf{T}_{WB_i}^{-1} \\  \\mathbf{T}_{WB_{i+1}} \\right)\n\nSince the noise is defined in the tangent space, both sides denote vector expressions in \\mathbb{R}^{6}. Both also correspond to zero-mean Gaussians, hence the right-hand side can be used as a proper factor in our estimation framework. In fact, the expression on the right side is exactly the same used in GTSAM to define the \n\nBetweenFactor.\n\n\n\n\n\nThe Between residual, illustrated. On the left hand side, we have the corresponding transformations involved in the computation expressed on the manifold. Since the error (in red) is also defined on the manifold, it is not described by a straight line. However, by applying the local operation, given by the logarithm map for $\\text{SE(3)}$, we can map it to to the tangent space at $\\Delta\\mathbf{T}_{B_{i} B_{i+1}}$, which is a vector space (right). The error itself should lie within the Gaussian we created using the noise $_{B_{i+1}}\\mathbf{\\eta}$, which denotes the noise of the odometry model.\n\nWe must also keep in mind here that by using the local operation, the residual vector will follow the same ordering. As we mentioned before for Pose3 objects, it will encode orientation error in the first 3 components, while translation error in the last ones. In this way, if we write the expanded expression for the Gaussian factor, we can notice that all the components are weighted accordingly by the inverse covariance as we reviewed before (orientation first, and then translation):\\begin{split}\n\\mathbf{r}_{\\text{between}}(\\mathbf{T}_{WB_i}, \\mathbf{T}_{WB_{i+1}}) &= \\left|\\left| \\text{Log}\\left( \\Delta\\mathbf{T}_{B_{i} B_{i+1}}^{-1}\\ \\mathbf{T}_{WB_i}^{-1} \\  \\mathbf{T}_{WB_{i+1}} \\right)\\right|\\right|^{2}_{\\Sigma_i} \\\\\n& =\n\\text{Log}\\left( \\Delta\\mathbf{T}_{B_{i} B_{i+1}}^{-1}\\ \\mathbf{T}_{WB_i}^{-1} \\  \\mathbf{T}_{WB_{i+1}} \\right)^{T} \\ \\Sigma_i^{-1} \\\\\n& \\qquad \\qquad \\qquad \\text{Log}\\left( \\Delta\\mathbf{T}_{B_{i} B_{i+1}}^{-1}\\ \\mathbf{T}_{WB_i}^{-1} \\  \\mathbf{T}_{WB_{i+1}} \\right)\n\\end{split}\n\nThe factor is now a nonlinear vector expression that can be solved using the nonlinear optimization techniques we presented before. In fact, GTSAM already implements Jacobians for the local operator of all the objects implemented, which simplifies the process. However, there are subtle differences that we must clarify.\n\nFirst, since the factor defines a residual in the tangent space at the current linearization point, the optimization itself is executed in the tangent space defined in the current linearization point. This means that when we linearize the factors and build the normal equations, the increment {_{B_i}}\\delta\\mathbf{T}^{k} we compute lies in the tangent space.\n\nFor this reason, we need to update the variables on the manifold using the retraction. For example, for \\text{SE(3)}:\\mathbf{\\tilde{T}}_{WB_i}^{k+1} = \\mathbf{T}_{WB_i}^k \\text{Exp}( {_{B_i}}\\delta\\mathbf{T}^{k} )\n\nThe second important point, is that the covariance that we can recover from the information matrix, will be also defined in the tangent space around the linearization point, following the convention of the retraction. It means that if our information matrix at the current linearization point is given by:\\Sigma^{k+1} = ((\\mathbf{H}^k)^{T} (\\Sigma^{k})^{-1} \\mathbf{H}^k)^{-1}\n\nThen, the corresponding distribution of the solution around the current linearization point \\mathbf{T}_{WB_i}^{k+1} will be given by:Gaussian(\\mathbf{T}_{WB_i}^{k+1}, \\Sigma^{k+1}) := \\mathbf{T}_{WB_i}^{k+1} \\text{Exp}( {_{B_i}}\\eta^{k+1} )\n\nwhere {_{B_i}}\\eta^{k+1} \\sim Gaussian(\\mathbf{0}_{6\\times1}, {_{B_i}}\\Sigma^{k+1})\n\n. As a consequence of the convention on the retraction, the resulting covariance is expressed in the body frame as well, and uses orientation-then-translation for the ordering of the covariance matrix.","type":"content","url":"/content/blogs/2021/2021-02-23-uncertainties-part2#defining-the-residual","position":25},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 2: Frames and manifolds","lvl2":"Conclusions"},"type":"lvl2","url":"/content/blogs/2021/2021-02-23-uncertainties-part2#conclusions","position":26},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 2: Frames and manifolds","lvl2":"Conclusions"},"content":"In this second part we extended the estimation framework presented previously by introducing the reference frames in an explicit manner into our notation, which helped to understand the meaning of the quantities.\n\nWe also reviewed the concept of groups of manifolds. We discussed that manifolds are the essential concept to generalize the optimization framework and probability distributions as defined in GTSAM.\n\nWe presented the idea of right-hand and left-hand conventions which, while not standard, allowed us to identify different formulations that can be found in the literature to operate groups and manifolds. By explicitly stating that GTSAM uses a right-hand convention for the composition of groups as well as retractions on manifolds we could identify the frames used to define the variables, as well as the covariance we obtain from the solution via  Marginals.\n\nAdditionally, the definition of the local and retract operations also have direct impact on the ordering of the covariance matrices, which also varies depending on the object. For instance, we discussed that Pose2 use a translation-then-orientation convention, while Pose3 does orientation-then-translation. This is particularly important when we want to use GTSAM quantities with different software, such as ROS, which use a translation-then-orientation convention for their 3D pose structures for instance (\n\nPoseWithCovariance).\n\nThe \n\nfinal post will cover some final remarks regarding the conventions we defined and applications of the adjoint of a Lie group, which is quite useful to properly transform covariance matrices defined for poses.","type":"content","url":"/content/blogs/2021/2021-02-23-uncertainties-part2#conclusions","position":27},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 3: Adjoints and covariances"},"type":"lvl1","url":"/content/blogs/2021/2021-02-23-uncertainties-part3","position":0},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 3: Adjoints and covariances"},"content":"Author: \n\nMatias Mattamala\n\n custom latex commands here $ \\usepackage{amsmath} \\usepackage[makeroom]{cancel} \\DeclareMathOperator*{\\argmin}{argmin} \\newcommand{\\coloneqq}{\\mathrel{:=}} $ horizontal scrolling  - TOC   \n\n{:toc}","type":"content","url":"/content/blogs/2021/2021-02-23-uncertainties-part3","position":1},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 3: Adjoints and covariances","lvl2":"Introduction"},"type":"lvl2","url":"/content/blogs/2021/2021-02-23-uncertainties-part3#introduction","position":2},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 3: Adjoints and covariances","lvl2":"Introduction"},"content":"In our \n\nprevious post we presented the concepts of reference frames, and how they are related to groups and manifolds to give physical meaning to the variables of our estimation framework. We remarked that the only property required for our GTSAM objects to be optimized and define probability distributions was to be manifolds. However, groups were useful to understand the composition of some objects, and to present the idea of Lie groups, which were both groups and differentiable manifolds.\n\nThis last section is mainly concerned about Lie groups, which is the case for most of the objects we use to represent robot pose. In particular, we will review the concept of adjoint of a Lie group, which will helps us to relate increments or correction applied on the right-hand side, with those applied on the left-side. Such property will allow us to manipulate uncertainties defined for Lie groups algebraically, and to obtain expressions for different covariance transformations. We will focus on 3D poses, i.e \\text{SE(3)}, because of its wide applicability but similar definitions should apply for other Lie groups since they mainly rely on a definition of the adjoint.\n\nMost of this expressions have been already shown in the literature by \n\nBarfoot and Furgale (2014) and \n\nMangelson et al. (2020) but since they follow a left-hand convention they are not straightforward to use with GTSAM. We provide the resulting expressions for the covariance transformations following Mangelson et al. but we recommend to refer to their work to understand the details of the process.","type":"content","url":"/content/blogs/2021/2021-02-23-uncertainties-part3#introduction","position":3},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 3: Adjoints and covariances","lvl2":"Adjoints"},"type":"lvl2","url":"/content/blogs/2021/2021-02-23-uncertainties-part3#adjoints","position":4},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 3: Adjoints and covariances","lvl2":"Adjoints"},"content":"Let us consider a case similar to previous examples in which we were adding a small increment _{B_i}\\mathbf{\\xi}\n\n to a pose \\mathbf{T}_{WB_i}\n\n.\\mathbf{T}_{W_i B_{i}} \\text{Exp}( _{B_i}\\mathbf{\\xi})\n\nwhere we have used the retraction of \\text{SE(3)} to add a small increment using the right-hand convention as GTSAM does.\n\n\n\n\n\nSimilarly to the cases discussed previously, applying a correction from the right can be interpreted as creating a new body frame at time $i+1$ from a previous frame $B_i$.\n\nHowever, for some applications we can be interested in applying a correction from the left:\\mathbf{T}_{W_{i+1} B} = \\text{Exp}( _{W_i}\\mathbf{\\xi}) \\mathbf{T}_{W_i B_i}\n\nPlease note that this case is different to the example in the previous post in which we had the frames inverted. Here we are effectively applying a correction on the world frame.\n\n\n\n\n\nIf we define a correction from the left, it is like we are creating a new reference frame at time $i+1$ from a frame $W_i$.\n\nWe are interested in finding out a correction on the body that can lead to the same result that a correction applied on the right. For that specific correction both formulations would be effectively representing the same pose but using different reference frames. This is shown in the next figure:\n\n\n\n\n\nThere exists a correction applied to the body frame (right convention) that can lead to equivalent transformations than corrections applied on the world frame (left convention)\n\nIn order to find it, we can write an equivalence between corrections applied on the body frame and the world frame as follows:\\text{Exp}( _{W}\\mathbf{\\xi}) \\mathbf{T}_{WB} = \\mathbf{T}_{WB} \\text{Exp}( _{B}\\mathbf{\\xi})\n\nWe dropped the time-related indices for simplicity, since this is a geometric relationship. In order to satisfy this condition, the incremental change we applied on the left-hand side, i.e in the world frame, _{W}\\mathbf{\\xi}\n\n must be given by:\\text{Exp}( _{W}\\mathbf{\\xi}) = \\mathbf{T}_{WB} \\text{Exp}( _{B}\\mathbf{\\xi}) \\mathbf{T}_{WB}^{-1}\n\nwhich we obtained by isolating the term. The expression on the right-hand side is known as the adjoint action of \\text{SE(3)}. This relates increments applied on the left to ones applied on the right.\n\n\n\n\n\nOur previous procedure allowed us to identify that such relationship is described by the Adjoint, shown at the right-hand side of the equation.\n\nFor our purposes, it is useful to use an equivalent alternative expression that applies directly to elements from the tangent space (\n\nSolà et al. gives a more complete derivation as a result of some properties we omitted here):\\text{Exp}( _{W}\\mathbf{\\xi}) \\mathbf{T}_{WB_i} = \\mathbf{T}_{WB_i} \\text{Exp}( \\text{Ad}_{T_{WB_i}^{-1}}  {_{W}}\\mathbf{\\xi})\n\nwhere \\text{Ad}_{T_{WB_i}^{-1}}\n\n is known as the adjoint matrix or adjoint of T_{WB_i}^{-1}\n\n. The adjoint acts over elements of the tangent space directly, changing their reference frame. Please note that the same subindex cancelation applies here, so we can confirm that the transformations are correctly defined.\n\nWe can also interpret this as a way to consistently move increments applied on the left-hand side (in world frame) to the right-hand side (body frame), which is particularly useful to keep the right-hand convention for the retractions and probability distributions consistent. This is the main property we will use in the next sections to define some covariance transformations, and it is already implemented in Pose3 as \n\nAdjointMap.","type":"content","url":"/content/blogs/2021/2021-02-23-uncertainties-part3#adjoints","position":5},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 3: Adjoints and covariances","lvl2":"Distribution of the inverse"},"type":"lvl2","url":"/content/blogs/2021/2021-02-23-uncertainties-part3#distribution-of-the-inverse","position":6},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 3: Adjoints and covariances","lvl2":"Distribution of the inverse"},"content":"As a first example on how the adjoint helps to manipulate covariances, let us consider the case in which we have the solution of a factor graph, with covariances defined in the body frame as we have discussed previously. We are interested in obtaining an expression to express the covariance in the world frame:\n\n\n\n\n\nGiven the covariance of the robot in the body frame $B_i$ (left), and we are interested in obtaining a formula to express the covariance in the world frame $W$ following GTSAM's right-hand convention (right).\n\nThe distribution of the pose assuming Gaussian distribution will be expressed by:\\mathbf{\\tilde{T}}_{WB} = \\mathbf{T}_{WB} \\text{Exp}( _{B}\\mathbf{\\eta})\n\nwith _{B}\\mathbf{\\eta}\n\n zero-mean Gaussian noise with covariance \\Sigma_{B} as before. The distribution of the inverse can be computed by inverting the expression:\\begin{align}\n(\\mathbf{\\tilde{T}}_{WB})^{-1} & = (\\mathbf{T}_{WB} \\text{Exp}( _{B}\\mathbf{\\eta}) )^{-1}\\\\\n& = (\\text{Exp}( _{B}\\mathbf{\\eta}) )^{-1}\\ \\mathbf{T}_{WB}^{-1}\\\\\n& = \\text{Exp}(- _{B}\\mathbf{\\eta}) \\ \\mathbf{T}_{WB}^{-1}\n\\end{align}\n\nHowever, the noise is defined on the left, which is inconvenient because is still a covariance in the body frame, but it is also inconsistent with right-hand convention. We can move it to the right using the adjoint:(\\mathbf{\\tilde{T}}_{WB})^{-1} = \\ \\mathbf{T}_{WB}^{-1}\\ \\text{Exp}(- \\text{Ad}_{\\mathbf{T}_{WB}} {_{B}}\\mathbf{\\eta})\n\nThis is a proper distribution following the right-hand convention, which defines the covariance in the world frame. The covariance of the inverse is then given by:\\Sigma_{W} = \\text{Ad}_{\\mathbf{T}_{WB}} \\Sigma_B \\text{Ad}_{\\mathbf{T}_{WB}}^{T}","type":"content","url":"/content/blogs/2021/2021-02-23-uncertainties-part3#distribution-of-the-inverse","position":7},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 3: Adjoints and covariances","lvl2":"Distribution of the composition"},"type":"lvl2","url":"/content/blogs/2021/2021-02-23-uncertainties-part3#distribution-of-the-composition","position":8},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 3: Adjoints and covariances","lvl2":"Distribution of the composition"},"content":"A different case is if we have the distribution of a pose \\mathbf{\\tilde{T}}_{WB_i}\n\n and we also have an increment given by another distribution \\mathbf{\\tilde{T}}_{B_i B_{i+1}}\n\n. By doing some algebraic manipulation, we can determine the distribution of the composition \\mathbf{\\tilde{T}}_{WB_{i+1}}\n\n. This is useful when doing dead reckoning for instance:\n\n\n\n\n\nIn our second example, we want to algebraically determine the covariance of $\\mathbf{\\tilde{T}}_{WB_{i+1}}$ (right), given the distributions of the initial pose $\\mathbf{\\tilde{T}}_{WB_i}$ and the relative increment $\\mathbf{\\tilde{T}}_{B_i B_{i+1}}$ (left).\n\nWe can determine the distribution of the composition (and its covariance) if we follow a similar formulation as before:\\begin{align}\n\\mathbf{\\tilde{T}}_{WB_{i+1}} &= \\mathbf{\\tilde{T}}_{WB_i} \\mathbf{\\tilde{T}}_{B_i B_{i+1}}\\\\\n&= \\mathbf{T}_{WB_i} \\text{Exp}( _{B_i}\\mathbf{\\eta})\\ \\mathbf{T}_{B_i B_{i+1}} \\text{Exp}( _{B_{i+1}}\\mathbf{\\eta})\n\\end{align}\n\nAnalogously, we need to move the noise _{B_i}\\mathbf{\\eta}\n\n to the right, so as to have the transformations to the left (which represent the mean of the distribution), and the noises to the right (that encode the covariance). We can use the adjoint again:\\mathbf{\\tilde{T}}_{WB_i} = \\mathbf{T}_{WB_i} \\ \\mathbf{T}_{B_i B_{i+1}} \\text{Exp}(\\text{Ad}_{\\mathbf{T}_{B_i B_{i+1}}^{-1}} {_{B_i}}\\mathbf{\\eta})\\  \\text{Exp}( _{B_{i+1}}\\mathbf{\\eta})\n\nHowever, we cannot combine the exponentials because that would assume commutativity in the group that does not hold for \\text{SE(3)} as we discussed previously. Still, it is possible to use some approximations (also discussed in Mangelson’s) to end up with the following expressions for the covariance of the composition:\\Sigma_{B_{i+1}} = \\text{Ad}_{\\mathbf{T}_{B_i B_{i+1}}^{-1}} \\Sigma_{B_{i}} \\text{Ad}_{\\mathbf{T}_{B_i B_{i+1}}^{-1}}^{T} + \\Sigma_{B_{i+1}}\n\nAdditionally, if we consider that the poses are correlated and then the covariance of the joint distribution is given by:\\begin{bmatrix} \\Sigma_{B_{i}} & \\Sigma_{B_{i}, B_{i+1}} \\\\ \\Sigma_{B_{i}, B_{i+1}}^{T} & \\Sigma_{B_{i+1}} \\end{bmatrix}\n\nWe have that the covariance of the composition is:\\begin{split}\n\\Sigma_{B_{i+1}} &=  \\text{Ad}_{\\mathbf{T}_{B_i B_{i+1}}^{-1}} \\Sigma_{B_{i}} \\text{Ad}_{\\mathbf{T}_{B_i B_{i+1}}^{-1}}^{T}  + \\Sigma_{B_{i+1}}  \\\\\n&\\qquad + \\text{Ad}_{\\mathbf{T}_{B_i B_{i+1}}^{-1}} \\Sigma_{B_{i}, B_{i+1}} + \\Sigma_{B_{i}, B_{i+1}}^{T} \\text{Ad}_{\\mathbf{T}_{B_i B_{i+1}}^{-1}}^{T}\n\\end{split}","type":"content","url":"/content/blogs/2021/2021-02-23-uncertainties-part3#distribution-of-the-composition","position":9},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 3: Adjoints and covariances","lvl2":"Distribution of the relative transformation"},"type":"lvl2","url":"/content/blogs/2021/2021-02-23-uncertainties-part3#distribution-of-the-relative-transformation","position":10},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 3: Adjoints and covariances","lvl2":"Distribution of the relative transformation"},"content":"Lastly, it may be the case that we have the distributions of two poses expressed in the same reference frame, and we want to compute the distribution of the relative transformation between them. For example, if we have an odometry system that provides estimates with covariance and we want to use relative measurements as factors for a pose graph SLAM system, we will need the mean and covariance of the relative transformation.\n\n\n\n\n\nIn this last example we want to determine the covariance of the relative transformation $\\mathbf{\\tilde{T}}_{B_{i} B_{i+1}}$ (right) given the distributions of the poses $\\mathbf{\\tilde{T}}_{W B_{i}}$ and $\\mathbf{\\tilde{T}}_{B_i B_{i+1}}$ (left).\n\nTo determine it we follow the same algebraic procedure:\\begin{align}\n\\mathbf{\\tilde{T}}_{B_i B_{i+1}} &= \\mathbf{\\tilde{T}}_{W B_{i}}^{-1} \\mathbf{\\tilde{T}}_{WB_{i+1}}\\\\\n &= \\left( \\mathbf{T}_{WB_i} \\text{Exp}( _{B_i}\\mathbf{\\eta}) \\right)^{-1} \\ \\mathbf{T}_{WB_{i+1}} \\text{Exp}( _{B_{i+1}}\\mathbf{\\eta})\\\\\n &= \\text{Exp}(- _{B_i}\\mathbf{\\eta}) \\ \\mathbf{T}_{WB_i}^{-1} \\ \\mathbf{T}_{WB_{i+1}} \\text{Exp}( _{B_{i+1}}\\mathbf{\\eta})\n\\end{align}\n\nSimilarly, we use the adjoint to move the exponential twice:\\begin{align}\n\\mathbf{\\tilde{T}}_{B_i B_{i+1}} &=  \\mathbf{T}_{WB_i}^{-1}  \\text{Exp}(- \\text{Ad}_{\\mathbf{T}_{WB_i}} \\ {_{B_i}}\\mathbf{\\eta}) \\ \\mathbf{T}_{WB_{i+1}} \\text{Exp}( _{B_{i+1}}\\mathbf{\\eta})\\\\\n &=  \\mathbf{T}_{WB_i}^{-1} \\ \\mathbf{T}_{WB_{i+1}} \\ \\text{Exp}(- \\text{Ad}_{\\mathbf{T}_{WB_{i+1}}^{-1}} \\ \\text{Ad}_{\\mathbf{T}_{WB_i}}\\ {_{B_i}}\\mathbf{\\eta}) \\ \\text{Exp}( _{B_{i+1}}\\mathbf{\\eta})\n\\end{align}\n\nHence, the following covariance holds for the relative pose assuming independent poses:\\Sigma_{B_{i+1}} = \\left(\\text{Ad}_{\\mathbf{T}_{WB_{i+1}}^{-1}} \\text{Ad}_{\\mathbf{T}_{WB_i}} \\right) \\Sigma_{B_{i}} \\left(\\text{Ad}_{\\mathbf{T}_{WB_{i+1}}^{-1}} \\text{Ad}_{\\mathbf{T}_{WB_i}}\\right)^{T} + \\Sigma_{B_{i+1}}\n\nA problem that exist with this expression, however, is that by assuming independence the covariance of the relative poses will be larger than the covariance of each pose separately. This is consistent with the \n\n1-dimensional case in which we compute the distribution of the difference of independent Gaussians, in which the mean is the difference while the covariance gets increased. However, this is not the result that we would want, since our odometry factors will degrade over time.\n\nMangelson et al. showed that if some correlations exists (as we showed for the composition example) and it is explicitly considered in the computation, the estimates get more accurate and the covariance is not over or underestimated. The corresponding expression that complies with GTSAM is then:\\begin{split}\n\\Sigma_{B_{i+1}} & = \\left(\\text{Ad}_{\\mathbf{T}_{WB_{i+1}}^{-1}} \\text{Ad}_{\\mathbf{T}_{WB_i}} \\right) \\Sigma_{B_{i}} \\left(\\text{Ad}_{\\mathbf{T}_{WB_{i+1}}^{-1}} \\text{Ad}_{\\mathbf{T}_{WB_i}}\\right)^{T}  + \\Sigma_{B_{i+1}} \\\\\n&\\qquad  - \\left(\\text{Ad}_{\\mathbf{T}_{WB_{i+1}}^{-1}} \\text{Ad}_{\\mathbf{T}_{WB_i}} \\right) \\Sigma_{B_{i} B_{i+1}} - \\Sigma_{B_{i} B_{i+1}}^{T} \\left(\\text{Ad}_{\\mathbf{T}_{WB_{i+1}}^{-1}} \\text{Ad}_{\\mathbf{T}_{WB_i}} \\right)^{T}\n\\end{split}","type":"content","url":"/content/blogs/2021/2021-02-23-uncertainties-part3#distribution-of-the-relative-transformation","position":11},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 3: Adjoints and covariances","lvl2":"Conclusions"},"type":"lvl2","url":"/content/blogs/2021/2021-02-23-uncertainties-part3#conclusions","position":12},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 3: Adjoints and covariances","lvl2":"Conclusions"},"content":"In this final post we reviewed the concept of the adjoint of a Lie group to explain the relationship between increments applied on the left with increments applied on the right. This was the final piece we needed to ensure that our estimates are consistent with the conventions used in GTSAM.\n\nWe also presented a few expressions to operate distributions of poses. While they were presented previously in the literature, we showed general guidelines on how to manipulate the uncertainties to be consistent with the frames and convention we use.","type":"content","url":"/content/blogs/2021/2021-02-23-uncertainties-part3#conclusions","position":13},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 3: Adjoints and covariances","lvl2":"Acknowledgments"},"type":"lvl2","url":"/content/blogs/2021/2021-02-23-uncertainties-part3#acknowledgments","position":14},{"hierarchy":{"lvl1":"Reducing the uncertainty about the uncertainties, part 3: Adjoints and covariances","lvl2":"Acknowledgments"},"content":"I would like to thank again the interesting discussions originated in the \n\ngtsam-users group. Stefan Gächter guided a rich conversation doing some important questions that motivated the idea of writing this post.\n\nCoincidently, similar discussions we had at the \n\nDynamic Robot Systems group at the University of Oxford were aligned with the topics discussed here and facilitated the writing process. Special thanks to Yiduo Wang and Milad Ramezani for our conversations, derivation and testing of the formulas for the covariance of relative poses presented here, Marco Camurri for feedback on the notation and proofreading, and Maurice Fallon for encouraging to write this post.\n\nFinally, thanks to Frank Dellaert, José Luis Blanco-Claraco, and John Lambert for proofreading the posts and their feedback, as well as Michael Bosse for providing insightful comments to ensure the consistency of the topics presented here.","type":"content","url":"/content/blogs/2021/2021-02-23-uncertainties-part3#acknowledgments","position":15},{"hierarchy":{"lvl1":"LOST in Triangulation"},"type":"lvl1","url":"/content/blogs/2023/2023-02-04-lost-triangulation","position":0},{"hierarchy":{"lvl1":"LOST in Triangulation"},"content":"This post introduces the triangulation problem and some commonly used solutions (both optimal and sub-optimal), along with GTSAM code examples. We also discuss a recently proposed linear optimal sine triangulation method (LOST) and compare its performance to other methods.\n\nTL;DR:\n\nLOST can be easily enabled in GTSAM by setting useLOST=True in triangulatePoint3 to get state-of-the-art linear triangulation!","type":"content","url":"/content/blogs/2023/2023-02-04-lost-triangulation","position":1},{"hierarchy":{"lvl1":"LOST in Triangulation","lvl2":"The triangulation problem"},"type":"lvl2","url":"/content/blogs/2023/2023-02-04-lost-triangulation#the-triangulation-problem","position":2},{"hierarchy":{"lvl1":"LOST in Triangulation","lvl2":"The triangulation problem"},"content":"Triangulation is the problem of estimating the location of a 3D point from multiple direction measurements. The problem is commonly encountered in the domains of 3D reconstruction, robot localization, and spacecraft navigation, amongst others. The “direction measurements” are usually obtained from 2D observations of the same 3D point in different cameras.","type":"content","url":"/content/blogs/2023/2023-02-04-lost-triangulation#the-triangulation-problem","position":3},{"hierarchy":{"lvl1":"LOST in Triangulation","lvl3":"A simple solution: the Direct Linear Transform","lvl2":"The triangulation problem"},"type":"lvl3","url":"/content/blogs/2023/2023-02-04-lost-triangulation#a-simple-solution-the-direct-linear-transform","position":4},{"hierarchy":{"lvl1":"LOST in Triangulation","lvl3":"A simple solution: the Direct Linear Transform","lvl2":"The triangulation problem"},"content":"A commonly used method for triangulation is the Direct Linear Transform (DLT), which poses the estimation of the 3D point as a least squares problem. Given the position of camera i in the world frame as \\mathbf{^wp_i} and the 2D measurement (in homogeneous coordinates) of the point in this camera as \\mathbf{x_i}, it uses the following constraint: \\mathbf{x_i} \\times \\mathbf{R^i_w} (\\mathbf{^wr} - \\mathbf{^wp_i})  &= \\mathbf{0} \\\\\n\n\\text{i.e } [\\mathbf{x_i} \\times] \\mathbf{R^i_w} (\\mathbf{^wr} - \\mathbf{^wp_i})  &= \\mathbf{0}\n\nHere:\n\n\\mathbf{^wr} is the position of the 3D point in the world frame, which is to be estimated\n\n[\\mathbf{x_i} \\times] is a skew-symmetric matrix with elements \\mathbf{x_i}, such that [\\mathbf{x_i} \\times] \\ \\mathbf{b} = \\mathbf{x_i} \\times \\mathbf{b}\n\n\\mathbf{R^i_w} \\in SO(3) is the rotation from world to camera frame i\n\n(1) can be expressed in terms of the pixel measurements \\mathbf{u_i} = \\mathbf{K_i} \\mathbf{x_i} using the intrinsic calibration matrix of the camera \\mathbf{K_i}:    [\\mathbf{K_i^{-1}} \\mathbf{u_i} \\times] \\mathbf{R^i_w} (\\mathbf{^wr} - \\mathbf{^wp_i})  = \\mathbf{0}\n\nWith at least 2 camera measurements, this can be expressed as a least-squares problem of the form \\mathbf{A x} = \\mathbf{b} with a unique solution (except in some degenerate conditions). For n measurements, this equation is:\\left[\\begin{array}{c}\n\\left[\\mathbf{K_1^{-1}} \\mathbf{u_1} \\times\\right] \\mathbf{R^1_w} \\cr\t\\\\\n\\left[\\mathbf{K_2^{-1}} \\mathbf{u_2} \\times\\right] \\mathbf{R^2_w} \\cr\t\\\\\n... \\cr \\\\\n\\left[\\mathbf{K_n^{-1}} \\mathbf{u_n} \\times\\right] \\mathbf{R^n_w} \\cr\n\\end{array}\\right] \\mathbf{^wr} = \n\\left[\\begin{array}{c}\n\\left[\\mathbf{K_1^{-1}} \\mathbf{u_1} \\times\\right] \\mathbf{R^1_w} \\mathbf{^wp_1} \\cr\t\\\\\n\\left[\\mathbf{K_2^{-1}} \\mathbf{u_2} \\times\\right] \\mathbf{R^2_w} \\mathbf{^wp_2} \\cr\t\\\\\n... \\cr \\\\\n\\left[\\mathbf{K_n^{-1}} \\mathbf{u_n} \\times\\right] \\mathbf{R^n_w} \\mathbf{^wp_n} \\cr\n\\end{array}\\right]\n\nDLT solves the above equation using standard least-squares techniques. It also has interesting connections to the trigonometric sine rule, as discussed in the \n\nLOST paper (\n\nArxiv).\n\nThe triangulatePoint3 function in GTSAM provides a convenient way to triangulate points using DLT. Let’s look at an example of triangulating a point from noisy measurements.import gtsam\nimport numpy as np\nfrom gtsam import Pose3, Rot3, Point3\n\n# Define parameters for 2 cameras, with shared intrinsics.\npose1 = Pose3()\npose2 = Pose3(Rot3(), Point3(5., 0., -5.))\nintrinsics = gtsam.Cal3_S2()\ncamera1 = gtsam.PinholeCameraCal3_S2(pose1, intrinsics)\ncamera2 = gtsam.PinholeCameraCal3_S2(pose2, intrinsics)\ncameras = gtsam.CameraSetCal3_S2([camera1, camera2])\n\n# Define a 3D point, generate measurements by projecting it to the \n# cameras and adding some noise.\nlandmark = Point3(0.1, 0.1, 1.5)\nm1_noisy = cameras[0].project(landmark) + gtsam.Point2(0.00817, 0.00977)\nm2_noisy = cameras[1].project(landmark) + gtsam.Point2(-0.00610, 0.01969)\nmeasurements = gtsam.Point2Vector([m1_noisy, m2_noisy])\n\n# Triangulate!\ndlt_estimate = gtsam.triangulatePoint3(cameras, measurements, rank_tol=1e-9, optimize=False)\nprint(\"DLT estimation error: {:.04f}\".format(np.linalg.norm(dlt_estimate - landmark)))DLT estimation error: 0.0832\n\nHere is an interactive visualization of the DLT estimate, the two camera frustums, and the back-projected rays. Note the difference between the ground truth and the triangulated point. We shall revisit this example below, and compare it against the result from other methods.\n\nimport plotly.graph_objects as go\n\n# Define traces\ntraces = [\n    go.Scatter3d(\n        x=[0.0,-0.8,None,0.0,-0.8,None,0.0,0.8,None,0.0,0.8,None,-0.8,-0.8,None,-0.8,0.8,None,0.8,-0.8,None,0.8,0.8,None],\n        y=[0.0,-0.6,None,0.0,0.6,None,0.0,-0.6,None,0.0,0.6,None,-0.6,0.6,None,0.6,0.6,None,-0.6,-0.6,None,0.6,-0.6,None],\n        z=[0.0,1.0,None,0.0,1.0,None,0.0,1.0,None,0.0,1.0,None,1.0,1.0,None,1.0,1.0,None,1.0,1.0,None,1.0,1.0,None],\n        mode='lines',\n        name='camera 1'\n    ),\n    go.Scatter3d(\n        x=[5.0,4.2,None,5.0,4.2,None,5.0,5.8,None,5.0,5.8,None,4.2,4.2,None,4.2,5.8,None,5.8,4.2,None,5.8,5.8,None],\n        y=[0.0,-0.6,None,0.0,0.6,None,0.0,-0.6,None,0.0,0.6,None,-0.6,0.6,None,0.6,0.6,None,-0.6,-0.6,None,0.6,-0.6,None],\n        z=[-5.0,-4.0,None,-5.0,-4.0,None,-5.0,-4.0,None,-5.0,-4.0,None,-4.0,-4.0,None,-4.0,-4.0,None,-4.0,-4.0,None,-4.0,-4.0,None],\n        mode='lines',\n        name='camera 2'\n    ),\n    go.Scatter3d(\n        x=[0.1], y=[0.1], z=[1.5],\n        mode='markers',\n        marker=dict(size=2),\n        name='GT point'\n    ),\n    go.Scatter3d(\n        x=[0.102], y=[0.168], z=[1.453],\n        mode='markers',\n        marker=dict(size=2),\n        name='DLT estimate'\n    ),\n    go.Scatter3d(\n        x=[0.0, 0.149], y=[0.0, 0.152], z=[0.0, 2.0],\n        mode='lines+markers',\n        marker=dict(size=[0,6,0], symbol='diamond'),\n        name='ray 1'\n    ),\n    go.Scatter3d(\n        x=[5.0, -0.319], y=[0.0, 0.245], z=[-5.0, 2.0],\n        mode='lines+markers',\n        marker=dict(size=[0,6,0], symbol='diamond'),\n        name='ray 2'\n    )\n]\n\n# Define layout\nlayout = go.Layout(\n    scene=dict(aspectmode='data'),\n    margin=dict(l=0, r=0, t=0, b=0),\n    legend=dict(y=0.96)\n)\n\n# Create figure\nfig = go.Figure(data=traces, layout=layout)\nfig.show()\n\n","type":"content","url":"/content/blogs/2023/2023-02-04-lost-triangulation#a-simple-solution-the-direct-linear-transform","position":5},{"hierarchy":{"lvl1":"LOST in Triangulation","lvl3":"The problem with DLT","lvl2":"The triangulation problem"},"type":"lvl3","url":"/content/blogs/2023/2023-02-04-lost-triangulation#the-problem-with-dlt","position":6},{"hierarchy":{"lvl1":"LOST in Triangulation","lvl3":"The problem with DLT","lvl2":"The triangulation problem"},"content":"DLT provides an exact solution to the triangulation problem in the absence of measurement noise. In practice however, the 2D measurements we use are almost always noisy, so their back-projected rays may no longer intersect in 3D. The DLT, being an unweighted least squares solution, places equal weight on the residual from each measurement (each row of the linear system in \n\n(3)). The trouble is that this does not minimize the proper cost function. The covariance of the estimate with respect to a measurement depends on factors like the range of the estimate from the camera (i.e, measurements from closer cameras should be trusted more). Ideally, we should minimize the covariance-weighted reprojection errors on the image plane instead of the residuals of the arbitrarily scaled rows of \n\n(3).","type":"content","url":"/content/blogs/2023/2023-02-04-lost-triangulation#the-problem-with-dlt","position":7},{"hierarchy":{"lvl1":"LOST in Triangulation","lvl2":"Optimal triangulation"},"type":"lvl2","url":"/content/blogs/2023/2023-02-04-lost-triangulation#optimal-triangulation","position":8},{"hierarchy":{"lvl1":"LOST in Triangulation","lvl2":"Optimal triangulation"},"content":"Optimal triangulation computes the maximum likelihood estimate of the weighted residual norm. Consider the common case of noisy measurements \\mathbf{\\tilde{u}_i} = \\mathbf{u_i} + \\mathbf{n_i}, where \\mathbf{n_i} is Gaussian noise in 2D pixel space with covariance \\mathbf{\\Sigma_{u_i}}. The noise-free measurement \\mathbf{u_i} is a function of the unknown point \\mathbf{^wr}:\\mathbf{u_i} = \\mathbf{\\mathit{h_i}(^wr)} = \\mathbf{K_i} \\mathbf{R^i_w} (\\mathbf{^wr} - \\mathbf{^wp_i}) where $\\mathbf{S}$ is a matrix of the form $[\\mathbf{I}_{2 \\times 2} \\ \\mathbf{0}_{2 \\times 1}]$.  \n\nThe optimal estimate in an MLE framework is the solution that minimizes the weighted residual norm:J(\\mathbf{^wr}) = \\sum_{i=1}^{n} \\mathbf{n_i^T} \\mathbf{\\Sigma_{u_i}^{-1}} \\mathbf{n_i} =  \\sum_{i=1}^{n} \\mathbf{(\\tilde{u}_i - \\mathbf{\\mathit{h_i}(^wr)})^T} \\mathbf{\\Sigma_{u_i}^{-1}} \\mathbf{(\\tilde{u}_i - \\mathbf{\\mathit{h_i}(^wr)})}\n\nFor the case of triangulating a point from 2 camera measurements, \n\n(5) can be solved analytically as shown by \n\nHartley and Strum and the \n\nLOST paper (\n\nArxiv). This usually involves solving a polynomial of degree 6.\n\nWe often encounter triangulation problems with more than two measurements where the degree 6 polynomial solution of Hartley & Sturm no longer applies. In such cases, it is common to minimize \n\n(5) using an iterative nonlinear solver starting with the DLT estimate as the initialization. In GTSAM, this can be achieved using a factor graph by setting optimize=True in the call to triangulatePoint3. This scales to the general case with more than 2 camera measurements, but like all iterative methods, significantly increases the estimation latency.# Optimization needs the measurement noise model.\nnoisemodel = gtsam.noiseModel.Isotropic.Sigma(2, 1e-3)\n\noptimal_estimate = gtsam.triangulatePoint3(cameras, measurements, rank_tol=1e-9, optimize=True, model=noisemodel)\nprint(\"Optimal estimation error: {:.04f}\".format(np.linalg.norm(optimal_estimate - landmark)))Optimal estimation error: 0.0549","type":"content","url":"/content/blogs/2023/2023-02-04-lost-triangulation#optimal-triangulation","position":9},{"hierarchy":{"lvl1":"LOST in Triangulation","lvl2":"The Linear Optimal Sine Triangulation (LOST) approach"},"type":"lvl2","url":"/content/blogs/2023/2023-02-04-lost-triangulation#the-linear-optimal-sine-triangulation-lost-approach","position":10},{"hierarchy":{"lvl1":"LOST in Triangulation","lvl2":"The Linear Optimal Sine Triangulation (LOST) approach"},"content":"In their paper on \n\n“Absolute Triangulation Algorithms for Space Exploration” (\n\nArxiv), Henry and Christian propose the linear optimal sine triangulation (LOST) method that non-iteratively solves the statistically optimal triangulation problem as a linear system. When only two measurements are available, LOST provides the same answer as Hartley & Sturm’s polynomial solution. However, unlike the polynomial solution, LOST scales linearly to an arbitrary number of measurements and remains non-iterative. LOST is a weighted least squares approach that both\n\nprovides the same solution as and\n\nis significantly faster than the iterative nonlinear optimization approach commonly used for optimal triangulation with many (more than two) cameras.\n\nTheir approach minimizes the weighted residual norm in the image-plane coordinates (it can easily be rewritten in terms of pixel coordinates). For a noisy measurement on the image plane \\mathbf{\\tilde{x}_i} = \\mathbf{x_i} + \\mathbf{w_i}, \n\n(1) will have a residual \\mathbf{\\epsilon_i} given by:{\\mathbf{\\tilde{x}_i}} \\times \\mathbf{R^i_w} (\\mathbf{^wr} - \\mathbf{^wp_i})  = \\mathbf{\\epsilon_i}\n\nSince [\\mathbf{R^i_w} (\\mathbf{^wr} - \\mathbf{^wp_i}) \\times] is a skew symmetric matrix, and \\mathbf{\\tilde{x}_i} = \\mathbf{K_i}^{-1} \\mathbf{\\tilde{u}_i}\\mathbf{\\epsilon_i} =  [\\mathbf{R^i_w} (\\mathbf{^wp_i} - \\mathbf{^wr}) \\times] \\  \\mathbf{K_i^{-1}} \\mathbf{\\tilde{u}_i}\n\nThe weighted residual norm to be minimized is:\nJ(\\mathbf{^wr}) = \\sum_{i=1}^{n} \\mathbf{\\epsilon_i^T} \\mathbf{\\Sigma_{\\epsilon_i}^{-1}} \\mathbf{\\epsilon_i}\n\nWe skip through the bulk of the math in what follows, so please refer to the \n\nLOST paper (\n\nArxiv) by Henry and Christian for a neat derivation. The error covariance \\mathbf{\\Sigma_{\\epsilon_i}} can be expressed in terms of the 2D measurement covariance \\mathbf{\\Sigma_{x_i}} as:\\mathbf{\\Sigma_{\\epsilon_i}} = -\\frac{\\rho_i^{2}}{||\\mathbf{K_i^{-1}} \\mathbf{\\tilde{u}_i}||^2} [\\mathbf{K_i^{-1}} \\mathbf{\\tilde{u}_i} \\times ] \\mathbf{\\Sigma_{x_i}} [\\mathbf{K_i^{-1}} \\mathbf{\\tilde{u}_i} \\times ]\n\nwhere \\rho_i is the range of the range of the 3D point from the center of camera i. The difficulty with this expression for \\mathbf{\\Sigma_{\\epsilon_i}} is that it is not full rank (not invertible) and the ranges \\rho_i are not known a priori (since the 3D point’s location is not yet known). The LOST paper shows how both of these difficulties can be avoided and provides a general non-iterative solution. An especially nice result occurs when the image plane measurement errors are isotropic. In this case, minimizing the cost function J(\\mathbf{^wr}) leads to a simple least squares expression:\\left[\\begin{array}{c}\nq_1 \\left[\\mathbf{K_1^{-1}} \\mathbf{u_1} \\times\\right] \\mathbf{R^1_w} \\cr\t\\\\\nq_2 \\left[\\mathbf{K_2^{-1}} \\mathbf{u_2} \\times\\right] \\mathbf{R^2_w} \\cr\t\\\\\n... \\cr \\\\\nq_n \\left[\\mathbf{K_n^{-1}} \\mathbf{u_n} \\times\\right] \\mathbf{R^n_w} \\cr\n\\end{array}\\right] \\mathbf{^wr} = \n\\left[\\begin{array}{c}\nq_1 \\left[\\mathbf{K_1^{-1}} \\mathbf{u_1} \\times\\right] \\mathbf{R^1_w} \\mathbf{^wp_1} \\cr\t\\\\\nq_2 \\left[\\mathbf{K_2^{-1}} \\mathbf{u_2} \\times\\right] \\mathbf{R^2_w} \\mathbf{^wp_2} \\cr\t\\\\\n... \\cr \\\\\nq_n \\left[\\mathbf{K_n^{-1}} \\mathbf{u_n} \\times\\right] \\mathbf{R^n_w} \\mathbf{^wp_n} \\cr\n\\end{array}\\right]\n\nNote that this looks very much like \n\n(3), except for the inclusion of the coefficients q_i (weighted least squares, of course!). The final expression for q_i is:q_i = \\frac{||\\mathbf{K_i^{-1}} \\mathbf{u_i}||}{\\sigma_{x_i} \\rho_i} = \\frac{|| \\mathbf{R^w_i} \\mathbf{K_i^{-1}} \\mathbf{\\tilde{u}_i} \\times \\ \\mathbf{R^w_j} \\mathbf{K_j^{-1}} \\mathbf{\\tilde{u}_j} ||}{\\sigma_{x_i} || \\mathbf{d_{ij}} \\times \\ \\mathbf{R^w_j} \\mathbf{K_j^{-1}} \\mathbf{\\tilde{u}_j} ||}\n\nwhere \\mathbf{d_{ij}} = (\\mathbf{^wp_j} - \\mathbf{^wp_i}) is the known baseline between cameras i and j. Note that everything on the right-hand side of this expression for q_i is known a priori, and so the optimal LOST weights q_i may be found directly and without any iteration.\n\nStarting from release \n\n4.2a8, gtsam includes an implementation of LOST which can be easily used as follows:lost_estimate = gtsam.triangulatePoint3(cameras, measurements, 1e-9, optimize=False, model=noisemodel, useLOST=True)\nprint(\"LOST estimation error: {:.04f}\".format(np.linalg.norm(lost_estimate - landmark)))LOST estimation error: 0.0581\n\nThe estimation error obtained above is comparable to that obtained by iterative optimization. Although the LOST error in this particular instance is slightly larger than the iterative solution, sometimes the reverse is true (i.e., sometimes LOST is slightly better). Indeed, as we will see in the next section, the statistics of LOST and the iterative LS solution are identical.\n\nThe visualization below adds the LOST estimate to the DLT estimate we obtained above. Note how the LOST estimate is much closer to the ground truth (especially along X and Y axes) compared to the DLT estimate.\n\nimport plotly.graph_objects as go\n\n# Define traces\ntraces = [\n    go.Scatter3d(\n        x=[0.0,-0.8,None,0.0,-0.8,None,0.0,0.8,None,0.0,0.8,None,-0.8,-0.8,None,-0.8,0.8,None,0.8,-0.8,None,0.8,0.8,None],\n        y=[0.0,-0.6,None,0.0,0.6,None,0.0,-0.6,None,0.0,0.6,None,-0.6,0.6,None,0.6,0.6,None,-0.6,-0.6,None,0.6,-0.6,None],\n        z=[0.0,1.0,None,0.0,1.0,None,0.0,1.0,None,0.0,1.0,None,1.0,1.0,None,1.0,1.0,None,1.0,1.0,None,1.0,1.0,None],\n        mode='lines',\n        name='camera 1'\n    ),\n    go.Scatter3d(\n        x=[5.0,4.2,None,5.0,4.2,None,5.0,5.8,None,5.0,5.8,None,4.2,4.2,None,4.2,5.8,None,5.8,4.2,None,5.8,5.8,None],\n        y=[0.0,-0.6,None,0.0,0.6,None,0.0,-0.6,None,0.0,0.6,None,-0.6,0.6,None,0.6,0.6,None,-0.6,-0.6,None,0.6,-0.6,None],\n        z=[-5.0,-4.0,None,-5.0,-4.0,None,-5.0,-4.0,None,-5.0,-4.0,None,-4.0,-4.0,None,-4.0,-4.0,None,-4.0,-4.0,None,-4.0,-4.0,None],\n        mode='lines',\n        name='camera 2'\n    ),\n    go.Scatter3d(\n        x=[0.1], y=[0.1], z=[1.5],\n        mode='markers',\n        marker=dict(size=2),\n        name='GT point'\n    ),\n    go.Scatter3d(\n        x=[0.102], y=[0.168], z=[1.453],\n        mode='markers',\n        marker=dict(size=2),\n        name='DLT estimate'\n    ),\n    go.Scatter3d(\n        x=[0.107], y=[0.116], z=[1.444],\n        mode='markers',\n        marker=dict(size=2),\n        name='LOST estimate'\n    ),\n    go.Scatter3d(\n        x=[0.0, 0.149], y=[0.0, 0.152], z=[0.0, 2.0],\n        mode='lines+markers',\n        marker=dict(size=[0,6,0], symbol='diamond'),\n        name='ray 1'\n    ),\n    go.Scatter3d(\n        x=[5.0, -0.319], y=[0.0, 0.245], z=[-5.0, 2.0],\n        mode='lines+markers',\n        marker=dict(size=[0,6,0], symbol='diamond'),\n        name='ray 2'\n    )\n]\n\n# Define layout\nlayout = go.Layout(\n    scene=dict(aspectmode='data'),\n    margin=dict(l=0, r=0, t=0, b=0),\n    legend=dict(y=0.96)\n)\n\n# Create figure\nfig = go.Figure(data=traces, layout=layout)\nfig.show()\n\n","type":"content","url":"/content/blogs/2023/2023-02-04-lost-triangulation#the-linear-optimal-sine-triangulation-lost-approach","position":11},{"hierarchy":{"lvl1":"LOST in Triangulation","lvl2":"How does LOST compare to DLT and iterative optimization?"},"type":"lvl2","url":"/content/blogs/2023/2023-02-04-lost-triangulation#how-does-lost-compare-to-dlt-and-iterative-optimization","position":12},{"hierarchy":{"lvl1":"LOST in Triangulation","lvl2":"How does LOST compare to DLT and iterative optimization?"},"content":"When triangulating a point several times starting from different noisy measurements, it was found that the standard deviation of the LOST error is comparable to that of iterative optimization, and much lesser than DLT. This improvement reduces with increasing number of camera measurements. The results from 100 trials for each camera configuration are shown in \n\nFigure 1.\n\n\n\nFigure 1:Error standard deviation for different triangulation methods as a function of number of cameras, with 100 trials for each camera configuration.\n\nThe latency of LOST is also comparable to DLT and is much lesser than that of iterative optimization. The mean latencies for these methods as a function of the number of cameras are plotted in \n\nFigure 2. The latency of iterative optimization increases more rapidly with an increase in the number of camera measurements.\n\n\n\nFigure 2:Mean runtime of different triangulation methods as a function of number of cameras.\n\nThere are two key conclusions from these numerical experiments. First, LOST provides identical triangulation performance (i.e., identical errors) as the iterative nonlinear least squares (DLT+OPT) but at a fraction of the computational cost. There seems to be a strong case for using LOST instead of the conventional iterative methods in most situations - especially when runtime is important. Second, there are certainly some cases (such as this one) where using optimal triangulation provides substantial performance benefits when compared to the DLT.","type":"content","url":"/content/blogs/2023/2023-02-04-lost-triangulation#how-does-lost-compare-to-dlt-and-iterative-optimization","position":13},{"hierarchy":{"lvl1":"LOST in Triangulation","lvl2":"When does using optimal triangulation make the most difference?"},"type":"lvl2","url":"/content/blogs/2023/2023-02-04-lost-triangulation#when-does-using-optimal-triangulation-make-the-most-difference","position":14},{"hierarchy":{"lvl1":"LOST in Triangulation","lvl2":"When does using optimal triangulation make the most difference?"},"content":"Is optimal triangulation always better than DLT? There are two cases when it can provide much better results than DLT:\n\nThe 3D point is at significantly different ranges from each camera that observes it. Since the covariance of the estimate increases with its range from the camera, optimal triangulation weighs measurements from closer cameras more than those from farther cameras. This can also be seen in (12): the weights of each measurement are inversely proportional to the \\rho_i. The optimal estimates are therefore closer to the measurements from closer cameras.\n\nWhen different measurements have different 2D noise models. This can happen if the cameras observing these measurements are of different quality, or the measurement noise is non-uniformly distributed across the image (more on the edges and lesser at the center, for instance).\n\nIn other cases, the results from DLT and optimal triangulation are unlikely to be very different. For highly runtime-constrained applications that encounter the above scenarios, using LOST instead of results from DLT can reduce estimation errors.","type":"content","url":"/content/blogs/2023/2023-02-04-lost-triangulation#when-does-using-optimal-triangulation-make-the-most-difference","position":15},{"hierarchy":{"lvl1":"Contributing to GTSAM"},"type":"lvl1","url":"/content/notes/contributing","position":0},{"hierarchy":{"lvl1":"Contributing to GTSAM"},"content":"","type":"content","url":"/content/notes/contributing","position":1},{"hierarchy":{"lvl1":"Contributing to GTSAM","lvl2":"Introduction"},"type":"lvl2","url":"/content/notes/contributing#introduction","position":2},{"hierarchy":{"lvl1":"Contributing to GTSAM","lvl2":"Introduction"},"content":"If you have improvements to GTSAM, send us your pull requests!\n\nOur standard workflow is to fork GTSAM’s \n\nofficial GitHub repository into your own GitHub account and then push your changes into a branch on your fork. Once you believe your code is ready to be merged into GTSAM’s primary repository, open a pull request via the GitHub website. Your code will then undergo an interactive review process and Continuous Integration (CI) tests before it is merged into GTSAM’s primary repository.\n\nWe use \n\nGiflow as our branching model, as illustrated below, so most likely your branch will be either a fix or a feature branch.\n\nGTSAM’s CI service runs on all pull requests each time they are submitted and updated. Pull requests cannot be merged into master unless all unit tests pass on all supported platform configurations. We would like to hear about your success stories if you’ve used GTSAM in your own projects. Please consider contributing to our GTSAM Gallery by editing doc/gallery.rst and submitting a pull request with the update! ","type":"content","url":"/content/notes/contributing#introduction","position":3},{"hierarchy":{"lvl1":"Contributing to GTSAM","lvl2":"Licensing"},"type":"lvl2","url":"/content/notes/contributing#licensing","position":4},{"hierarchy":{"lvl1":"Contributing to GTSAM","lvl2":"Licensing"},"content":"Important note: GTSAM is an open source project licensed under extremely flexible terms intended to encourage use by anyone, for any purpose. When you make a contribution to the GTSAM project, you are agreeing to do so under those same terms.","type":"content","url":"/content/notes/contributing#licensing","position":5},{"hierarchy":{"lvl1":"Contributing to GTSAM","lvl2":"Testing"},"type":"lvl2","url":"/content/notes/contributing#testing","position":6},{"hierarchy":{"lvl1":"Contributing to GTSAM","lvl2":"Testing"},"content":"We are strong adherents of test-driven design and debugging. When you contribute a new feature, start with a unit test that exercises the API you want to provide, and only then write the code. If you believe there is an issue with GTSAM, please try to write a minimal unit test to reproduce the behavior, then fix it. Always write the test first.","type":"content","url":"/content/notes/contributing#testing","position":7},{"hierarchy":{"lvl1":"Contributing to GTSAM","lvl2":"Issue Tracking"},"type":"lvl2","url":"/content/notes/contributing#issue-tracking","position":8},{"hierarchy":{"lvl1":"Contributing to GTSAM","lvl2":"Issue Tracking"},"content":"For complex changes, especially those that will span multiple PRs, please open a GitHub issue and solicit design feedback before you invest a lot of time in code.\n\nBe prepared to engage in active code review on your pull requests. If a reviewer asks you for more information, that is a sign you should add more documentation to your PR.\n\nA PR generally should not include more than 750 added or changed lines (the green +### number as reported by github), and must not include more than 1500 lines of code.","type":"content","url":"/content/notes/contributing#issue-tracking","position":9},{"hierarchy":{"lvl1":"Contributing to GTSAM","lvl2":"Coding Conventions:"},"type":"lvl2","url":"/content/notes/contributing#coding-conventions","position":10},{"hierarchy":{"lvl1":"Contributing to GTSAM","lvl2":"Coding Conventions:"},"content":"For C++ we follow the \n\nGoogle C++ style guide.\n\nFor Python we use pep8 formatting and ask that you resolve all pylint issues.","type":"content","url":"/content/notes/contributing#coding-conventions","position":11},{"hierarchy":{"lvl1":"Contributing to GTSAM","lvl3":"Using GTSAM_EXPORT:","lvl2":"Coding Conventions:"},"type":"lvl3","url":"/content/notes/contributing#using-gtsam-export","position":12},{"hierarchy":{"lvl1":"Contributing to GTSAM","lvl3":"Using GTSAM_EXPORT:","lvl2":"Coding Conventions:"},"content":"On Windows it is necessary to explicitly export all functions from the library which should be externally accessible. To do this, include the macro GTSAM_EXPORT in your class or function definition.\n\nFor example:class GTSAM_EXPORT MyClass { ... };\n\nGTSAM_EXPORT myFunction();","type":"content","url":"/content/notes/contributing#using-gtsam-export","position":13},{"hierarchy":{"lvl1":"GTSAM Concepts"},"type":"lvl1","url":"/content/notes/gtsam-concepts","position":0},{"hierarchy":{"lvl1":"GTSAM Concepts"},"content":"As discussed in \n\nGeneric Programming Techniques, concepts define\n\nassociated types\n\nvalid expressions, like functions and values\n\ninvariants\n\ncomplexity guarantees\n\nBelow we discuss the most important concepts use in GTSAM, and after that we discuss how they are implemented/used/enforced.","type":"content","url":"/content/notes/gtsam-concepts","position":1},{"hierarchy":{"lvl1":"GTSAM Concepts","lvl3":"Manifold"},"type":"lvl3","url":"/content/notes/gtsam-concepts#manifold","position":2},{"hierarchy":{"lvl1":"GTSAM Concepts","lvl3":"Manifold"},"content":"To optimize over continuous types, we assume they are manifolds. This is central to GTSAM and hence discussed in some more detail below.\n\nManifolds and \n\ncharts are intimately linked concepts. We are only interested here in \n\ndifferentiable manifolds, continuous spaces that can be locally approximated at any point using a local vector space, called the \n\ntangent space. A chart is an invertible map from the manifold to that tangent space.\n\nIn GTSAM, all properties and operations needed to use a type must be defined through template specialization of the struct gtsam::traits. Concept checks are used to check that all required functions are implemented.\nIn detail, we ask that the following items are defined in the traits object (although, not all are needed for optimization):\n\nvalues:\n\nenum { dimension = D};, an enum that indicates the dimensionality n of the manifold. In Eigen-fashion, we also support manifolds whose dimensionality is only defined at runtime, by specifying the value -1.\n\ntypes:\n\nTangentVector, type that lives in tangent space. This will almost always be an Eigen::Matrix<double,n,1>.\n\nChartJacobian, a typedef for OptionalJacobian<dimension, dimension>.\n\nManifoldType, a pointer back to the type.\n\nstructure_category, a tag type that defines what requirements the type fulfills, and therefore what requirements this traits class must fulfill. It should be defined to be one of the following:\n\ngtsam::traits::manifold_tag -- Everything in this list is expected\n\ngtsam::traits::group_tag -- The functions defined under Groups below.\n\ngtsam::traits::lie_group_tag -- Everything in this list is expected, plus the functions defined under Groups, and Lie Groups below.\n\ngtsam::traits::vector_space_tag -- Everything in this list is expected, plus the functions defined under Groups, and Lie Groups below.\n\nvalid expressions:\n\nsize_t dim = traits<T>::GetDimension(p); static function should be defined. This is mostly useful if the size is not known at compile time.\n\nv = traits<T>::Local(p,q), the chart, from manifold to tangent space, think of it as q (-) p, where p and q are elements of the manifold and the result, v is an element of the vector space.\n\np = traits<T>::Retract(p,v), the inverse chart, from tangent space to manifold, think of it as p (+) v, where p is an element of the manifold and the result, v is an element of the vector space.\n\ninvariants\n\nRetract(p, Local(p,q)) == q\n\nLocal(p, Retract(p, v)) == v","type":"content","url":"/content/notes/gtsam-concepts#manifold","position":3},{"hierarchy":{"lvl1":"GTSAM Concepts","lvl3":"Group"},"type":"lvl3","url":"/content/notes/gtsam-concepts#group","position":4},{"hierarchy":{"lvl1":"GTSAM Concepts","lvl3":"Group"},"content":"A \n\ngroup should be well known from grade school :-), and provides a type with a composition operation that is closed, associative, has an identity element, and an inverse for each element. The following should be added to the traits class for a group:\n\nvalid expressions:\n\nr = traits<T>::Compose(p,q), where p, q, and r are elements of the manifold.\n\nq = traits<T>::Inverse(p), where p and q are elements of the manifold.\n\nr = traits<T>::Between(p,q), where p, q, and r are elements of the manifold.\n\nstatic members:\n\ntraits<T>::Identity, a static const member that represents the group’s identity element.\n\ninvariants:\n\nCompose(p,Inverse(p)) == Identity\n\nCompose(p,Between(p,q)) == q\n\nBetween(p,q) == Compose(Inverse(p),q)\n\nThe gtsam::group::traits namespace defines the following:\n\nvalues:\n\ntraits<T>::Identity -- The identity element for this group stored as a static const.\n\ntraits<T>::group_flavor -- the flavor of this group’s compose() operator, either:\n\ngtsam::traits::group_multiplicative_tag for multiplicative operator syntax, or\n\ngtsam::traits::group_additive_tag for additive operator syntax.\n\nWe do not at this time support more than one composition operator per type. Although mathematically possible, it is hardly ever needed, and the machinery to support it would be burdensome and counter-intuitive.\n\nAlso, a type should provide either multiplication or addition operators depending on the flavor of the operation. To distinguish between the two, we will use a tag (see below).","type":"content","url":"/content/notes/gtsam-concepts#group","position":5},{"hierarchy":{"lvl1":"GTSAM Concepts","lvl3":"Lie Group"},"type":"lvl3","url":"/content/notes/gtsam-concepts#lie-group","position":6},{"hierarchy":{"lvl1":"GTSAM Concepts","lvl3":"Lie Group"},"content":"A Lie group is both a manifold and a group. Hence, a LIE_GROUP type should implements both MANIFOLD and GROUP concepts.\nHowever, we now also need to be able to evaluate the derivatives of compose and inverse.\nHence, we have the following extra valid static functions defined in the struct gtsam::traits<T>:\n\nr = traits<T>::Compose(p,q,Hq,Hp)\n\nq = traits<T>::Inverse(p,Hp)\n\nr = traits<T>::Between(p,q,Hq,H2p)\n\nwhere above the H arguments stand for optional Jacobian arguments.\nThat makes it possible to create factors implementing priors (PriorFactor) or relations between two instances of a Lie group type (BetweenFactor).\n\nIn addition, a Lie group has a Lie algebra, which affords two extra valid expressions:\n\nv = traits<T>::Logmap(p,Hp), the log map, with optional Jacobian\n\np = traits<T>::Expmap(v,Hv), the exponential map, with optional Jacobian\n\nNote that in the Lie group case, the usual valid expressions for Retract and Local can be generated automatically, e.g.    T Retract(p,v,Hp,Hv) {\n      T q = Expmap(v,Hqv);\n      T r = Compose(p,q,Hrp,Hrq);\n      Hv = Hrq * Hqv; // chain rule\n      return r;\n    }\n\nFor Lie groups, the exponential map above is the most obvious mapping: it\nassociates straight lines in the tangent space with geodesics on the manifold\n(and it’s inverse, the log map). However, there are several cases in which we deviate from this:\n\nHowever, the exponential map is unnecessarily expensive for use in optimization. Hence, in GTSAM there is the option to provide a cheaper chart by means of the ChartAtOrigin struct in a class. This is done for SE(2), SO(3) and SE(3) (see Pose2, Rot3, Pose3)\n\nMost Lie groups we care about are Matrix groups, continuous sub-groups of GL(n), the group of n x n invertible matrices. In this case, a lot of the derivatives calculations needed can be standardized, and this is done by the LieGroup superclass. You only need to provide an AdjointMap method.\n\nA CRTP helper class LieGroup is available that can take a class and create some of the Lie group methods automatically. The class needs:\n\noperator* : implements group operator\n\ninverse: implements group inverse\n\nAdjointMap: maps tangent vectors according to group element\n\nExpmap/Logmap: exponential map and its inverse\n\nChartAtOrigin: struct where you define Retract/Local at origin\n\nTo use, simply derive, but also say using LieGroup<Class,N>::inverse so you get an inverse with a derivative.\n\nFinally, to create the traits automatically you can use internal::LieGroupTraits<Class>","type":"content","url":"/content/notes/gtsam-concepts#lie-group","position":7},{"hierarchy":{"lvl1":"GTSAM Concepts","lvl3":"Vector Space"},"type":"lvl3","url":"/content/notes/gtsam-concepts#vector-space","position":8},{"hierarchy":{"lvl1":"GTSAM Concepts","lvl3":"Vector Space"},"content":"While vector spaces are in principle also manifolds, it is overkill to think about charts etc. Really, we should simply think about vector addition and subtraction. I.e. where\n\nIdentity == 0\n\nInverse(p) == -p\n\nCompose(p,q) == p+q\n\nBetween(p,q) == q-p\n\nLocal(q) == p-q\n\nRetract(v) == p+v\n\nThis considerably simplifies certain operations. A VectorSpace superclass is available to implement the traits. Types that are vector space models include Matrix, Vector, any fixed or dynamic Eigen Matrix, Point2, and Point3.","type":"content","url":"/content/notes/gtsam-concepts#vector-space","position":9},{"hierarchy":{"lvl1":"GTSAM Concepts","lvl3":"Testable"},"type":"lvl3","url":"/content/notes/gtsam-concepts#testable","position":10},{"hierarchy":{"lvl1":"GTSAM Concepts","lvl3":"Testable"},"content":"Unit tests heavily depend on the following two functions being defined for all types that need to be tested:\n\nvalid expressions:\n\nPrint(p,s) where s is an optional string\n\nEquals(p,q,tol) where tol is an optional (double) tolerance","type":"content","url":"/content/notes/gtsam-concepts#testable","position":11},{"hierarchy":{"lvl1":"GTSAM Concepts","lvl2":"Implementation"},"type":"lvl2","url":"/content/notes/gtsam-concepts#implementation","position":12},{"hierarchy":{"lvl1":"GTSAM Concepts","lvl2":"Implementation"},"content":"GTSAM Types start with Uppercase, e.g., gtsam::Point2, and are models of the\nTESTABLE, MANIFOLD, GROUP, LIE_GROUP, and VECTOR_SPACE concepts.\n\ngtsam::traits is our way to associate these concepts with types,\nand we also define a limited number of gtsam::tags to select the correct implementation\nof certain functions at compile time (tag dispatching).","type":"content","url":"/content/notes/gtsam-concepts#implementation","position":13},{"hierarchy":{"lvl1":"GTSAM Concepts","lvl3":"Traits","lvl2":"Implementation"},"type":"lvl3","url":"/content/notes/gtsam-concepts#traits","position":14},{"hierarchy":{"lvl1":"GTSAM Concepts","lvl3":"Traits","lvl2":"Implementation"},"content":"However, a base class is not a good way to implement/check the other concepts, as we would like these\nto apply equally well to types that are outside GTSAM control, e.g., Eigen::VectorXd. This is where\n\n\ntraits come in.\n\nWe use Eigen-style or STL-style traits, that define many properties at once.\n\nNote that not everything that makes a concept is defined by traits. Valid expressions such as traits<T>::Compose are\ndefined simply as static functions within the traits class.\nFinally, for GTSAM types, it is perfectly acceptable (and even desired) to define associated types as internal types,\nrather than having to use traits internally.","type":"content","url":"/content/notes/gtsam-concepts#traits","position":15},{"hierarchy":{"lvl1":"GTSAM Concepts","lvl3":"Concept Checks","lvl2":"Implementation"},"type":"lvl3","url":"/content/notes/gtsam-concepts#concept-checks","position":16},{"hierarchy":{"lvl1":"GTSAM Concepts","lvl3":"Concept Checks","lvl2":"Implementation"},"content":"Boost provides a nice way to check whether a given type satisfies a concept. For example, the followingBOOST_CONCEPT_ASSERT(IsVectorSpace<Point2>)\n\nasserts that Point2 indeed is a model for the VectorSpace concept.","type":"content","url":"/content/notes/gtsam-concepts#concept-checks","position":17},{"hierarchy":{"lvl1":"GTSAM Concepts","lvl2":"Future Concepts"},"type":"lvl2","url":"/content/notes/gtsam-concepts#future-concepts","position":18},{"hierarchy":{"lvl1":"GTSAM Concepts","lvl2":"Future Concepts"},"content":"","type":"content","url":"/content/notes/gtsam-concepts#future-concepts","position":19},{"hierarchy":{"lvl1":"GTSAM Concepts","lvl3":"Group Action","lvl2":"Future Concepts"},"type":"lvl3","url":"/content/notes/gtsam-concepts#group-action","position":20},{"hierarchy":{"lvl1":"GTSAM Concepts","lvl3":"Group Action","lvl2":"Future Concepts"},"content":"Group actions are concepts in and of themselves that can be concept checked (see below).\nIn particular, a group can act on another space.\nFor example, the \n\ncyclic group of order 6 can rotate 2D vectors around the origin:q = R(i)*p\nwhere R(i) = R(60)^i, where R(60) rotates by 60 degrees\n\nHence, we formalize by the following extension of the concept:\n\nvalid expressions:\n\nq = traits<T>::Act(g,p), for some instance, p,  of a space S, that can be acted upon by the group element g to produce q in S.\n\nq = traits<T>::Act(g,p,Hp), if the space acted upon is a continuous differentiable manifold.\n\nIn the latter case, if S is an n-dimensional manifold, Hp is an output argument that should be\nfilled with the nxn Jacobian matrix of the action with respect to a change in p. It typically depends\non the group element g, but in most common examples will not depend on the value of p. For example, in\nthe cyclic group example above, we simply haveHp = R(i)\n\nNote there is no derivative of the action with respect to a change in g. That will only be defined\nfor Lie groups, which we introduce now.","type":"content","url":"/content/notes/gtsam-concepts#group-action","position":21},{"hierarchy":{"lvl1":"GTSAM Concepts","lvl3":"Lie Group Action","lvl2":"Future Concepts"},"type":"lvl3","url":"/content/notes/gtsam-concepts#lie-group-action","position":22},{"hierarchy":{"lvl1":"GTSAM Concepts","lvl3":"Lie Group Action","lvl2":"Future Concepts"},"content":"When a Lie group acts on a space, we have two derivatives to care about:\n\ngtasm::manifold::traits<T>::act(g,p,Hg,Hp), if the space acted upon is a continuous differentiable manifold.\n\nAn example is a similarity transform in 3D, which can act on 3D space, likeq = s*R*p + t\n\nNote that again the derivative in p, Hp is simply s R, which depends on g but not on p.\nThe derivative in g, Hg, is in general more complex.\n\nFor now, we won’t care about Lie groups acting on non-manifolds.","type":"content","url":"/content/notes/gtsam-concepts#lie-group-action","position":23},{"hierarchy":{"lvl1":"The Preintegrated IMU Factor"},"type":"lvl1","url":"/content/notes/imu-factor","position":0},{"hierarchy":{"lvl1":"The Preintegrated IMU Factor"},"content":"GTSAM includes a state of the art IMU handling scheme based on\n\nTodd Lupton and Salah Sukkarieh, “Visual-Inertial-Aided Navigation for High-Dynamic Motion in Built Environments Without Initial Conditions”, TRO, 28(1):61-76, 2012.\n\nOur implementation improves on this using integration on the manifold, as detailed in\n\nLuca Carlone, Zsolt Kira, Chris Beall, Vadim Indelman, and Frank Dellaert, “Eliminating conditionally independent sets in factor graphs: a unifying perspective based on smart factors”, Int. Conf. on Robotics and Automation (ICRA), 2014.\n\nChristian Forster, Luca Carlone, Frank Dellaert, and Davide Scaramuzza, “IMU Preintegration on Manifold for Efficient Visual-Inertial Maximum-a-Posteriori Estimation”, Robotics: Science and Systems (RSS), 2015.\n\nIf you are using the factor in academic work, please cite the publications above.\n\nIn GTSAM 4 a new and more efficient implementation, based on integrating on the NavState tangent space and detailed in docs/ImuFactor.pdf, is enabled by default. To switch to the RSS 2015 version, set the flag GTSAM_TANGENT_PREINTEGRATION to OFF.","type":"content","url":"/content/notes/imu-factor","position":1},{"hierarchy":{"lvl1":"Migrating from GTSAM 3"},"type":"lvl1","url":"/content/notes/migrating-from-3","position":0},{"hierarchy":{"lvl1":"Migrating from GTSAM 3"},"content":"GTSAM 4 introduces several new features, most notably Expressions and a python toolbox. We also deprecated some legacy functionality and wrongly named methods, but by default the flag GTSAM_ALLOW_DEPRECATED_SINCE_V4 is enabled, allowing anyone to just pull V4 and compile. To build the python toolbox, however, you will have to explicitly disable that flag.\n\nAlso, GTSAM 4 introduces traits, a C++ technique that allows optimizing with non-GTSAM types. A significant change which will not trigger a compile error is that zero-initializing of Point2 and Point3 will be deprecated, so please be aware that this might render functions using their default constructor incorrect.","type":"content","url":"/content/notes/migrating-from-3","position":1},{"hierarchy":{"lvl1":"About"},"type":"lvl1","url":"/content/about","position":0},{"hierarchy":{"lvl1":"About"},"content":"GTSAM is a sensor fusion library based on factor graphs, developed by Frank Dellaert and his students in Georgia Tech’s BORG Lab, as well as numerous open source contributors.\n\nCurrent Georgia Tech BORG Lab contributors include:\n\nVarun Agrawal\n\nAbhinav Jain\n\nMatthew Sklar\n\nMandy Xie\n\nBelow are the many Georgia Tech BORG lab alumni with their current afffiliation, if known:\n\nJeremy Aguilon, Facebook\n\nPablo Alcantarilla, iRobot\n\nSungtae An\n\nDoru Balcan, Bank of America\n\nChris Beall, Nuro\n\nLuca Carlone, MIT\n\nKrunal Chande, Fyusion\n\nAlex Cunningham, TRI\n\nJing Dong, Facebook Reality Labs\n\nPaul Drews, TRI\n\nAlireza Fathi, Google\n\nEohan George\n\nAlex Hagiopol, Microsoft\n\nViorela Ila, U. Sydney\n\nVadim Indelman, the Technion\n\nDavid Jensen, GTRI\n\nYong-Dian Jian, Nvidia\n\nMichael Kaess, Carnegie Mellon\n\nAbhijit Jundu, Google\n\nZhaoyang Lv, Facebook Reality Labs\n\nAndrew Melim, Oculus\n\nKai Ni, Holomatic\n\nCarlos Nieto, UCSD\n\nDuy-Nguyen Ta, TRI\n\nManohar Paluri, Facebook\n\nChristian Potthast, USC\n\nRichard Roberts, Google X\n\nGrant Schindler, Consultant\n\nNatesh Srinivasan, Apple\n\nAlex Trevor, Fyusion\n\nStephen Williams, BossaNova\n\nIn addition, we have had contrubutions from many others at different institutions and labs, again listed below with their current affiliations if known:\n\nAbe Bachrach, Skydio\n\nJose Luis Blanco-Claraco, University of Almería\n\nMatthew Broadway\n\nAdam Bry, Skydio\n\nMike Bosse, ETHZ\n\nGareth Cross, Skydio\n\nChristian Forster, Oculus Zurich\n\nPaul Furgale, Oculus Zurich\n\nHayk Martiros, Skydio\n\nEllon Paiva, LAAS-CNRS\n\nDavid M. Rosen, MIT\n\nThomas Schneider, ETHZ\n\nHannes Sommer, ETHZ\n\nAkash Patel, Georgia Tech\n\nChristian Vaugelade Berg, EasyMile\n\nIf you contributed but do not see your name listed above, or you are listed with an incorrect affiliation, please consider making a PR for this file at the \n\ngtsam.org github repo.","type":"content","url":"/content/about","position":1},{"hierarchy":{"lvl1":"The GTSAM Blog!"},"type":"lvl1","url":"/content/blog","position":0},{"hierarchy":{"lvl1":"The GTSAM Blog!"},"content":"Blogs are currently being ported from the older version of this site, formatting issues may still exist.","type":"content","url":"/content/blog","position":1},{"hierarchy":{"lvl1":"The GTSAM Blog!","lvl2":"2023"},"type":"lvl2","url":"/content/blog#id-2023","position":2},{"hierarchy":{"lvl1":"The GTSAM Blog!","lvl2":"2023"},"content":"February 04, 2023 → LOST in Triangulation","type":"content","url":"/content/blog#id-2023","position":3},{"hierarchy":{"lvl1":"The GTSAM Blog!","lvl3":"LOST in Triangulation","lvl2":"2023"},"type":"lvl3","url":"/content/blog#lost-in-triangulation","position":4},{"hierarchy":{"lvl1":"The GTSAM Blog!","lvl3":"LOST in Triangulation","lvl2":"2023"},"content":"Authors: Akshay Krishnan, Sebastien Henry, Frank Dellaert, John Christian\n\nThis post introduces the triangulation problem and some commonly used solutions (both optimal and sub-optimal), along with GTSAM code examples. We also discuss a recently proposed linear optimal triangulation method (LOST) and compare its performance to other methods...\n\nRead more →\n\n","type":"content","url":"/content/blog#lost-in-triangulation","position":5},{"hierarchy":{"lvl1":"The GTSAM Blog!","lvl2":"2021"},"type":"lvl2","url":"/content/blog#id-2021","position":6},{"hierarchy":{"lvl1":"The GTSAM Blog!","lvl2":"2021"},"content":"February 23, 2021 → Reducing Uncertainties, Part 1: Linear & Nonlinear","type":"content","url":"/content/blog#id-2021","position":7},{"hierarchy":{"lvl1":"The GTSAM Blog!","lvl3":"Reducing the uncertainty about the uncertainties, part 1: Linear and nonlinear","lvl2":"2021"},"type":"lvl3","url":"/content/blog#reducing-the-uncertainty-about-the-uncertainties-part-1-linear-and-nonlinear","position":8},{"hierarchy":{"lvl1":"The GTSAM Blog!","lvl3":"Reducing the uncertainty about the uncertainties, part 1: Linear and nonlinear","lvl2":"2021"},"content":"Author: Matias Mattamala\n\nThis post introduces linear and nonlinear uncertainty propagation in GTSAM, focusing on their implementation in factor graphs and their practical applications. We demonstrate how uncertainty affects graph-based optimization and discuss methods for handling it.\n\nRead more →\n\nFebruary 23, 2021 → Reducing Uncertainties, Part 2: Frames & Manifolds","type":"content","url":"/content/blog#reducing-the-uncertainty-about-the-uncertainties-part-1-linear-and-nonlinear","position":9},{"hierarchy":{"lvl1":"The GTSAM Blog!","lvl3":"Reducing the uncertainty about the uncertainties, part 2: Frames and manifolds","lvl2":"2021"},"type":"lvl3","url":"/content/blog#reducing-the-uncertainty-about-the-uncertainties-part-2-frames-and-manifolds","position":10},{"hierarchy":{"lvl1":"The GTSAM Blog!","lvl3":"Reducing the uncertainty about the uncertainties, part 2: Frames and manifolds","lvl2":"2021"},"content":"Author: Matias Mattamala\n\nThis post explores how frames and manifolds can be used to reduce uncertainty in graph-based optimization problems, offering insights into how GTSAM manages these elements.\n\nRead more →\n\nFebruary 23, 2021 → Reducing Uncertainties, Part 3: Adjoints & Covariances","type":"content","url":"/content/blog#reducing-the-uncertainty-about-the-uncertainties-part-2-frames-and-manifolds","position":11},{"hierarchy":{"lvl1":"The GTSAM Blog!","lvl3":"Reducing the uncertainty about the uncertainties, part 3: Adjoints and covariances","lvl2":"2021"},"type":"lvl3","url":"/content/blog#reducing-the-uncertainty-about-the-uncertainties-part-3-adjoints-and-covariances","position":12},{"hierarchy":{"lvl1":"The GTSAM Blog!","lvl3":"Reducing the uncertainty about the uncertainties, part 3: Adjoints and covariances","lvl2":"2021"},"content":"Author: Matias Mattamala\n\nIn this final post in the uncertainty series, we focus on adjoints and covariances, demonstrating their critical role in minimizing errors in GTSAM’s graph-based methods.\n\nRead more →\n\n","type":"content","url":"/content/blog#reducing-the-uncertainty-about-the-uncertainties-part-3-adjoints-and-covariances","position":13},{"hierarchy":{"lvl1":"The GTSAM Blog!","lvl2":"2020"},"type":"lvl2","url":"/content/blog#id-2020","position":14},{"hierarchy":{"lvl1":"The GTSAM Blog!","lvl2":"2020"},"content":"June 01, 2020 → What are Factor Graphs?","type":"content","url":"/content/blog#id-2020","position":15},{"hierarchy":{"lvl1":"The GTSAM Blog!","lvl3":"What are Factor Graphs?","lvl2":"2020"},"type":"lvl3","url":"/content/blog#what-are-factor-graphs","position":16},{"hierarchy":{"lvl1":"The GTSAM Blog!","lvl3":"What are Factor Graphs?","lvl2":"2020"},"content":"This post introduces the concept of factor graphs and explains how they are utilized in GTSAM to solve optimization problems by modeling complex systems of variables.\n\nRead more →\n\nJune 28, 2020 → Geometry and Variable Naming Conventions","type":"content","url":"/content/blog#what-are-factor-graphs","position":17},{"hierarchy":{"lvl1":"The GTSAM Blog!","lvl3":"Geometry and Variable Naming Conventions","lvl2":"2020"},"type":"lvl3","url":"/content/blog#geometry-and-variable-naming-conventions","position":18},{"hierarchy":{"lvl1":"The GTSAM Blog!","lvl3":"Geometry and Variable Naming Conventions","lvl2":"2020"},"content":"This blog explains the essential geometry concepts and variable naming conventions in GTSAM, ensuring consistency across the software and helping users apply graph-based optimization more efficiently.\n\nRead more →\n\nJuly 16, 2020 → Releasing GTSAM 4.0.3","type":"content","url":"/content/blog#geometry-and-variable-naming-conventions","position":19},{"hierarchy":{"lvl1":"The GTSAM Blog!","lvl3":"Releasing GTSAM 4.0.3","lvl2":"2020"},"type":"lvl3","url":"/content/blog#releasing-gtsam-4-0-3","position":20},{"hierarchy":{"lvl1":"The GTSAM Blog!","lvl3":"Releasing GTSAM 4.0.3","lvl2":"2020"},"content":"We are excited to announce the release of GTSAM 4.0.3! This update includes important fixes and performance improvements, ensuring the stability of factor graph-based optimization workflows.\n\nRead more →\n\nAugust 30, 2020 → Mount Rainier’s Eigenvectors","type":"content","url":"/content/blog#releasing-gtsam-4-0-3","position":21},{"hierarchy":{"lvl1":"The GTSAM Blog!","lvl3":"Mount Rainier’s Eigenvectors","lvl2":"2020"},"type":"lvl3","url":"/content/blog#mount-rainiers-eigenvectors","position":22},{"hierarchy":{"lvl1":"The GTSAM Blog!","lvl3":"Mount Rainier’s Eigenvectors","lvl2":"2020"},"content":"This post dives into the application of eigenvectors in GTSAM’s optimization routines, taking inspiration from the computational challenges of modeling Mount Rainier’s terrain.\n\nRead more →\n\n","type":"content","url":"/content/blog#mount-rainiers-eigenvectors","position":23},{"hierarchy":{"lvl1":"The GTSAM Blog!","lvl2":"2019"},"type":"lvl2","url":"/content/blog#id-2019","position":24},{"hierarchy":{"lvl1":"The GTSAM Blog!","lvl2":"2019"},"content":"May 18, 2019 → Moving to Github!","type":"content","url":"/content/blog#id-2019","position":25},{"hierarchy":{"lvl1":"The GTSAM Blog!","lvl3":"Moving to Github!","lvl2":"2019"},"type":"lvl3","url":"/content/blog#moving-to-github","position":26},{"hierarchy":{"lvl1":"The GTSAM Blog!","lvl3":"Moving to Github!","lvl2":"2019"},"content":"We’ve moved the GTSAM repository to GitHub to foster better collaboration. This change allows contributors to easily report issues, submit patches, and engage with the development of GTSAM.\n\nRead more →\n\nMay 20, 2019 → Launching \n\ngtsam.org","type":"content","url":"/content/blog#moving-to-github","position":27},{"hierarchy":{"lvl1":"The GTSAM Blog!","lvl3":"Launching gtsam.org","lvl2":"2019"},"type":"lvl3","url":"/content/blog#launching-gtsam-org","position":28},{"hierarchy":{"lvl1":"The GTSAM Blog!","lvl3":"Launching gtsam.org","lvl2":"2019"},"content":"We are thrilled to announce the launch of \n\ngtsam.org, a dedicated website for GTSAM, where users can find resources, documentation, and updates related to the software.\n\nRead more →\n\nSeptember 18, 2019 → So, what makes legged robots different?","type":"content","url":"/content/blog#launching-gtsam-org","position":29},{"hierarchy":{"lvl1":"The GTSAM Blog!","lvl3":"So, what makes legged robots different?","lvl2":"2019"},"type":"lvl3","url":"/content/blog#so-what-makes-legged-robots-different","position":30},{"hierarchy":{"lvl1":"The GTSAM Blog!","lvl3":"So, what makes legged robots different?","lvl2":"2019"},"content":"In this post, we discuss the unique challenges that come with optimizing factor graphs for legged robots, including factors like legged robot dynamics and sensor fusion.\n\nRead more →\n\nSeptember 18, 2019 → Look Ma, No RANSAC","type":"content","url":"/content/blog#so-what-makes-legged-robots-different","position":31},{"hierarchy":{"lvl1":"The GTSAM Blog!","lvl3":"Look Ma, No RANSAC","lvl2":"2019"},"type":"lvl3","url":"/content/blog#look-ma-no-ransac","position":32},{"hierarchy":{"lvl1":"The GTSAM Blog!","lvl3":"Look Ma, No RANSAC","lvl2":"2019"},"content":"We explore robust noise models in GTSAM and discuss how we can achieve reliable results without relying on traditional outlier rejection techniques like RANSAC.\n\nRead more →\n\nNovember 18, 2019 → LQR Control Using Factor Graphs","type":"content","url":"/content/blog#look-ma-no-ransac","position":33},{"hierarchy":{"lvl1":"The GTSAM Blog!","lvl3":"LQR Control Using Factor Graphs","lvl2":"2019"},"type":"lvl3","url":"/content/blog#lqr-control-using-factor-graphs","position":34},{"hierarchy":{"lvl1":"The GTSAM Blog!","lvl3":"LQR Control Using Factor Graphs","lvl2":"2019"},"content":"This post explains how to use GTSAM’s factor graph framework to solve optimal control problems, including an example of Linear Quadratic Regulator (LQR) control.\n\nRead more →","type":"content","url":"/content/blog#lqr-control-using-factor-graphs","position":35},{"hierarchy":{"lvl1":"Documentation"},"type":"lvl1","url":"/content/docs","position":0},{"hierarchy":{"lvl1":"Documentation"},"content":"For a hands-on mathematical introduction see the tutorial on \n\nFactor Graphs and GTSAM.\n\nA more thorough introduction to the use of factor graphs in robotics is the 2017 article \n\nFactor graphs for robot perception by Frank Dellaert and Michael Kaess.","type":"content","url":"/content/docs","position":1},{"hierarchy":{"lvl1":"Documentation","lvl2":"API and Wrapper Documentation"},"type":"lvl2","url":"/content/docs#api-and-wrapper-documentation","position":2},{"hierarchy":{"lvl1":"Documentation","lvl2":"API and Wrapper Documentation"},"content":"Currently, detailed API documentation is available only for C++ via the \n\nC++ Doxygen generated site.\n\nGTSAM comes with a python wrapper (see cython directory) and a matlab wrapper (see matlab directory), and for prototyping with GTSAM we highly recommend using one of the above. The auto-generated API documentation for python/MATLAB is limited to the number and type of input arguments, and again the \n\ndoxygen docs provide the details.","type":"content","url":"/content/docs#api-and-wrapper-documentation","position":3},{"hierarchy":{"lvl1":"Documentation","lvl2":"Notes on GTSAM"},"type":"lvl2","url":"/content/docs#notes-on-gtsam","position":4},{"hierarchy":{"lvl1":"Documentation","lvl2":"Notes on GTSAM"},"content":"GTSAM Concepts\n\nManifolds, groups, lie groups, vector spaces, testables...\n\nThe Preintegrated IMU Factor\n\nIMU handling scheme\n\nMigrating from GTSAM 3\n\nGTSAM 4 adds Expressions, a Python toolbox, and C++ traits for optimizing non-GTSAM types while deprecating some legacy features.\n\nContributing to GTSAM\n\nIf you have improvements to GTSAM, send us your pull requests!","type":"content","url":"/content/docs#notes-on-gtsam","position":5},{"hierarchy":{"lvl1":"Documentation","lvl2":"Additional Information"},"type":"lvl2","url":"/content/docs#additional-information","position":6},{"hierarchy":{"lvl1":"Documentation","lvl2":"Additional Information"},"content":"There is a GTSAM users \n\nGoogle group for general discussion.","type":"content","url":"/content/docs#additional-information","position":7},{"hierarchy":{"lvl1":"Get Started"},"type":"lvl1","url":"/content/get-started","position":0},{"hierarchy":{"lvl1":"Get Started"},"content":"Welcome to GTSAM! Please follow this guide to use GTSAM on your platform.\n\n","type":"content","url":"/content/get-started","position":1},{"hierarchy":{"lvl1":"Get Started","lvl2":"Build GTSAM"},"type":"lvl2","url":"/content/get-started#build-gtsam","position":2},{"hierarchy":{"lvl1":"Get Started","lvl2":"Build GTSAM"},"content":"","type":"content","url":"/content/get-started#build-gtsam","position":3},{"hierarchy":{"lvl1":"Get Started","lvl3":"Use a compatible configuration","lvl2":"Build GTSAM"},"type":"lvl3","url":"/content/get-started#use-a-compatible-configuration","position":4},{"hierarchy":{"lvl1":"Get Started","lvl3":"Use a compatible configuration","lvl2":"Build GTSAM"},"content":"First, make sure to use a compatible operating system and compiler:\n\nTested Operating Systems\n\nTested Compilers\n\nUbuntu 16.04 - 18.04\n\nGCC 4.2-7.3\n\nMacOS 10.6 - 10.14, 15.2-15.3\n\nOS X Clang 2.9-10.0\n\nWindows 7, 8, 8.1, 10\n\nOS X GCC 4.2\n\n\n\nMSVC 2017","type":"content","url":"/content/get-started#use-a-compatible-configuration","position":5},{"hierarchy":{"lvl1":"Get Started","lvl3":"Download prerequisite libraries","lvl2":"Build GTSAM"},"type":"lvl3","url":"/content/get-started#download-prerequisite-libraries","position":6},{"hierarchy":{"lvl1":"Get Started","lvl3":"Download prerequisite libraries","lvl2":"Build GTSAM"},"content":"Then install the following libraries on your system:\n\nBoost ≥ 1.43 (install through Linux repositories or MacPorts)\n\nCMake ≥ 3.0\n\n(MacOS) Support for XCode 4.3 command line tools requires CMake 2.8.8 or higher\n\nOptional libraries\n\nThese additional libraries are used automatically—if findable by CMake.\n\nIntel Threaded Building Blocks \n\n(oneTBB)\n\nIf oneTBB is installed and detectable by CMake, GTSAM will use it automatically. Ensure that CMake prints Use Intel TBB : Yes.\n\nTo disable the use of TBB, disable the CMake flag GTSAM_WITH_TBB (enabled by default).\n\nOn Ubuntu, TBB may be installed from the Ubuntu repositories, and for other platforms it may be downloaded from \n\nhere.\n\nIntel Math Kernel Library \n\n(MKL)\n\nMKL may not provide a speedup in all cases. Make sure to benchmark your problem with and without MKL.\n\nGTSAM may be configured to use MKL by toggling GTSAM_WITH_EIGEN_MKL and GTSAM_WITH_EIGEN_MKL_OPENMP to ON. However, best performance is usually achieved with MKL disabled. We therefore advise you to benchmark your problem before using MKL.\n\nSee \n\nInstalling MKL on Linux for more installation information.\n\nInstalling libraries on Ubuntusudo apt-get install libboost-all-dev   # Boost\nsudo apt-get install cmake              # Cmake\n\nFor optional libraries:\n\nTBB: sudo apt-get install libtbb-dev\n\nMKL: \n\ninstall using APT\n\n","type":"content","url":"/content/get-started#download-prerequisite-libraries","position":7},{"hierarchy":{"lvl1":"Get Started","lvl3":"Install GTSAM","lvl2":"Build GTSAM"},"type":"lvl3","url":"/content/get-started#install-gtsam","position":8},{"hierarchy":{"lvl1":"Get Started","lvl3":"Install GTSAM","lvl2":"Build GTSAM"},"content":"Clone or download the latest release from the \n\nGTSAM Github repo.\n\nThen execute the commands below in the root directory for an out-of-source build.#!bash\n$ mkdir build\n$ cd build\n$ cmake ..\n$ make check    # optional, runs unit tests\n$ make install\n\nThis will build the library and unit tests (to the default system install path), run all of the unit tests, and then install the library itself.\n\nGTSAM can be installed on Ubuntu via \n\nthese PPA repositories as well.\nAs of November 2020, packages for Xenial (u16.04), Bionic (u18.04), and Focal (u20.04) are published.\n\nFollow the code below to add PPA for your preferred branch:\n\nLatest 4.x stable release# Add PPA\nsudo add-apt-repository ppa:borglab/gtsam-release-4.0\nsudo apt update  # not necessary since Bionic\n# Install:\nsudo apt install libgtsam-dev libgtsam-unstable-dev\n\nNightly builds (develop branch)# Add PPA\nsudo add-apt-repository ppa:borglab/gtsam-develop\nsudo apt update  # not necessary since Bionic\n# Install:\nsudo apt install libgtsam-dev libgtsam-unstable-dev\n\nWarning\n\nInstalling GTSAM on Arch Linux has not been tested by GTSAM developers.\n\nGTSAM is available in the Arch User Repository\n(\n\nAUR) as\n\n\ngtsam.\n\nYou can manually install the package by following the instructions on the\n\n\nArch Wiki\nor use an \n\nAUR helper like\n\n\nyay\n(recommended for ease of install).\n\nIt is also recommended to use the\n\n\narch4edu\nrepository. They are hosting many packages related to education and research,\nincluding robotics such as ROS. Adding a repository allows for you to install\nbinaries of packages, instead of compiling them from source.\nThis will greatly speed up your installation time. Visit \n\nhere to add and use arch4edu.\n\nInstall GTSAMyay -S gtsam\n\nor\n\nInstall GTSAM with Intel Accelerationsyay -S gtsam-mkl\n\nTo discuss any issues related to this package refer to the comments section on\nthe AUR page of gtsam \n\nhere.\n\n","type":"content","url":"/content/get-started#install-gtsam","position":9},{"hierarchy":{"lvl1":"Get Started","lvl2":"Run unit tests"},"type":"lvl2","url":"/content/get-started#run-unit-tests","position":10},{"hierarchy":{"lvl1":"Get Started","lvl2":"Run unit tests"},"content":"make check will build and run all of the tests. Note that tests will only be built when using the “check” targets, to prevent make install from building the tests unnecessarily.\n\nYou can also run make timing to build all of the timing scripts.\nTo run check on a particular module only, run make check.[subfolder]. So to run just the geometry tests, execute:make check.geometry\n\nIndividual tests can be run by appending .run to the name of the test. For example, to run testMatrix, execute:make testMatrix.run\n\n","type":"content","url":"/content/get-started#run-unit-tests","position":11},{"hierarchy":{"lvl1":"Get Started","lvl2":"CMake configuration options"},"type":"lvl2","url":"/content/get-started#cmake-configuration-options","position":12},{"hierarchy":{"lvl1":"Get Started","lvl2":"CMake configuration options"},"content":"GTSAM has a number of options that can be configured, which is best done with\none of the following:\n\nccmake:      the \n\ncurses GUI for cmake\n\ncmake-gui:   a real GUI for cmake\n\nImportant options include:\n\nCMAKE_BUILD_TYPE\n\nWe support several build configurations for GTSAM (case insensitive)cmake -DCMAKE_BUILD_TYPE=[Option] ..\n\nDebug: All error checking options on, no optimization. Use for development of new features and fixing issues.\n\nRelease: Optimizations turned on, no debug symbols.\n\nTiming: Adds ENABLE_TIMING flag to provide statistics on operation\n\nProfiling: Standard configuration for use during profiling\n\nRelWithDebInfo: Same as Release, but with the - g flag\nfor debug symbols\n\nCMAKE_INSTALL_PREFIX\n\nThe install folder. The default is typically /usr/local/.\nTo configure to install to your home directory, you could execute:cmake -DCMAKE_INSTALL_PREFIX:PATH=$HOME ..\n\nGTSAM_TOOLBOX_INSTALL_PATH\n\nThe Matlab toolbox will be installed in a subdirectory of this folder, called gtsam.cmake -DGTSAM_TOOLBOX_INSTALL_PATH:PATH=$HOME/toolbox ..\n\nGTSAM_BUILD_CONVENIENCE_LIBRARIES\n\nThis is a build option to allow for tests in subfolders to be linked against convenience libraries rather than the full libgtsam.\nSet with the command line as follows:cmake -DGTSAM_BUILD_CONVENIENCE_LIBRARIES:OPTION=ON ..\n\nON (Default): This builds convenience libraries and links tests against them.\n\nThis option is suggested for gtsam developers, as it is possible to build and run tests without first building the rest of the library, and speeds up compilation for a single test. The downside of this option is that it will build the entire library again to build the full libgtsam library, so build / install will be slower.\n\nOFF: This will build all of libgtsam before any of the tests, and then link all of the tests at once.\n\nThis option is best\nfor users of GTSAM, as it avoids rebuilding the entirety of gtsam an extra time.\n\nGTSAM_BUILD_UNSTABLE\n\nEnable build and install for libgtsam_unstable library.\nSet with the command line as follows:cmake -DGTSAM_BUILD_UNSTABLE:OPTION=ON ..\n\nON (Default): When enabled, libgtsam_unstable will be built and installed with the same options as libgtsam. In addition, if tests are enabled, the unit tests will be built as well. The Matlab toolbox will also be generated\nif the matlab toolbox is enabled, installing into a folder called gtsam_unstable.\n\nOFF: If disabled, no gtsam_unstable code will be included in build or install.\n\nMEX_COMMAND\n\nPath to the mex compiler. Defaults to assume the path is included in your shell’s PATH environment variable. mex is installed with matlab at $MATLABROOT/bin/mex.\n\nThe correct value for  MATLABROOT can be found by executing the command matlabroot in MATLAB.\n\n","type":"content","url":"/content/get-started#cmake-configuration-options","position":13},{"hierarchy":{"lvl1":"Get Started","lvl2":"Debugging tips"},"type":"lvl2","url":"/content/get-started#debugging-tips","position":14},{"hierarchy":{"lvl1":"Get Started","lvl2":"Debugging tips"},"content":"GTSAM makes extensive use of debug assertions, so we highly recommend you work in Debug mode while developing (which is not enabled by default).\n\nLikewise, it is imperative that you switch back to release mode when running finished code and for timing. GTSAM will run up to 10x faster in Release mode!\n\nAnother useful debugging symbol is _GLIBCXX_DEBUG, which enables debug checks and safe containers in the standard C++ library and makes problems much easier to find.\n\nWarning\n\nThe native Snow Leopard g++ compiler / library contains a bug that makes it impossible to use _GLIBCXX_DEBUG. MacPorts g++ compilers do work with it though.\n\nIf _GLIBCXX_DEBUG is used to compile gtsam, anything that links against gtsam will need to be compiled with _GLIBCXX_DEBUG as well, due to the use of header - only Eigen.\n\n\n\n","type":"content","url":"/content/get-started#debugging-tips","position":15},{"hierarchy":{"lvl1":"Get Started","lvl2":"Performance tips"},"type":"lvl2","url":"/content/get-started#performance-tips","position":16},{"hierarchy":{"lvl1":"Get Started","lvl2":"Performance tips"},"content":"Here are some tips to get the best possible performance out of GTSAM.\n\nBuild in Release mode.\n\nGTSAM will run up to 10x faster compared to Debug mode.\n\nEnable TBB.\n\nOn modern processors with multiple cores, this can easily speed up optimization 30-50%.\n\nPlease note that this may not be true for very small problems where the overhead of dispatching work to multiple threads outweighs the benefit. We recommend that you benchmark your problem with / without TBB.\n\nAdd -march=native to GTSAM_CMAKE_CXX_FLAGS. A performance gain of 25 - 30% can be expected on modern processors.\n\nNote that this affects the portability of your executable. It may not run when copied to another system with older / different processor architecture. Also note that all dependent projects must be compiled with the same flag, or segfaults and other undefined behavior may result.\n\nPossibly enable MKL.\n\nPlease note that our benchmarks have shown that this helps only in very limited cases, and actually hurts performance in the usual case. We therefore recommend that you do not enable MKL, unless you have benchmarked it on your problem and have verified that it improves performance.\n\n","type":"content","url":"/content/get-started#performance-tips","position":17},{"hierarchy":{"lvl1":"Get Started","lvl2":"View documentation"},"type":"lvl2","url":"/content/get-started#view-documentation","position":18},{"hierarchy":{"lvl1":"Get Started","lvl2":"View documentation"},"content":"GTSAM has \n\nDoxygen documentation. To generate, run make doc from your build directory, or refer to the \n\nstatically generated version on this website.","type":"content","url":"/content/get-started#view-documentation","position":19},{"hierarchy":{"lvl1":"GTSAM 4.1"},"type":"lvl1","url":"/content","position":0},{"hierarchy":{"lvl1":"GTSAM 4.1"},"content":"Factor graphs for Sensor Fusion in Robotics.\n\n\n\nGTSAM: Georgia Tech Smoothing and Mapping\n\nGTSAM 4.1 is a BSD-licensed C++ library that implements sensor fusion for robotics and computer vision applications, including SLAM (Simultaneous Localization and Mapping), VO (Visual Odometry), and SFM (Structure from Motion). It uses factor graphs and Bayes networks as the underlying computing paradigm rather than sparse matrices to optimize for the most probable configuration or an optimal plan.\n\nCoupled with a capable sensor front-end (not provided here), GTSAM powers many impressive autonomous systems, in both academia and industry.","type":"content","url":"/content","position":1},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM"},"type":"lvl1","url":"/content/tutorial","position":0},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM"},"content":"This is an updated version of the 2012 tech-report \n\nFactor Graphs and GTSAM: A Hands-on Introduction by \n\nFrank Dellaert. A more thorough introduction to the use of factor graphs in robotics is the 2017 article \n\nFactor graphs for robot perception by Frank Dellaert and Michael Kaess.","type":"content","url":"/content/tutorial","position":1},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl2":"Overview"},"type":"lvl2","url":"/content/tutorial#overview","position":2},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl2":"Overview"},"content":"Factor graphs are graphical models (\n\nKoller and Friedman, 2009) that are well suited to modeling complex estimation problems, such as Simultaneous Localization and Mapping (SLAM) or Structure from Motion (SFM). You might be familiar with another often used graphical model, Bayes networks, which are directed acyclic graphs. A factor graph, however, is a bipartite graph consisting of factors connected to variables. The variables represent the unknown random variables in the estimation problem, whereas the factors represent probabilistic constraints on those variables, derived from measurements or prior knowledge. In the following sections I will illustrate this with examples from both robotics and vision.\n\nThe GTSAM toolbox (GTSAM stands for “Georgia Tech Smoothing and Mapping”) toolbox is a BSD-licensed C++ library based on factor graphs, developed at the Georgia Institute of Technology by myself, many of my students, and collaborators. It provides state of the art solutions to the SLAM and SFM problems, but can also be used to model and solve both simpler and more complex estimation problems. It also provides a MATLAB interface which allows for rapid prototype development, visualization, and user interaction.\n\nGTSAM exploits sparsity to be computationally efficient. Typically measurements only provide information on the relationship between a handful of variables, and hence the resulting factor graph will be sparsely connected. This is exploited by the algorithms implemented in GTSAM to reduce computational complexity. Even when graphs are too dense to be handled efficiently by direct methods, GTSAM provides iterative methods that are quite efficient regardless.\n\nYou can download the latest version of GTSAM from our \n\nGithub repo.","type":"content","url":"/content/tutorial#overview","position":3},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl2":"1. Factor Graphs"},"type":"lvl2","url":"/content/tutorial#id-1-factor-graphs","position":4},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl2":"1. Factor Graphs"},"content":"Let us start with a one-page primer on factor graphs, which in no way replaces the excellent and detailed reviews by \n\nKschischang & Loeliger (2001) and \n\nLoeliger (2004) (2004).\n\n\n\nFigure 1:An HMM, unrolled over three time-steps, represented by a Bayes net.\n\nFigure 1 shows the Bayes network for a hidden Markov model (HMM) over three time steps. In a Bayes net, each node is associated with a conditional density: the top Markov chain encodes the prior P(X_1) and transition probabilities P(X_2|X_1) and P(X_3|X_2), whereas measurements Z_t depend only on the state X_t, modeled by conditional densities P(Z_t|X_t).\n\nGiven known measurements z_1, z_2, and z_3 we are interested in the hidden state sequence (X_1, X_2, X_3) that maximizes the posterior probability P(X_1, X_2, X_3 | Z_1 = z_1, Z_2 = z_2, Z_3 = z_3). Since the measurements z_1, z_2, and z_3 are known, the posterior is proportional to the product of six factors, three of which derive from the the Markov chain, and three likelihood factors defined as L(X_t; z) \\propto P(Z_t = z | X_t):P(X_1, X_2, X_3 | Z_1 = z_1, Z_2 = z_2, Z_3 = z_3) \\propto P(X_1) P(X_2|X_1) P(X_3|X_2) L(X_1; z_1) L(X_2; z_2) L(X_3; z_3)\n\n\n\nFigure 2:An HMM with observed measurements, unrolled over time, represented as a factor graph.\n\nThis motivates a different graphical model, a factor graph, in which we only represent the unknown variables X_1, X_2, and X_3, connected to factors that encode probabilistic information on them, as in Figure 2. To do maximum a-posteriori (MAP) inference, we then maximize the productf(X_1, X_2, X_3) = \\prod f_i(\\mathscr{X}_i)\n\ni.e., the value of the factor graph. It should be clear from the figure that the connectivity of a factor graph encodes, for each factor f_i, which subset of variables  (\\mathscr{X}_i it depends on. In the examples below, we use factor graphs to model more complex MAP inference problems in robotics.","type":"content","url":"/content/tutorial#id-1-factor-graphs","position":5},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl2":"2. Modeling Robot Motion"},"type":"lvl2","url":"/content/tutorial#id-2-modeling-robot-motion","position":6},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl2":"2. Modeling Robot Motion"},"content":"","type":"content","url":"/content/tutorial#id-2-modeling-robot-motion","position":7},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"2.1 Modeling with Factor Graphs","lvl2":"2. Modeling Robot Motion"},"type":"lvl3","url":"/content/tutorial#id-2-1-modeling-with-factor-graphs","position":8},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"2.1 Modeling with Factor Graphs","lvl2":"2. Modeling Robot Motion"},"content":"Before diving into a SLAM example, let us consider the simpler problem of modeling robot motion. This can be done with a continuous Markov chain, and provides a gentle introduction to GTSAM.\n\n\n\nFigure 3:Factor graph for robot localization.\n\nThe factor graph for a simple example is shown in Figure 3. There are three variables x_1, x_2, and x_3 which represent the poses of the robot over time, rendered in the figure by the open-circle variable nodes. In this example, we have one unary factor f_0(x_1) on the first pose x_1 that encodes our prior knowledge about x_1, and two binary factors that relate successive poses, respectively f_1(x_1, x_2; o_1) and f_2(x_2, x_3; o_2) where o_1 and o_2 represent odometry measurements.","type":"content","url":"/content/tutorial#id-2-1-modeling-with-factor-graphs","position":9},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"2.2 Creating a Factor Graph","lvl2":"2. Modeling Robot Motion"},"type":"lvl3","url":"/content/tutorial#id-2-2-creating-a-factor-graph","position":10},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"2.2 Creating a Factor Graph","lvl2":"2. Modeling Robot Motion"},"content":"The following C++ code, included in GTSAM as an example, creates the factor graph in Figure 3:// Create an empty nonlinear factor graph\nNonlinearFactorGraph graph;\n\n// Add a Gaussian prior on pose x_1\nPose2 priorMean(0.0, 0.0, 0.0);\nnoiseModel::Diagonal::shared_ptr priorNoise =\n  noiseModel::Diagonal::Sigmas(Vector3(0.3, 0.3, 0.1));\ngraph.add(PriorFactor<Pose2>(1, priorMean, priorNoise));\n\n// Add two odometry factors\nPose2 odometry(2.0, 0.0, 0.0);\nnoiseModel::Diagonal::shared_ptr odometryNoise =\n  noiseModel::Diagonal::Sigmas(Vector3(0.2, 0.2, 0.1));\ngraph.add(BetweenFactor<Pose2>(1, 2, odometry, odometryNoise));\ngraph.add(BetweenFactor<Pose2>(2, 3, odometry, odometryNoise));\n\nAbove, line 2 creates an empty factor graph. We then add the factor f_0(x_1) on lines 5-8 as an instance of PriorFactor<T>, a templated class provided in the slam subfolder, with T=Pose2. Its constructor takes a variable Key (in this case 1), a mean of type Pose2, created on Line 5, and a noise model for the prior density. We provide a diagonal Gaussian of type noiseModel::Diagonal by specifying three standard deviations in line 7, respectively 30 cm. on the robot’s position, and 0.1 radians on the robot’s orientation. Note that the Sigmas constructor returns a shared pointer, anticipating that typically the same noise models are used for many different factors.\n\nSimilarly, odometry measurements are specified as Pose2 on line 11, with a slightly different noise model defined on line 12-13. We then add the two factors f_1(x_1, x_2; o_1) and f_2(x_2, x_3; o_2) on lines 14-15, as instances of yet another templated class, BetweenFactor<T>, again with T=Pose2.\n\nWhen running the example (make OdometryExample.run on the command prompt), it will print out the factor graph as follows:Factor Graph:\nsize: 3\nFactor 0: PriorFactor on 1\nprior mean: (0, 0, 0)\nnoise model: diagonal sigmas [0.3; 0.3; 0.1];\nFactor 1: BetweenFactor(1,2)\nmeasured: (2, 0, 0)\nnoise model: diagonal sigmas [0.2; 0.2; 0.1];\nFactor 2: BetweenFactor(2,3)\nmeasured: (2, 0, 0)\nnoise model: diagonal sigmas [0.2; 0.2; 0.1];","type":"content","url":"/content/tutorial#id-2-2-creating-a-factor-graph","position":11},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"2.3 Factor Graphs vs. Values","lvl2":"2. Modeling Robot Motion"},"type":"lvl3","url":"/content/tutorial#id-2-3-factor-graphs-vs-values","position":12},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"2.3 Factor Graphs vs. Values","lvl2":"2. Modeling Robot Motion"},"content":"At this point it is instructive to emphasize two important design ideas underlying GTSAM:\n\nThe factor graph and its embodiment in code specify the joint probability distribution P(X|Z) over the entire trajectory X = {x_1, x_2, x_3} of the robot, rather than just the last pose. This smoothing view of the world gives GTSAM its name: “smoothing and mapping”. Later in this document we will talk about how we can also use GTSAM to do filtering (which you often do not want to do) or incremental inference (which we do all the time).\n\nA factor graph in GTSAM is just the specification of the probability density P(X|Z), and the corresponding FactorGraph class and its derived classes do not ever contain a “solution”. Rather, there is a separate type Values that is used to specify specific values for (in this case) x_1, x_2, and x_3, which can then be used to evaluate the probability (or, more commonly, the error) associated with particular values.\n\nThe latter point is often a point of confusion with beginning users of GTSAM. It helps to remember that when designing GTSAM we took a functional approach of classes corresponding to mathematical objects, which are usually immutable. You should think of a factor graph as a function to be applied to values—as the notation f(X) \\propto P(X|Z) implies—rather than as an object to be modified.","type":"content","url":"/content/tutorial#id-2-3-factor-graphs-vs-values","position":13},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"2.4 Non-linear Optimization in GTSAM","lvl2":"2. Modeling Robot Motion"},"type":"lvl3","url":"/content/tutorial#id-2-4-non-linear-optimization-in-gtsam","position":14},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"2.4 Non-linear Optimization in GTSAM","lvl2":"2. Modeling Robot Motion"},"content":"The listing below creates a Values instance, and uses it as the initial estimate to find the maximum a-posteriori (MAP) assignment for the trajectory X:// create (deliberately inaccurate) initial estimate\nValues initial;\ninitial.insert(1, Pose2(0.5, 0.0, 0.2));\ninitial.insert(2, Pose2(2.3, 0.1, -0.2));\ninitial.insert(3, Pose2(4.1, 0.1, 0.1));\n\n// optimize using Levenberg-Marquardt optimization\nValues result = LevenbergMarquardtOptimizer(graph, initial).optimize();\n\nLines 2-5 in Listing 2.4 create the initial estimate, and on line 8 we create a non-linear Levenberg-Marquardt style optimizer, and call optimize using default parameter settings. The reason why GTSAM needs to perform non-linear optimization is because the odometry factors f_1(x_1, x_2; o_1) and f_2(x_2, x_3; o_2) are non-linear, as they involve the orientation of the robot. This also explains why the factor graph we created in Listing 2.2 is of type NonlinearFactorGraph. The optimization class linearizes this graph, possibly multiple times, to minimize the non-linear squared error specified by the factors.\n\nThe relevant output from running the example is as follows:Initial Estimate:\nValues with 3 values:\nValue 1: (0.5, 0, 0.2)\nValue 2: (2.3, 0.1, -0.2)\nValue 3: (4.1, 0.1, 0.1)\n\nFinal Result:\nValues with 3 values:\nValue 1: (-1.8e-16, 8.7e-18, -9.1e-19)\nValue 2: (2, 7.4e-18, -2.5e-18)\nValue 3: (4, -1.8e-18, -3.1e-18)\n\nIt can be seen that, subject to very small tolerance, the ground truth solution x_1=(0, 0, 0), x_2=(2, 0, 0), and x_3=(4, 0, 0) is recovered.","type":"content","url":"/content/tutorial#id-2-4-non-linear-optimization-in-gtsam","position":15},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"2.5 Full Posterior Inference","lvl2":"2. Modeling Robot Motion"},"type":"lvl3","url":"/content/tutorial#id-2-5-full-posterior-inference","position":16},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"2.5 Full Posterior Inference","lvl2":"2. Modeling Robot Motion"},"content":"GTSAM can also be used to calculate the covariance matrix for each pose after incorporating the information from all measurements Z. Recognizing that the factor graph encodes the posterior density P(X|Z), the mean μ together with the covariance Σ for each pose x approximate the marginal posterior density P(x|Z). Note that this is just an approximation, as even in this simple case the odometry factors are actually non-linear in their arguments, and GTSAM only computes a Gaussian approximation to the true underlying posterior.\n\nThe following C++ code will recover the posterior marginals:// Query the marginals\ncout.precision(2);\nMarginals marginals(graph, result);\ncout << \"x1 covariance:\\n\" << marginals.marginalCovariance(1) << endl;\ncout << \"x2 covariance:\\n\" << marginals.marginalCovariance(2) << endl;\ncout << \"x3 covariance:\\n\" << marginals.marginalCovariance(3) << endl;\n\nThe relevant output from running the example is as follows:x1 covariance:\n    0.09     1.1e-47     5.7e-33\n    1.1e-47        0.09     1.9e-17\n    5.7e-33     1.9e-17        0.01\nx2 covariance:\n    0.13     4.7e-18     2.4e-18\n    4.7e-18        0.17        0.02\n    2.4e-18        0.02        0.02\nx3 covariance:\n    0.17     2.7e-17     8.4e-18\n    2.7e-17        0.37        0.06\n    8.4e-18        0.06        0.03\n\nWhat we see is that the marginal covariance P(x_1|Z) on x_1 is simply the prior knowledge on x_1, but as the robot moves the uncertainty in all dimensions grows without bound, and the y and θ components of the pose become (positively) correlated.\n\nAn important fact to note when interpreting these numbers is that covariance matrices are given in relative coordinates, not absolute coordinates. This is because internally GTSAM optimizes for a change with respect to a linearization point, as do all nonlinear optimization libraries.","type":"content","url":"/content/tutorial#id-2-5-full-posterior-inference","position":17},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl2":"3. Robot Localization"},"type":"lvl2","url":"/content/tutorial#id-3-robot-localization","position":18},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl2":"3. Robot Localization"},"content":"","type":"content","url":"/content/tutorial#id-3-robot-localization","position":19},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"3.1 Unary Measurement Factors","lvl2":"3. Robot Localization"},"type":"lvl3","url":"/content/tutorial#id-3-1-unary-measurement-factors","position":20},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"3.1 Unary Measurement Factors","lvl2":"3. Robot Localization"},"content":"In this section we add measurements to the factor graph that will help us actually localize the robot over time. The example also serves as a tutorial on creating new factor types.\n\n\n\nFigure 4:Robot localization factor graph with unary measurement factors at each time step.\n\nIn particular, we use unary measurement factors to handle external measurements. The example from Section 2 is not very useful on a real robot, because it only contains factors corresponding to odometry measurements. These are imperfect and will lead to quickly accumulating uncertainty on the last robot pose, at least in the absence of any external measurements (see Section 2.5). Figure 4 shows a new factor graph where the prior f_0(x_1) is omitted and instead we added three unary factors f_1(x_1; z_1), f_2(x_2; z_2), f_3(x_3; z_3) one for each localization measurement z_t, respectively. Such unary factors are applicable for measurements z_tthat depend only on the current robot pose, e.g., GPS readings, correlation of a laser range-finder in a pre-existing map, or indeed the presence of absence of ceiling lights (see \n\nDellaert & Thrun (1999) for that amusing example).","type":"content","url":"/content/tutorial#id-3-1-unary-measurement-factors","position":21},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"3.2 Defining Custom Factors","lvl2":"3. Robot Localization"},"type":"lvl3","url":"/content/tutorial#id-3-2-defining-custom-factors","position":22},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"3.2 Defining Custom Factors","lvl2":"3. Robot Localization"},"content":"In GTSAM, you can create custom unary factors by deriving a new class from the built-in class NoiseModelFactor1<T>, which implements a unary factor corresponding to a measurement likelihood with a Gaussian noise model, L(q; m) = \\exp \\left\\{ -\\frac{1}{2} \\| h(q) - m \\|^2_{\\Sigma} \\right\\} = f(q) where m is the measurement, q is the unknown variable, h(q) is a (possibly nonlinear) measurement function, and Σ is the noise covariance. Note that m is considered known above, and the likelihood L(q; m) will only ever be evaluated as a function of q, which explains why it is a unary factor f(q). It is always the unknown variable q that is either likely or unlikely, given the measurement.\n\nNote\n\nMany people get this backwards, often misled by the conditional density notation P(m|q).\n\nIn fact, the likelihood L(q; m) is defined as any function of q proportional to P(m|q).\n\nListing 3.2 shows an example on how to define the custom factor class UnaryFactor which implements a “GPS-like” measurement likelihood:class UnaryFactor: public NoiseModelFactor1<Pose2> {\n  double mx_, my_; ///< X and Y measurements\n\npublic:\n  UnaryFactor(Key j, double x, double y, const SharedNoiseModel& model):\n    NoiseModelFactor1<Pose2>(model, j), mx_(x), my_(y) {}\n\n  Vector evaluateError(const Pose2& q,\n                       boost::optional<Matrix&> H = boost::none) const\n  {\n    const Rot2& R = q.rotation();\n    if (H) (*H) = (gtsam::Matrix(2, 3) <<\n            R.c(), -R.s(), 0.0,\n            R.s(), R.c(), 0.0).finished();\n    return (Vector(2) << q.x() - mx_, q.y() - my_).finished();\n  }\n};\n\nIn defining the derived class on line 1, we provide the template argument Pose2 to indicate the type of the variable q, whereas the measurement is stored as the instance variables mx_ and my_, defined on line 2. The constructor on lines 5-6 simply passes on the variable key j and the noise model to the superclass, and stores the measurement values provided. The most important function to has be implemented by every factor class is evaluateError(), which should return E(q) = h(q) - m which is done on line 12. Importantly, because we want to use this factor for nonlinear optimization (see e.g., \n\nKaess & Dellaert (2009) for details), whenever the optional argument H is provided, a Matrix reference, the function should assign the Jacobian of h(q), evauated at the provided value for q. This is done for this example on line 11. In this case, the Jacobian of the 2-dimensional function h, which just returns the position of the robot,h(q) = \\begin{bmatrix} q_x \\\\ q_y \\end{bmatrix}\n\nwith respect the 3-dimensional pose q = (q_x, q_y, q_\\theta), yields the following 2 \\times 3 matrix:H = \n\\begin{bmatrix}\n\\cos(q_\\theta) & -\\sin(q_\\theta) & 0 \\\\\n\\sin(q_\\theta) & \\cos(q_\\theta) & 0\n\\end{bmatrix}\n\nImportant\n\nMany of our users, when attempting to create a custom factor, are initially surprised at the Jacobian matrix not agreeing with their intuition. For example, above you might simply expect a 2 \\times 3 diagonal matrix. This would be true for variables belonging to a vector space. However, in GTSAM we define the Jacobian more generally to be the matrix H such thath(q e^\\xi) \\approx h(q) + H \\xi\n\nwhere \\xi = (\\delta x, \\delta y, \\delta \\theta) is an incremental update and \\mathrm{exp} \\hat{\\xi} is the exponential map for the variable we want to update, In this case q \\in SE(2), where SE(2) is the group of 2D rigid transforms, implemented by Pose2. The exponential map for SE(2) can be approximated to first order as\\exp \\hat{\\xi} \\approx \n\\begin{bmatrix}\n1 & -\\delta\\theta & \\delta x \\\\\n\\delta\\theta & 1 & \\delta y \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\nwhen using the 3 \\times 3 matric representation for 2D poses, and henceh \\left( q e^\\xi \\right) \\approx \nh \\left( \n\\begin{bmatrix}\n\\cos(q_\\theta) & -\\sin(q_\\theta) & q_x \\\\\n\\sin(q_\\theta) & \\cos(q_\\theta) & q_y \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & -\\delta\\theta & \\delta x \\\\\n\\delta\\theta & 1 & \\delta y \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\right)\n=\n\\begin{bmatrix}\nq_x + \\cos(q_\\theta) \\delta x - \\sin(q_\\theta) \\delta y \\\\\nq_y + \\sin(q_\\theta) \\delta x + \\cos(q_\\theta) \\delta y\n\\end{bmatrix}\n\nwhich then explains the Jacobian H.","type":"content","url":"/content/tutorial#id-3-2-defining-custom-factors","position":23},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"3.3 Using Custom Factors","lvl2":"3. Robot Localization"},"type":"lvl3","url":"/content/tutorial#id-3-3-using-custom-factors","position":24},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"3.3 Using Custom Factors","lvl2":"3. Robot Localization"},"content":"The following C++ code fragment illustrates how to create and add custom factors to a factor graph:// add unary measurement factors, like GPS, on all three poses\nnoiseModel::Diagonal::shared_ptr unaryNoise =\n noiseModel::Diagonal::Sigmas(Vector2(0.1, 0.1)); // 10cm std on x,y\ngraph.add(boost::make_shared<UnaryFactor>(1, 0.0, 0.0, unaryNoise));\ngraph.add(boost::make_shared<UnaryFactor>(2, 2.0, 0.0, unaryNoise));\ngraph.add(boost::make_shared<UnaryFactor>(3, 4.0, 0.0, unaryNoise));\n\nIn Listing 3.3, we create the noise model on line 2-3, which now specifies two standard deviations on the measurements m_x and m_y. On lines 4-6 we create shared_ptr versions of three newly created UnaryFactor instances, and add them to graph. GTSAM uses shared pointers to refer to factors in factor graphs, and boost::make_shared is a convenience function to simultaneously construct a class and create a shared_ptr to it. We obtain the factor graph from Figure 4.","type":"content","url":"/content/tutorial#id-3-3-using-custom-factors","position":25},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"3.4 Full Posterior Inference","lvl2":"3. Robot Localization"},"type":"lvl3","url":"/content/tutorial#id-3-4-full-posterior-inference","position":26},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"3.4 Full Posterior Inference","lvl2":"3. Robot Localization"},"content":"The three GPS factors are enough to fully constrain all unknown poses and tie them to a “global” reference frame, including the three unknown orientations. If not, GTSAM would have exited with a singular matrix exception. The marginals can be recovered exactly as in Section 2.5, and the solution and marginal covariances are now given by the following:Final Result:\nValues with 3 values:\nValue 1: (-1.5e-14, 1.3e-15, -1.4e-16)\nValue 2: (2, 3.1e-16, -8.5e-17)\nValue 3: (4, -6e-16, -8.2e-17)\n\nx1 covariance:\n    0.0083      4.3e-19     -1.1e-18\n    4.3e-19       0.0094      -0.0031\n    -1.1e-18      -0.0031       0.0082\nx2 covariance:\n    0.0071      2.5e-19     -3.4e-19\n    2.5e-19       0.0078      -0.0011\n    -3.4e-19      -0.0011       0.0082\nx3 covariance:\n    0.0083     4.4e-19     1.2e-18\n    4.4e-19      0.0094      0.0031\n    1.2e-18      0.0031       0.018\n\nComparing this with the covariance matrices in Section 2.5, we can see that the uncertainty no longer grows without bounds as measurement uncertainty accumulates. Instead, the “GPS” measurements more or less constrain the poses evenly, as expected.\n\n\n\nFigure 5:Comparing the marginals resulting from the “odometry” factor graph in Figure 3 and the “localization” factor graph in Figure 4.\n\nLocalization Marginals\n\nIt helps a lot when we view this graphically, as in Figure 5, where I show the marginals on position as 5-sigma covariance ellipses that contain 99.9996% of all probability mass. For the odometry marginals, it is immediately apparent from the figure that (1) the uncertainty on pose keeps growing, and (2) the uncertainty on angular odometry translates into increasing uncertainty on y. The localization marginals, in contrast, are constrained by the unary factors and are all much smaller. In addition, while less apparent, the uncertainty on the middle pose is actually smaller as it is constrained by odometry from two sides.\nYou might now be wondering how we produced these figures. The answer is via the MATLAB interface of GTSAM, which we will demonstrate in the next section.","type":"content","url":"/content/tutorial#id-3-4-full-posterior-inference","position":27},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl2":"4. PoseSLAM"},"type":"lvl2","url":"/content/tutorial#id-4-poseslam","position":28},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl2":"4. PoseSLAM"},"content":"","type":"content","url":"/content/tutorial#id-4-poseslam","position":29},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"4.1 Loop Closure Constraints","lvl2":"4. PoseSLAM"},"type":"lvl3","url":"/content/tutorial#id-4-1-loop-closure-constraints","position":30},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"4.1 Loop Closure Constraints","lvl2":"4. PoseSLAM"},"content":"The simplest instantiation of a SLAM problem is PoseSLAM, which avoids building an explicit map of the environment. The goal of SLAM is to simultaneously localize a robot and map the environment given incoming sensor measurements (\n\nDurrant-Whyte & Bailey (2006)). Besides wheel odometry, one of the most popular sensors for robots moving on a plane is a 2D laser-range finder, which provides both odometry constraints between successive poses, and loop-closure constraints when the robot re-visits a previously explored part of the environment.\n\n\n\nFigure 6:Factor graph for PoseSLAM.\n\nA factor graph example for PoseSLAM is shown in Figure 6. The following C++ code, included in GTSAM as an example, creates this factor graph in code:NonlinearFactorGraph graph;\nnoiseModel::Diagonal::shared_ptr priorNoise =\n  noiseModel::Diagonal::Sigmas(Vector3(0.3, 0.3, 0.1));\ngraph.add(PriorFactor<Pose2>(1, Pose2(0, 0, 0), priorNoise));\n\n// Add odometry factors\nnoiseModel::Diagonal::shared_ptr model =\n  noiseModel::Diagonal::Sigmas(Vector3(0.2, 0.2, 0.1));\ngraph.add(BetweenFactor<Pose2>(1, 2, Pose2(2, 0, 0     ), model));\ngraph.add(BetweenFactor<Pose2>(2, 3, Pose2(2, 0, M_PI_2), model));\ngraph.add(BetweenFactor<Pose2>(3, 4, Pose2(2, 0, M_PI_2), model));\ngraph.add(BetweenFactor<Pose2>(4, 5, Pose2(2, 0, M_PI_2), model));\n\n// Add the loop closure constraint\ngraph.add(BetweenFactor<Pose2>(5, 2, Pose2(2, 0, M_PI_2), model));\n\nAs before, lines 1-4 create a nonlinear factor graph and add the unary factor f_0(x_1). As the robot travels through the world, it creates binary factors\nf_t(x_t, x_{t+1}) corresponding to odometry, added to the graph in lines 6-12 (Note that M_PI_2 refers to pi/2). But line 15 models a different event: a loop closure. For example, the robot might recognize the same location using vision or a laser range finder, and calculate the geometric pose constraint to when it first visited this location. This is illustrated for poses x_5 and x_2, and generates the (red) loop closing factor f_5(x_5, x_2).\n\n\n\nFigure 7:The result of running optimize on the factor graph in Figure 6.\n\nWe can optimize this factor graph as before, by creating an initial estimate of type Values, and creating and running an optimizer. The result is shown graphically in Figure 7, along with covariance ellipses shown in green. These 5-sigma covariance ellipses in 2D indicate the marginal over position, over all possible orientations, and show the area which contain 99.9996% of the probability mass. The graph shows in a clear manner that the uncertainty on pose x_5 is now much less than if there would be only odometry measurements. The pose with the highest uncertainty, x_4, is the one furthest away from the unary constraint f_0(x_1), which is the only factor tying the graph to a global coordinate frame.\n\nThe figure above was created using an interface that allows you to use GTSAM from within MATLAB, which provides for visualization and rapid development. We discuss this next.\n\n4.2 Using the MATLAB Interface\nA large subset of the GTSAM functionality can be accessed through wrapped classes from within MATLAB. The following code excerpt is the MATLAB equivalent of the C++ code in Listing 4.1:graph = NonlinearFactorGraph;\npriorNoise = noiseModel.Diagonal.Sigmas([0.3; 0.3; 0.1]);\ngraph.add(PriorFactorPose2(1, Pose2(0, 0, 0), priorNoise));\n\n%% Add odometry factors\nmodel = noiseModel.Diagonal.Sigmas([0.2; 0.2; 0.1]);\ngraph.add(BetweenFactorPose2(1, 2, Pose2(2, 0, 0   ), model));\ngraph.add(BetweenFactorPose2(2, 3, Pose2(2, 0, pi/2), model));\ngraph.add(BetweenFactorPose2(3, 4, Pose2(2, 0, pi/2), model));\ngraph.add(BetweenFactorPose2(4, 5, Pose2(2, 0, pi/2), model));\n\n%% Add pose constraint\ngraph.add(BetweenFactorPose2(5, 2, Pose2(2, 0, pi/2), model));\n\nNote that the code is almost identical, although there are a few syntax and naming differences:\n\nObjects are created by calling a constructor instead of allocating them on the heap.\n\nNamespaces are done using dot notation, i.e., noiseModel::Diagonal::SigmasClasses becomes noiseModel.Diagonal.Sigmas.\n\nVector and Matrix classes in C++ are just vectors/matrices in MATLAB.\n\nAs templated classes do not exist in MATLAB, these have been hardcoded in the GTSAM interface, e.g., PriorFactorPose2 corresponds to the C++ class PriorFactor<Pose2>, etc.\n\nAfter executing the code, you can call whos on the MATLAB command prompt to see the objects created. Note that the indicated Class corresponds to the wrapped C++ classes:>> whos\nName                 Size            Bytes  Class\ngraph                1x1               112  gtsam.NonlinearFactorGraph\npriorNoise           1x1               112  gtsam.noiseModel.Diagonal\nmodel                1x1               112  gtsam.noiseModel.Diagonal\ninitialEstimate      1x1               112  gtsam.Values\noptimizer            1x1               112  gtsam.LevenbergMarquardtOptimizer\n\nIn addition, any GTSAM object can be examined in detail, yielding identical output to C++:>> priorNoise\ndiagonal sigmas [0.3; 0.3; 0.1];\n\n>> graph\nsize: 6\nfactor 0: PriorFactor on 1\nprior mean: (0, 0, 0)\nnoise model: diagonal sigmas [0.3; 0.3; 0.1];\nfactor 1: BetweenFactor(1,2)\nmeasured: (2, 0, 0)\nnoise model: diagonal sigmas [0.2; 0.2; 0.1];\nfactor 2: BetweenFactor(2,3)\nmeasured: (2, 0, 1.6)\nnoise model: diagonal sigmas [0.2; 0.2; 0.1];\nfactor 3: BetweenFactor(3,4)\nmeasured: (2, 0, 1.6)\nnoise model: diagonal sigmas [0.2; 0.2; 0.1];\nfactor 4: BetweenFactor(4,5)\nmeasured: (2, 0, 1.6)\nnoise model: diagonal sigmas [0.2; 0.2; 0.1];\nfactor 5: BetweenFactor(5,2)\nmeasured: (2, 0, 1.6)\nnoise model: diagonal sigmas [0.2; 0.2; 0.1];\n\nAnd it does not stop there: we can also call some of the functions defined for factor graphs. E.g.,>> graph.error(initialEstimate)\nans =\n20.1086\n\n>> graph.error(result)\nans =\n8.2631e-18\n\ncomputes the sum-squared error \\frac{1}{2} \\sum_i \\| h_i(X_i) - z_i \\|^2_{\\Sigma} before and after optimization.","type":"content","url":"/content/tutorial#id-4-1-loop-closure-constraints","position":31},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"4.3 Reading and Optimizing Pose Graphs","lvl2":"4. PoseSLAM"},"type":"lvl3","url":"/content/tutorial#id-4-3-reading-and-optimizing-pose-graphs","position":32},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"4.3 Reading and Optimizing Pose Graphs","lvl2":"4. PoseSLAM"},"content":"\n\nFigure 8:MATLAB plot of small Manhattan world example with 100 poses (due to Ed Olson). The initial estimate is shown in green. The optimized trajectory, with covariance ellipses, in blue.\n\nThe ability to work in MATLAB adds a much quicker development cycle, and effortless graphical output. The optimized trajectory in Figure 8 was produced by the code below, in which load2D() reads TORO files. To see how plotting is done, refer to the full source code.%% Initialize graph, initial estimate, and odometry noise\ndatafile = findExampleDataFile('w100.graph');\nmodel = noiseModel.Diagonal.Sigmas([0.05; 0.05; 5*pi/180]);\n[graph,initial] = load2D(datafile, model);\n\n%% Add a Gaussian prior on pose x_0\npriorMean = Pose2(0, 0, 0);\npriorNoise = noiseModel.Diagonal.Sigmas([0.01; 0.01; 0.01]);\ngraph.add(PriorFactorPose2(0, priorMean, priorNoise));\n\n%% Optimize using Levenberg-Marquardt optimization and get marginals\noptimizer = LevenbergMarquardtOptimizer(graph, initial);\nresult = optimizer.optimizeSafely;\nmarginals = Marginals(graph, result);","type":"content","url":"/content/tutorial#id-4-3-reading-and-optimizing-pose-graphs","position":33},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"4.4 PoseSLAM in 3D","lvl2":"4. PoseSLAM"},"type":"lvl3","url":"/content/tutorial#id-4-4-poseslam-in-3d","position":34},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"4.4 PoseSLAM in 3D","lvl2":"4. PoseSLAM"},"content":"PoseSLAM can easily be extended to 3D poses, but some care is needed to update 3D rotations. GTSAM supports both quaternions and 3 \\times 3 rotation matrices to represent 3D rotations. The selection is made via the compile flag GTSAM_USE_QUATERNIONS.\n\n\n\nFigure 9:3D plot of sphere example (due to Michael Kaess). The very wrong initial estimate, derived from odometry, is shown in green. The optimized trajectory is shown red. Code below:%% Initialize graph, initial estimate, and odometry noise\ndatafile = findExampleDataFile('sphere2500.txt');\nmodel = noiseModel.Diagonal.Sigmas([5*pi/180; 5*pi/180; 5*pi/180; 0.05; 0.05; 0.05]);\n[graph,initial] = load3D(datafile, model, true, 2500);\nplot3DTrajectory(initial, 'g-', false); % Plot Initial Estimate\n\n%% Read again, now with all constraints, and optimize\ngraph = load3D(datafile, model, false, 2500);\ngraph.add(NonlinearEqualityPose3(0, initial.atPose3(0)));\noptimizer = LevenbergMarquardtOptimizer(graph, initial);\nresult = optimizer.optimizeSafely();\nplot3DTrajectory(result, 'r-', false); axis equal;","type":"content","url":"/content/tutorial#id-4-4-poseslam-in-3d","position":35},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl2":"5. Landmark-based SLAM"},"type":"lvl2","url":"/content/tutorial#id-5-landmark-based-slam","position":36},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl2":"5. Landmark-based SLAM"},"content":"","type":"content","url":"/content/tutorial#id-5-landmark-based-slam","position":37},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"5.1 Basics","lvl2":"5. Landmark-based SLAM"},"type":"lvl3","url":"/content/tutorial#id-5-1-basics","position":38},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"5.1 Basics","lvl2":"5. Landmark-based SLAM"},"content":"\n\nFigure 10:Factor graph for landmark-based SLAM\n\nIn landmark-based SLAM, we explicitly build a map with the location of observed landmarks, which introduces a second type of variable in the factor graph besides robot poses. An example factor graph for a landmark-based SLAM example is shown in Figure 10, which shows the typical connectivity: poses are connected in an odometry Markov chain, and landmarks are observed from multiple poses, inducing binary factors. In addition, the pose x_1 has the usual prior on it.\n\n\n\nFigure 11:The optimized result along with covariance ellipses for both poses (in green) and landmarks (in blue). Also shown are the trajectory (red) and landmark sightings (cyan).\n\nThe factor graph from Figure 10 can be created using the MATLAB code in Listing 5.1. As before, on line 2 we create the factor graph, and Lines 8-18 create the prior/odometry chain we are now familiar with. However, the code on lines 20-25 is new: it creates three measurement factors, in this case “bearing/range” measurements from the pose to the landmark.% Create graph container and add factors to it\ngraph = NonlinearFactorGraph;\n\n% Create keys for variables\ni1 = symbol('x',1); i2 = symbol('x',2); i3 = symbol('x',3);\nj1 = symbol('l',1); j2 = symbol('l',2);\n\n% Add prior\npriorMean = Pose2(0.0, 0.0, 0.0); % prior at origin\npriorNoise = noiseModel.Diagonal.Sigmas([0.3; 0.3; 0.1]);\n% add directly to graph\ngraph.add(PriorFactorPose2(i1, priorMean, priorNoise));\n\n% Add odometry\nodometry = Pose2(2.0, 0.0, 0.0);\nodometryNoise = noiseModel.Diagonal.Sigmas([0.2; 0.2; 0.1]);\ngraph.add(BetweenFactorPose2(i1, i2, odometry, odometryNoise));\ngraph.add(BetweenFactorPose2(i2, i3, odometry, odometryNoise));\n\n% Add bearing/range measurement factors\ndegrees = pi/180;\nbrNoise = noiseModel.Diagonal.Sigmas([0.1; 0.2]);\ngraph.add(BearingRangeFactor2D(i1, j1, Rot2(45*degrees), sqrt(8), brNoise));\ngraph.add(BearingRangeFactor2D(i2, j1, Rot2(90*degrees), 2, brNoise));\ngraph.add(BearingRangeFactor2D(i3, j2, Rot2(90*degrees), 2, brNoise));","type":"content","url":"/content/tutorial#id-5-1-basics","position":39},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"5.2 Of Keys and Symbols","lvl2":"5. Landmark-based SLAM"},"type":"lvl3","url":"/content/tutorial#id-5-2-of-keys-and-symbols","position":40},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"5.2 Of Keys and Symbols","lvl2":"5. Landmark-based SLAM"},"content":"The only unexplained code is on lines 4-6: here we create integer keys for the poses and landmarks using the symbol() function. In GTSAM, we address all variables using the Key type, which is just a typedef to size_t  . The keys do not have to be numbered continuously, but they do have to be unique within a given factor graph. For factor graphs with different types of variables, we provide the symbol() function in MATLAB, and the Symbol type in C++, to help you create (large) integer keys that are far apart in the space of possible keys, so you don’t have to think about starting the point numbering at some arbitrary offset. To create a symbol key you simply provide a character and an integer index. You can use base 0 or 1, or use arbitrary indices: it does not matter. In the code above, we we use ‘x’ for poses, and ‘l’ for landmarks.\n\nThe optimized result for the factor graph created by Listing 5.1 is shown in Figure 11, and it is readily apparent that the landmark l_1 with two measurements is better localized. In MATLAB we can also examine the actual numerical values, and doing so reveals some more GTSAM magic:result Values with 5 values: l1: (2, 2) l2: (4, 2) x1: (-1.8e-16, 5.1e-17, -1.5e-17) x2: (2, -5.8e-16, -4.6e-16) x3: (4, -3.1e-15, -4.6e-16)\n\nIndeed, the keys generated by symbol are automatically detected by the print method in the Values class, and rendered in human-readable form “x1”, “l2”, etc, rather than as large, unwieldy integers. This magic extends to most factors and other classes where the Key type is used.","type":"content","url":"/content/tutorial#id-5-2-of-keys-and-symbols","position":41},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"5.3 A Larger Example","lvl2":"5. Landmark-based SLAM"},"type":"lvl3","url":"/content/tutorial#id-5-3-a-larger-example","position":42},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"5.3 A Larger Example","lvl2":"5. Landmark-based SLAM"},"content":"\n\nFigure 12:A larger example with about 100 poses and 30 or so landmarks, as produced by gtsam_examples/PlanarSLAMExample_graph.m\n\nGTSAM comes with a slightly larger example that is read from a .graph file by PlanarSLAMExample_graph.m, shown in Figure 12. To not clutter the figure only the marginals are shown, not the lines of sight. This example, with 119 (multivariate) variables and 517 factors optimizes in less than 10 ms.","type":"content","url":"/content/tutorial#id-5-3-a-larger-example","position":43},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"5.4 A Real-World Example","lvl2":"5. Landmark-based SLAM"},"type":"lvl3","url":"/content/tutorial#id-5-4-a-real-world-example","position":44},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"5.4 A Real-World Example","lvl2":"5. Landmark-based SLAM"},"content":"\n\nFigure 13:Small section of optimized trajectory and landmarks (trees detected in a laser range finder scan) from data recorded in Sydney’s Victoria Park (dataset due to Jose Guivant, U. Sydney).\n\nA real-world example is shown in Figure 13, using data from a well known dataset collected in Sydney’s Victoria Park, using a truck equipped with a laser range-finder. The covariance matrices in this figure were computed very efficiently, as explained in detail in \n\nKaess & Dellaert (2009). The exact covariances (blue, smaller ellipses) obtained by our fast algorithm coincide with the exact covariances based on full inversion (orange, mostly hidden by blue). The much larger conservative covariance estimates (green, large ellipses) were based on our earlier work in \n\nKaess & Dellaert (2008).","type":"content","url":"/content/tutorial#id-5-4-a-real-world-example","position":45},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl2":"6. Structure from Motion"},"type":"lvl2","url":"/content/tutorial#id-6-structure-from-motion","position":46},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl2":"6. Structure from Motion"},"content":"\n\nFigure 14:An optimized “Structure from Motion” with 10 cameras arranged in a circle, observing the 8 vertices of a 20 \\times 20 \\times 20 cube centered around the origin. The camera is rendered with color-coded axes, (RGB for XYZ) and the viewing direction is is along the positive Z-axis. Also shown are the 3D error covariance ellipses for both cameras and points.\n\nStructure from Motion (SFM) is a technique to recover a 3D reconstruction of the environment from corresponding visual features in a collection of unordered images, see Figure 14. In GTSAM this is done using exactly the same factor graph framework, simply using SFM-specific measurement factors. In particular, there is a projection factor that calculates the reprojection error f(x_i, p_j, z_{ij}, K) for a given camera pose x_i (a Pose3) and a point p_j ( a Point3). The factor is parameterized by the 2D measurement z_{ij} (a Point 2), and known calibration parameters K (of type Cal3_S2). The following listing shows how to create the factor graph:%% Add factors for all measurements\nnoise = noiseModel.Isotropic.Sigma(2, measurementNoiseSigma);\nfor i = 1:length(Z),\n    for k = 1:length(Z{i})\n        j = J{i}{k};\n        G.add(GenericProjectionFactorCal3_S2(\n              Z{i}{k}, noise, symbol('x', i), symbol('p', j), K));\n    end\nend\n\nIn Listing 6, assuming that the factor graph was already created, we add measurement factors in the double loop. We loop over images with index i, and in this example the data is given as two cell arrays: Z{i} specifies a set of measurements z_k in image i, and J{i} specifies the corresponding point index. The specific factor type we use is a GenericProjectionFactorCal3_S2, which is the MATLAB equivalent of the C++ class GenericProjectionFactor<Cal3_S2>, where Cal3_S2 is the camera calibration type we choose to use (the standard, no-radial distortion, 5 parameter calibration matrix). As before landmark-based SLAM (Section 5), here we use symbol keys except we now use the character ‘p’ to denote points, rather than ‘l’ for landmark.\n\nImportant\n\nA very tricky and difficult part of making SFM work is (a) data association, and (b) initialization. GTSAM does neither of these things for you: it simply provides the “bundle adjustment” optimization. In the example, we simply assume the data association is known (it is encoded in the J sets), and we initialize with the ground truth, as the intent of the example is simply to show you how to set up the optimization problem.","type":"content","url":"/content/tutorial#id-6-structure-from-motion","position":47},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl2":"7. iSAM: Incremental Smoothing and Mapping"},"type":"lvl2","url":"/content/tutorial#id-7-isam-incremental-smoothing-and-mapping","position":48},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl2":"7. iSAM: Incremental Smoothing and Mapping"},"content":"GTSAM provides an incremental inference algorithm based on a more advanced graphical model, the Bayes tree, which is kept up to date by the iSAM algorithm (incremental Smoothing and Mapping, see \n\nKaess & Dellaert (2008); \n\nKaess & Dellaert (2012) for an in-depth treatment). For mobile robots operating in real-time it is important to have access to an updated map as soon as new sensor measurements come in. iSAM keeps the map up-to-date in an efficient manner.\n\nListing 7 shows how to use iSAM in a simple visual SLAM example. In line 1-2 we create a NonlinearISAM object which will relinearize and reorder the variables every 3 steps. The corect value for this parameter depends on how non-linear your problem is and how close you want to be to gold-standard solution at every step. In iSAM 2.0, this parameter is not needed, as iSAM2 automatically determines when linearization is needed and for which variables.\n\nThe example involves eight 3D points that are seen from eight successive camera poses. Hence in the first step -which is omitted here- all eight landmarks and the first pose are properly initialized. In the code this is done by perturbing the known ground truth, but in a real application great care is needed to properly initialize poses and landmarks, especially in a monocular sequence.int relinearizeInterval = 3;\nNonlinearISAM isam(relinearizeInterval);\n\n// ... first frame initialization omitted ...\n\n// Loop over the different poses, adding the observations to iSAM\nfor (size_t i = 1; i < poses.size(); ++i) {\n\n  // Add factors for each landmark observation\n  NonlinearFactorGraph graph;\n  for (size_t j = 0; j < points.size(); ++j) {\n    graph.add(\n      GenericProjectionFactor<Pose3, Point3, Cal3_S2>\n        (z[i][j], noise,Symbol('x', i), Symbol('l', j), K)\n    );\n  }\n\n  // Add an initial guess for the current pose\n  Values initialEstimate;\n  initialEstimate.insert(Symbol('x', i), initial_x[i]);\n\n  // Update iSAM with the new factors\n  isam.update(graph, initialEstimate);\n }\n\nThe remainder of the code illustrates a typical iSAM loop:\n\nCreate factors for new measurements. Here, in lines 9-18, a small NonlinearFactorGraph is created to hold the new factors of type GenericProjectionFactor<Pose3, Point3, Cal3_S2>.\n\nCreate an initial estimate for all newly introduced variables. In this small example, all landmarks have been observed in frame 1 and hence the only new variable that needs to be initialized at each time step is the new pose. This is done in lines 20-22. Note we assume a good initial estimate is available as initial_x[i].\n\nFinally, we call isam.update(), which takes the factors and initial estimates, and incrementally updates the solution, which is available through the method isam.estimate(), if desired.","type":"content","url":"/content/tutorial#id-7-isam-incremental-smoothing-and-mapping","position":49},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl2":"8. More Applications"},"type":"lvl2","url":"/content/tutorial#id-8-more-applications","position":50},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl2":"8. More Applications"},"content":"While a detailed discussion of all the things you can do with GTSAM will take us too far, below is a small survey of what you can expect to do, and which we did using GTSAM.","type":"content","url":"/content/tutorial#id-8-more-applications","position":51},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"8.1 Conjugate Gradient Optimization","lvl2":"8. More Applications"},"type":"lvl3","url":"/content/tutorial#id-8-1-conjugate-gradient-optimization","position":52},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"8.1 Conjugate Gradient Optimization","lvl2":"8. More Applications"},"content":"\n\nFigure 15:A map of Beijing, with a spanning tree shown in black, and the remaining loop-closing constraints shown in red. A spanning tree can be used as a preconditioner by GTSAM.\n\nGTSAM also includes efficient preconditioned conjugate gradients (PCG) methods for solving large-scale SLAM problems. While direct methods, popular in the literature, exhibit quadratic convergence and can be quite efficient for sparse problems, they typically require a lot of storage and efficient elimination orderings to be found. In contrast, iterative optimization methods only require access to the gradient and have a small memory footprint, but can suffer from poor convergence. Our method, subgraph preconditioning, explained in detail in \n\nDellaert & Thorpe (2010); \n\nJian & Dellaert (2011), combines the advantages of direct and iterative methods, by identifying a sub-problem that can be easily solved using direct methods, and solving for the remaining part using PCG. The easy sub-problems correspond to a spanning tree, a planar subgraph, or any other substructure that can be efficiently solved. An example of such a subgraph is shown in Figure 15.","type":"content","url":"/content/tutorial#id-8-1-conjugate-gradient-optimization","position":53},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"8.2 Visual Odometry","lvl2":"8. More Applications"},"type":"lvl3","url":"/content/tutorial#id-8-2-visual-odometry","position":54},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"8.2 Visual Odometry","lvl2":"8. More Applications"},"content":"A gentle introduction to vision-based sensing is Visual Odometry (abbreviated VO, see e.g. \n\nNistér & Bergen (2004)), which provides pose constraints between successive robot poses by tracking or associating visual features in successive images taken by a camera mounted rigidly on the robot. GTSAM includes both C++ and MATLAB example code, as well as VO-specific factors to help you on the way.","type":"content","url":"/content/tutorial#id-8-2-visual-odometry","position":55},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"8.3 Visual SLAM","lvl2":"8. More Applications"},"type":"lvl3","url":"/content/tutorial#id-8-3-visual-slam","position":56},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"8.3 Visual SLAM","lvl2":"8. More Applications"},"content":"Visual SLAM (see e.g., \n\nDavison (2003)) is a SLAM variant where 3D points are observed by a camera as the camera moves through space, either mounted on a robot or moved around by hand. GTSAM, and particularly iSAM (see below), can easily be adapted to be used as the back-end optimizer in such a scenario.","type":"content","url":"/content/tutorial#id-8-3-visual-slam","position":57},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"8.4 Fixed-lag Smoothing and Filtering","lvl2":"8. More Applications"},"type":"lvl3","url":"/content/tutorial#id-8-4-fixed-lag-smoothing-and-filtering","position":58},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"8.4 Fixed-lag Smoothing and Filtering","lvl2":"8. More Applications"},"content":"GTSAM can easily perform recursive estimation, where only a subset of the poses are kept in the factor graph, while the remaining poses are marginalized out. In all examples above we explicitly optimize for all variables using all available measurements, which is called Smoothing because the trajectory is “smoothed” out, and this is where GTSAM got its name (GT Smoothing and Mapping). When instead only the last few poses are kept in the graph, one speaks of Fixed-lag Smoothing. Finally, when only the single most recent poses is kept, one speaks of Filtering, and indeed the original formulation of SLAM was filter-based (\n\nSmith & Cheeseman (1988)).","type":"content","url":"/content/tutorial#id-8-4-fixed-lag-smoothing-and-filtering","position":59},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"8.5 Discrete Variables and HMMs","lvl2":"8. More Applications"},"type":"lvl3","url":"/content/tutorial#id-8-5-discrete-variables-and-hmms","position":60},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl3":"8.5 Discrete Variables and HMMs","lvl2":"8. More Applications"},"content":"Finally, factor graphs are not limited to continuous variables: GTSAM can also be used to model and solve discrete optimization problems. For example, a Hidden Markov Model (HMM) has the same graphical model structure as the Robot Localization problem from Section 2, except that in an HMM the variables are discrete. GTSAM can optimize and perform inference for discrete models.","type":"content","url":"/content/tutorial#id-8-5-discrete-variables-and-hmms","position":61},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl2":"Acknowledgements"},"type":"lvl2","url":"/content/tutorial#acknowledgements","position":62},{"hierarchy":{"lvl1":"Factor Graphs and GTSAM","lvl2":"Acknowledgements"},"content":"GTSAM was made possible by the efforts of many collaborators at Georgia Tech and elsewhere, including but not limited to Doru Balcan, Chris Beall, Alex Cunningham, Alireza Fathi, Eohan George, Viorela Ila, Yong-Dian Jian, Michael Kaess, Kai Ni, Carlos Nieto, Duy-Nguyen Ta, Manohar Paluri, Christian Potthast, Richard Roberts, Grant Schindler, and Stephen Williams. In addition, Paritosh Mohan helped me with the manual. Many thanks all for your hard work!\n\nGTSAM also allows you to wrap your own custom-made classes, although this is out of the scope of this manual. The following code excerpt is the MATLAB equivalent of the C++ code in Listing 4.1:\n\na 32 or 64 bit integer, depending on your platform","type":"content","url":"/content/tutorial#acknowledgements","position":63}]}